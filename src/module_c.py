import os
import json
import re
import glob
import datetime
from typing import List, Dict, Any, Tuple, Optional
from email.utils import parsedate_to_datetime
from collections import Counter, defaultdict
from src.config import load_config, llm_config
from src.timeutil import to_date, kst_date_str, kst_run_suffix
from src.utils import load_json, save_json, latest, clean_text

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

# ================= ê³µìš© ìŠ¤ìœ„ì¹˜/ë¡œê·¸ =================
def use_pro_mode() -> bool:
    v = os.getenv("USE_PRO", "").lower()
    if v in ("1","true","yes","y"):
        return True
    if v in ("0","false","no","n"):
        return False
    try:
        with open("config.json","r",encoding="utf-8") as f:
            cfg = json.load(f) or {}
            return bool(cfg.get("use_pro", False))
    except Exception:
        return False

def _log_mode(prefix="Module C"):
    try:
        is_pro = use_pro_mode()
    except Exception:
        is_pro = False
    mode = "PRO" if is_pro else "LITE"
    print(f"[INFO] USE_PRO={str(is_pro).lower()} â†’ {prefix} ({mode}) ì‹œì‘")

# ================= ì„¤ì • ë¡œë“œ =================
CFG = load_config()
LLM = llm_config(CFG)

# ================= ë°ì´í„° ë¡œë” =================
def select_latest_files_per_day(glob_pattern: str, days: int) -> List[str]:
    all_files = sorted(glob.glob(glob_pattern))
    daily_files = defaultdict(list)
    for f in all_files:
        date_key = os.path.basename(f)[:10]
        daily_files[date_key].append(f)
    latest_daily_files = []
    for date_key in sorted(daily_files.keys()):
        latest_file_for_day = sorted(daily_files[date_key])[-1]
        latest_daily_files.append(latest_file_for_day)
    return latest_daily_files

def load_today_meta() -> Tuple[List[str], List[str]]:
    meta_path = latest("data/news_meta_*.json")
    if not meta_path: return [], []
    docs, dates = [], []
    try:
        with open(meta_path, "r", encoding="utf-8") as f:
            items = json.load(f) or []
    except Exception:
        return [], []
    for it in items:
        title = clean_text((it.get("title") or it.get("title_og") or "").strip())
        desc  = clean_text((it.get("body") or it.get("description") or it.get("description_og") or "").strip())
        doc = (title + " " + desc).strip()
        if not doc: continue
        d_raw = it.get("published_time") or it.get("pubDate_raw") or ""
        docs.append(doc)
        dates.append(to_date(d_raw))
    return docs, dates

def load_warehouse_paths(days: int = 30) -> List[str]:
    # í•˜ë£¨ì¹˜ ì—¬ìœ ë¶„ì„ í¬í•¨í•˜ì—¬ D+1ì¼ì íŒŒì¼ì„ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
    return select_latest_files_per_day("data/warehouse/*.jsonl", days=(days + 1))

# ================= ì‹œê³„ì—´ =================
def calculate_stable_timeseries(warehouse_files: List[str]) -> Dict[str, Any]:
    """
    Dì¼ìì™€ D+1ì¼ì íŒŒì¼ì„ ê¸°ë°˜ìœ¼ë¡œ Dì¼ì˜ ìµœì¢… ê¸°ì‚¬ ìˆ˜ë¥¼ ê³„ì‚°í•˜ëŠ” ì•ˆì •ì ì¸ ì‹œê³„ì—´ ë¶„ì„ í•¨ìˆ˜
    """
    file_map = {os.path.basename(f)[:10]: f for f in warehouse_files}
    if not file_map:
        return {"daily": []}
    
    sorted_dates = sorted(file_map.keys())
    start_date = datetime.datetime.strptime(sorted_dates[0], "%Y-%m-%d").date()
    # ë§ˆì§€ë§‰ ë‚ ì§œëŠ” D+1ì¼ì´ ì—†ìœ¼ë¯€ë¡œ, ê·¸ ì „ë‚ ê¹Œì§€ë§Œ ê³„ì‚°í•©ë‹ˆë‹¤.
    end_date = datetime.datetime.strptime(sorted_dates[-1], "%Y-%m-%d").date() - datetime.timedelta(days=1)
    
    daily_counts = []
    current_date = start_date

    while current_date <= end_date:
        date_str_d = current_date.strftime("%Y-%m-%d")
        date_str_d_plus_1 = (current_date + datetime.timedelta(days=1)).strftime("%Y-%m-%d")
        
        count = 0
        
        files_to_check = []
        if date_str_d in file_map:
            files_to_check.append(file_map[date_str_d])
        if date_str_d_plus_1 in file_map:
            files_to_check.append(file_map[date_str_d_plus_1])
            
        for fp in files_to_check:
            try:
                with open(fp, "r", encoding="utf-8") as f:
                    for line in f:
                        try:
                            obj = json.loads(line)
                            d_raw = obj.get("published") or obj.get("created_at") or os.path.basename(fp)[:10]
                            published_date = to_date(d_raw)
                            
                            if published_date == date_str_d:
                                count += 1
                        except Exception:
                            continue
            except Exception:
                continue
                
        daily_counts.append({"date": date_str_d, "count": count})
        current_date += datetime.timedelta(days=1)
        
    return {"daily": daily_counts}

# ================= ë¶ˆìš©/ì»·(í† í”½ ê³µí†µ) =================
EN_STOP = {
    "the","and","to","of","in","for","on","with","at","by","from","as","is","are","be","it",
    "that","this","an","a","or","if","we","you","they","he","she","was","were","been","than",
    "into","about","over","under","per","via"
}
KO_FUNC = {
    "í•˜ë‹¤","ìˆë‹¤","ë˜ë‹¤","í†µí•´","ì´ë²ˆ","ëŒ€í•œ","ê²ƒìœ¼ë¡œ","ë°í˜”ë‹¤","ë‹¤ì–‘í•œ","í•¨ê»˜","í˜„ì¬",
    "ê¸°ì","ëŒ€í‘œ","íšŒì¥","ì£¼ìš”","ê¸°ì¤€","ìœ„í•´","ìœ„í•œ","ì§€ì›","ì „ëµ","ì •ì±…","í˜‘ë ¥","í™•ëŒ€",
    "ë§í–ˆë‹¤","ê°•ì¡°í–ˆë‹¤","ëŒ€ìƒ","ëŒ€ìƒìœ¼ë¡œ","ìµœê·¼","ì§€ë‚œí•´","ìƒí™œ","ì‹œì¥","ìŠ¤ë§ˆíŠ¸","ë””ì§€í„¸","ê¸€ë¡œë²Œ",
    "ê·¸ëŠ”","ê·¸ë…€ëŠ”","ì´ì–´","í•œí¸","ë˜í•œ","ì´ë‚ ","ì´ë¼ë©°","ì´ë¼ê³ ","ëª¨ë¸ì„","ì„±ê³¼ë¥¼","ë°›ì•˜ë‹¤","ì„œìš¸","ê¸°ë°˜ìœ¼ë¡œ",
    "ìˆëŠ”","ìˆìœ¼ë©°","ìˆë‹¤ëŠ”","ì´í›„","ì„¤ëª…í–ˆë‹¤","ì „í–ˆë‹¤","ê³„íšì´ë‹¤","ê´€ê³„ìëŠ”","ë”°ë¥´ë©´",
    "ì˜¬í•´","ë‚´ë…„","ìµœëŒ€","ì‹ ê·œ","ê¸°ì¡´","êµ­ì œ","êµ­ë‚´","ì„¸ê³„","ì˜¤ì „","ì˜¤í›„",
    "ë“±ì„", "ë”°ë¼", "ìˆë„ë¡", "ì§€ë‚œ", "íŠ¹íˆ", "ëŒ€ë¹„", "ì•„ë‹ˆë¼", "ë§Œì—", "ì˜ì›ì€", "ë¼ê³ ",
    "ìˆìŠµë‹ˆë‹¤", "ê´€ë ¨", "í•œë‹¤", "ì§„í–‰í•œë‹¤", "ì˜ˆì •ì´ë‹¤", "ê°€ëŠ¥í•˜ë‹¤", "ìˆì—ˆë‹¤",
    "ì´ìƒ", "ë„˜ì–´", "ì œê³µí•œë‹¤", "ê°™ì€", "í–ˆë‹¤", "ë§ì€", "ê·¸ë¦¬ê³ ", "ê°™ë‹¤", "ìš°ë¦¬", "í•˜ê³ ",
    "ë•Œë¬¸ì—", "ì´ë ‡ê²Œ", "ì´ëŸ°", "ë“±ì´", "ê°ê°"
}

def is_bad_token(base: str) -> bool:
    if base in KO_FUNC or base.lower() in EN_STOP: return True
    if re.fullmatch(r"\d+$", base): return True
    if re.fullmatch(r"\d{1,2}$", base): return True
    if re.fullmatch(r"\d{1,2}ì›”$", base): return True
    if re.fullmatch(r"\d{1,2}ì¼$", base): return True
    if re.search(r"(ì–µ|ì¡°|ë‹¬ëŸ¬|ì›)$", base): return True
    return False

# ================= ë‚´ë¶€ í—¬í¼: prob ì„¸ì´í”„ê°€ë“œ =================
def _ensure_prob_payload(obj: dict, topn: int = 10, decay: float = 0.95, floor: float = 0.2) -> dict:
    """
    - ì €ì¥ ì§ì „ì— í˜¸ì¶œí•˜ì—¬ ëª¨ë“  í† í”½ ë‹¨ì–´ì— probë¥¼ ê°•ì œ ì£¼ì….
    - ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë©´ dict ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ í›„ ì£¼ì….
    - ì´ë¯¸ probê°€ ìˆë”ë¼ë„ 0/ìŒìˆ˜/NaNì€ ìµœì†Œê°’ ë³´ì •.
    """
    topics = obj.get("topics") or []
    for t in topics:
        ws = t.get("top_words") or []
        # ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ â†’ dict ë³€í™˜
        if ws and isinstance(ws[0], str):
            ws = [{"word": w} for w in ws if w]
            t["top_words"] = ws
        # ì£¼ì…/ë³´ì •
        if not ws:
            continue
        all_have = all((isinstance(w, dict) and ("prob" in w)) for w in ws)
        if all_have:
            for w in ws:
                try:
                    p = float(w.get("prob", 0))
                    if not (p > 0):
                        w["prob"] = 1e-6
                except Exception:
                    w["prob"] = 1e-6
        else:
            for rank, w in enumerate(ws[:topn], start=0):
                if not isinstance(w, dict):
                    continue
                prob = max(floor, decay ** rank)
                w["prob"] = float(prob)
    return obj

# ================= Lite í† í”½(LDA) â€” prob í¬í•¨ =================
def build_topics_lite(docs: List[str],
                      max_features=8000,
                      topn=10) -> Dict[str, Any]:
    # --- ì œì•ˆ ë‚´ìš© ë°˜ì˜ ì‹œì‘ ---
    # 1. config.jsonì—ì„œ íŒŒë¼ë¯¸í„° ì½ì–´ì˜¤ê¸°
    k_candidates = CFG.get("topic_k_candidates", [8, 10, 12, 14])
    min_df_val = int(CFG.get("topic_min_df", 7))
    max_df_val = float(CFG.get("topic_max_df", 0.85))

    # 2. config.json ë° ì‚¬ì „ì˜ ë¶ˆìš©ì–´ ëª©ë¡ í†µí•©
    phrase_stop_cfg = set(CFG.get("phrase_stop", []) or [])
    stopwords_cfg = set(CFG.get("stopwords", []) or [])
    
    # phrase_stopì€ í…ìŠ¤íŠ¸ì—ì„œ ë¨¼ì € ì œê±° (ì½”ë“œëŠ” í•¨ìˆ˜ í›„ë°˜ë¶€ì— ì´ë¯¸ ì¡´ì¬)
    
    # CountVectorizerì— ì ìš©í•  ìµœì¢… ë¶ˆìš©ì–´ ëª©ë¡
    final_stopwords = list(set(EN_STOP) | set(KO_FUNC) | stopwords_cfg)
    
    print(f"[DEBUG][C] LITE builder ì§„ì… | k_candidates={k_candidates} min_df={min_df_val} max_df={max_df_val}")
    
    # 3. ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ì—ì„œ phrase_stop ë¨¼ì € ì œê±°
    processed_docs = []
    if docs:
        for doc in docs:
            temp_doc = doc
            for phrase in phrase_stop_cfg:
                temp_doc = temp_doc.replace(phrase, " ")
            processed_docs.append(temp_doc)
    else:
        return {"topics": []}

    vec = CountVectorizer(
        ngram_range=(1, 3), # 3-gram(tri-gram) í¬í•¨
        max_features=max_features,
        min_df=min_df_val, # ìƒí–¥ëœ min_df ì ìš©
        max_df=max_df_val,
        token_pattern=r"[ê°€-í£A-Za-z0-9_]{2,}",
        stop_words=final_stopwords # í†µí•©ëœ ë¶ˆìš©ì–´ ëª©ë¡ ì ìš©
    )
    # --- ì œì•ˆ ë‚´ìš© ë°˜ì˜ ë ---

    X = vec.fit_transform(processed_docs)
    vocab = vec.get_feature_names_out()
    if X.shape[1] == 0:
        return {"topics": []}

    # ì´í•˜ ë¡œì§ì€ ë™ì¼
    def topic_pairs(lda, n_top=topn):
        comps = lda.components_
        topics = []
        for tid, comp in enumerate(comps):
            idx = comp.argsort()[-max(n_top, 30):][::-1]
            pairs = [(vocab[i], float(comp[i])) for i in idx]
            topics.append((tid, pairs))
        return topics

    def bad_ratio_from_pairs(pairs):
        bad = 0
        for w, _s in pairs:
            base = w.split()[0] if " " in w else w
            if is_bad_token(base): bad += 1
        return bad / max(1, len(pairs))

    best_topics = None; best_score = -1.0
    for k in k_candidates:
        lda = LatentDirichletAllocation(n_components=k, learning_method="batch", random_state=42, max_iter=15)
        _ = lda.fit_transform(X)
        ts = topic_pairs(lda, n_top=topn)
        good = sum(1 for _, pairs in ts if bad_ratio_from_pairs(pairs) < 0.20)
        score = good / float(k)
        if score > best_score:
            best_score = score; best_topics = ts

    topics_obj = {"topics": []}
    if not best_topics:
        print("[DEBUG][C] LITE ìƒì„± ì™„ë£Œ | topics=0")
        return topics_obj

    for tid, pairs in best_topics:
        kept = []
        for w, s in pairs:
            base = w.split()[0] if " " in w else w
            if is_bad_token(base):
                continue
            kept.append((w, s))
        if not kept:
            kept = pairs[:]

        scores = [max(float(s or 0.0), 0.0) for _, s in kept]
        maxv = max(scores) if scores else 0.0
        payload = []
        if maxv > 0 and (max(scores) - min(scores)) > 1e-12:
            for (w, s) in kept[:topn]:
                prob = max(float(s or 0.0), 0.0) / maxv
                payload.append({"word": w, "prob": prob})
        else:
            decay = 0.95
            for rank, (w, _s) in enumerate(kept[:topn], start=0):
                prob = max(0.2, decay**rank)
                payload.append({"word": w, "prob": prob})

        topics_obj["topics"].append({"topic_id": int(tid), "top_words": payload})
    print(f"[DEBUG][C] LITE ìƒì„± ì™„ë£Œ | topics={len(topics_obj.get('topics', []))}")
    return topics_obj


# ================= Pro í† í”½(BERTopic) â€” prob í¬í•¨ =================
def pro_build_topics_bertopic(docs, topn=10):
    print("[DEBUG][C] PRO builder ì§„ì…")
    try:
        from bertopic import BERTopic
        from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance
        from sentence_transformers import SentenceTransformer
        from sklearn.feature_extraction.text import CountVectorizer
        import numpy as np
    except Exception as e:
        raise RuntimeError(f"Pro í† í”½ ëª¨ë“œ ì¤€ë¹„ ì‹¤íŒ¨(íŒ¨í‚¤ì§€ ì—†ìŒ): {e}")

    # --- ì£¼ì œ í•„í„°ë§ ë¡œì§ ì‹œì‘ ---
    core_keywords = set(CFG.get("pro_topic_core_keywords", []))
    min_core_keyword_match = 2

    if not docs or not core_keywords:
        print("[DEBUG][C] PRO ìƒì„± ì™„ë£Œ | topics=0 (docs or core_keywords empty)")
        return {"topics": []}

    filtered_docs = []
    for doc in docs:
        doc_lower = doc.lower()
        match_count = sum(1 for keyword in core_keywords if keyword in doc_lower)
        if match_count >= min_core_keyword_match:
            filtered_docs.append(doc)
    
    print(f"[DEBUG][C] Core Keyword Filtering: {len(docs)} -> {len(filtered_docs)} docs")

    if len(filtered_docs) < 10:
        print(f"[WARN] Pro í† í”½ ë¶„ì„ì„ ìœ„í•œ ë¬¸ì„œ ìˆ˜ê°€ ë¶€ì¡±í•˜ì—¬({len(filtered_docs)}ê°œ), Liteë¡œ í´ë°±í•©ë‹ˆë‹¤.")
        return build_topics_lite(docs, topn=topn)
    # --- ì£¼ì œ í•„í„°ë§ ë¡œì§ ë ---

    emb = SentenceTransformer("jhgan/ko-sroberta-multitask")
    vectorizer_model = CountVectorizer(
        ngram_range=(1,3),
        min_df=2,
        token_pattern=r"[ê°€-í£A-Za-z0-9_]{2,}",
        stop_words=list(set(EN_STOP)|set(KO_FUNC))
    )
    rep = [KeyBERTInspired(top_n_words=15), MaximalMarginalRelevance(diversity=0.5)]
    
    min_topic_size_pro = int(CFG.get("pro_topic_min_size", 5))
    nr_topics_pro = CFG.get("pro_nr_topics", None)

    model = BERTopic(
        embedding_model=emb,
        vectorizer_model=vectorizer_model,
        representation_model=rep,
        min_topic_size=min_topic_size_pro,
        nr_topics=nr_topics_pro,
        calculate_probabilities=False,
        verbose=False
    )
    topics, probs = model.fit_transform(filtered_docs)
    
    try:
        # ì•„ë˜ docsë¥¼ filtered_docsë¡œ ìˆ˜ì •
        model.reduce_outliers(filtered_docs, topics, probabilities=probs, strategy="c-tf-idf", threshold=0.08)
    except Exception:
        pass
    try:
        # ì•„ë˜ docsë¥¼ filtered_docsë¡œ ìˆ˜ì •
        model.merge_topics(filtered_docs, topics, threshold=0.88)
    except Exception:
        pass
        
    topics_obj = {"topics": []}
    try:
        ctfidf = model.c_tf_idf_
        terms = model.vectorizer_model.get_feature_names_out()
        for tid in sorted(set(topics)):
            if tid == -1:
                continue
            vec = ctfidf[tid].toarray().ravel()
            idx = vec.argsort()[::-1][:max(40, topn)]
            pairs = [(terms[i], float(vec[i])) for i in idx]

            kept = []
            for w, s in pairs:
                base = w.split()[0] if " " in w else w
                if is_bad_token(base):
                    continue
                kept.append((w, s))
            if not kept:
                kept = pairs[:]

            scores = [max(float(s or 0.0), 0.0) for _, s in kept]
            maxv = max(scores) if scores else 0.0
            payload = []
            if maxv > 0 and (max(scores) - min(scores)) > 1e-12:
                for (w, s) in kept[:topn]:
                    prob = max(float(s or 0.0), 0.0) / maxv
                    payload.append({"word": w, "prob": prob})
            else:
                decay = 0.95
                for rank, (w, _s) in enumerate(kept[:topn], start=0):
                    prob = max(0.2, decay**rank)
                    payload.append({"word": w, "prob": prob})

            topics_obj["topics"].append({"topic_id": int(tid), "top_words": payload[:topn]})
        print(f"[DEBUG][C] PRO ìƒì„± ì™„ë£Œ | topics={len(topics_obj.get('topics', []))}")
        return topics_obj

    except Exception:
        # í´ë°±: get_topics ì‚¬ìš©(ì—¬ê¸°ë„ prob ë³´ì¥)
        topic_info = model.get_topics()
        for tid, items in topic_info.items():
            if tid == -1:
                continue
            head = items[:max(40, topn)]
            kept = []
            for w, s in head:
                base = w.split()[0] if " " in w else w
                if is_bad_token(base):
                    continue
                kept.append((w, float(s or 0.0)))
            if not kept:
                kept = [(w, float(s or 0.0)) for w, s in head]
            scores = [max(float(s or 0.0), 0.0) for _, s in kept]
            maxv = max(scores) if scores else 0.0
            payload = []
            if maxv > 0 and (max(scores) - min(scores)) > 1e-12:
                for (w, s) in kept[:topn]:
                    prob = max(float(s or 0.0), 0.0) / maxv
                    payload.append({"word": w, "prob": prob})
            else:
                decay = 0.95
                for rank, (w, _s) in enumerate(kept[:topn], start=0):
                    prob = max(0.2, decay**rank)
                    payload.append({"word": w, "prob": prob})
            topics_obj["topics"].append({"topic_id": int(tid), "top_words": payload[:topn]})
        print(f"[DEBUG][C] PRO ìƒì„± ì™„ë£Œ(í´ë°±) | topics={len(topics_obj.get('topics', []))}")
        return topics_obj

# ================= LLM í™œìš© í† í”½ ë³´ê°• =================
def enrich_topics_with_llm(topics_obj: dict, api_key: str, model: str) -> dict:
    import google.generativeai as genai
    import re

    genai.configure(api_key=api_key)
    gmodel = genai.GenerativeModel(model)

    enriched = []
    for t in topics_obj.get("topics", []):
        words = [w["word"] for w in t.get("top_words", [])]
        prompt = (
            f"ë‹¤ìŒì€ ë‰´ìŠ¤ì—ì„œ ì¶”ì¶œëœ í‚¤ì›Œë“œì…ë‹ˆë‹¤: {', '.join(words)}\n"
            "ì´ í‚¤ì›Œë“œë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ì´ í† í”½ì˜ ì´ë¦„ê³¼ ê°„ë‹¨í•œ í•´ì„ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.\n"
            "ì¡°ê±´:\n"
            "- topic_nameì€ í•µì‹¬ë§Œ ë‹´ì•„ 10ê¸€ì ì´ë‚´ë¡œ ìƒì„±í•˜ì„¸ìš” (ìë¥´ì§€ ë§ê³  ì²˜ìŒë¶€í„° ì§§ê²Œ)\n"
            "- topic_summaryëŠ” ì´ í† í”½ì´ ë‹¤ë£¨ëŠ” ë‚´ìš©ì„ ê°„ê²°í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”\n"
            "í˜•ì‹:\n"
            "topic_name: <ì´ë¦„>\n"
            "topic_summary: <í•´ì„>"
        )
        try:
            resp = gmodel.generate_content(prompt)
            text = resp.text.strip()
            name_match = re.search(r"topic_name:\s*(.+)", text)
            summary_match = re.search(r"topic_summary:\s*(.+)", text)

            t["topic_name"] = name_match.group(1).strip() if name_match else f"Topic #{t['topic_id']}"
            t["topic_summary"] = summary_match.group(1).strip() if summary_match else ""
        except Exception as e:
            t["topic_name"] = f"Topic #{t['topic_id']}"
            t["topic_summary"] = "(LLM í•´ì„ ì‹¤íŒ¨)"
        enriched.append(t)

    return {"topics": enriched}


# ================= ì¸ì‚¬ì´íŠ¸ ìš”ì•½ =================
def gemini_insight(api_key: str, model: str, context: Dict[str, Any],
                   max_tokens: int = 2048, temperature: float = 0.3) -> str:
    
    prompt = (
    "ì•„ë˜ëŠ” í•œêµ­ì–´ ê¸°ìˆ /ì‹œì¥ ë‰´ìŠ¤ì—ì„œ ì¶”ì¶œí•œ ë°ì´í„°ì…ë‹ˆë‹¤: (1) ì£¼ìš” í† í”½, (2) ìµœìƒìœ„ í‚¤ì›Œë“œ.\n"
    "ë‹¹ì‹ ì€ ë””ìŠ¤í”Œë ˆì´ ì‚°ì—…ì˜ ì „ëµ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì´ ë°ì´í„°ë¥¼ ì¢…í•©í•˜ì—¬ 'ë°ì¼ë¦¬ ì¸í…”ë¦¬ì „ìŠ¤ ë¸Œë¦¬í•‘'ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.\n\n"
    "ìš”ì²­:\n"
    "1. í•µì‹¬ë§¥ë½: í† í”½ê³¼ í‚¤ì›Œë“œë¥¼ ì—°ê²°í•˜ì—¬, ì‹œì¥ì˜ ê°€ì¥ ì¤‘ìš”í•œ íë¦„ 2~3ê°€ì§€ì˜ ë°°ê²½ì„ ë…¼ë¦¬ì ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.\n"
    "2. ì¸ì‚¬ì´íŠ¸: í˜„ì¬ ì‚°ì—…ì˜ ì „ëµì  ì‹œì‚¬ì (ê¸°íšŒ, ìœ„í—˜, ê²½ìŸ êµ¬ë„ ë“±)ì„ ë„ì¶œí•˜ì„¸ìš”.\n"
    "ì£¼ì˜:\n"
    "- ê° í•­ëª©ì€ ëª…í™•í•œ ì œëª©ê³¼ í•¨ê»˜ êµ¬ë¶„í•˜ì—¬ ì‘ì„±í•˜ì„¸ìš”.\n"
    "- ë¬¸ì¥ì€ ê°„ê²°í•˜ê³  ì™„ê²°ì„± ìˆê²Œ ì‘ì„±í•˜ì„¸ìš”.\n\n"
    f"ë°ì´í„°: {json.dumps({'topics': context.get('topics', []), 'keywords': context.get('keywords', [])}, ensure_ascii=False)}"
    )

    if not api_key:
        return "(ì¸ì‚¬ì´íŠ¸ ë„ì¶œ ì‹¤íŒ¨) API í‚¤ê°€ ì—†ì–´ Gemini ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    try:
        import google.generativeai as genai
        genai.configure(api_key=api_key)
        gmodel = genai.GenerativeModel(model or "gemini-1.5-flash")
        resp = gmodel.generate_content(
            prompt,
            generation_config={
                "max_output_tokens": max_tokens or 2048,
                "temperature": temperature if temperature is not None else 0.3,
                "top_p": 0.9
            }
        )
        text = (getattr(resp, "text", None) or "").strip()
        if not text:
            raise RuntimeError("ë¹ˆ ì‘ë‹µ")

        # ìì—°ìŠ¤ëŸ¬ìš´ ë§ˆë¬´ë¦¬ ë³´ì™„
        if not re.search(r"[\.!?]$|[ë‹¤ìš”]$", text):
            try:
                resp2 = gmodel.generate_content(
                    text + "\n\nìœ„ ìš”ì•½ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë§ˆë¬´ë¦¬í•´ ì£¼ì„¸ìš”.",
                    generation_config={"max_output_tokens": 256, "temperature": temperature or 0.3, "top_p": 0.9}
                )
                add = (getattr(resp2, "text", None) or "").strip()
                if add:
                    text = (text + " " + add).strip()
            except Exception:
                pass

        return text

    except Exception as e:
        return f"(ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}) í‚¤ì›Œë“œì™€ í† í”½ ê¸°ë°˜ìœ¼ë¡œ ìš°ì„  ê³¼ì œë¥¼ ì •ë¦¬í•˜ì„¸ìš”."


# ================= ë©”ì¸ =================
def main():
    _log_mode("Module C")
    os.makedirs("outputs", exist_ok=True)

    api_key = os.getenv("GEMINI_API_KEY", "")
    model_name = str(LLM.get("model", "gemini-2.0-flash"))

    is_weekly_run = os.getenv("WEEKLY_RUN", "false").lower() == "true"
    is_monthly_run = os.getenv("MONTHLY_RUN", "false").lower() == "true"

    # --- â–¼â–¼â–¼â–¼â–¼ [ìˆ˜ì •] ì£¼ê°„/ì›”ê°„/ì¼ê°„ì— ë”°ë¼ ë°ì´í„° ë¡œë“œ ê²½ë¡œ ë³€ê²½ â–¼â–¼â–¼â–¼â–¼ ---
    if is_monthly_run:
        meta_path = "outputs/debug/monthly_meta_agg.json"
        print(f"[INFO] Monthly Run: Using aggregated meta file for {__name__}.")
    elif is_weekly_run: # ğŸ‘ˆ ì£¼ê°„ ì‹¤í–‰ ë¡œì§ ì¶”ê°€
        meta_path = "outputs/debug/weekly_meta_agg.json"
        print(f"[INFO] Weekly Run: Using aggregated meta file for {__name__}.")
    else: # ì¼ê°„ ì‹¤í–‰
        meta_path = "outputs/debug/news_meta_latest.json"
        if not os.path.exists(meta_path):
            meta_path = latest("data/news_meta_*.json")

    if not meta_path or not os.path.exists(meta_path):
        raise SystemExit(f"Input meta file not found for Module C.")
    
    print(f"[INFO] Module C loading meta data from: {meta_path}")
    items = load_json(meta_path, [])
    
    # load_today_meta() í•¨ìˆ˜ ëŒ€ì‹  main í•¨ìˆ˜ì—ì„œ ì§ì ‘ docs ìƒì„±
    docs_today = []
    for it in items:
        title = clean_text((it.get("title") or it.get("title_og") or "").strip())
        desc  = clean_text((it.get("body") or it.get("description") or it.get("description_og") or "").strip())
        doc = (title + " " + desc).strip()
        if doc:
            docs_today.append(doc)
    # --- â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–² ---

    warehouse_paths = load_warehouse_paths(days=30)
    ts_obj = calculate_stable_timeseries(warehouse_paths)
    save_json("outputs/trend_timeseries.json", ts_obj)

    try:
        if use_pro_mode():
            topics_obj = pro_build_topics_bertopic(docs_today or [], topn=10)
        else:
            topics_obj = build_topics_lite(docs_today or [], max_features=8000, topn=10)
    except Exception as e:
        print(f"[WARN] Pro í† í”½ ì‹¤íŒ¨, Liteë¡œ í´ë°±: {e}")
        topics_obj = build_topics_lite(docs_today or [], max_features=8000, topn=10)

    keywords_obj = load_json("outputs/keywords.json", {"keywords": []})
    top_keywords = [k.get("keyword") for k in keywords_obj.get("keywords", [])[:10]]

    # --- â–¼â–¼â–¼â–¼â–¼ [ìˆ˜ì •] ì£¼ê°„/ì›”ê°„ ì‹¤í–‰ ì‹œì—ë§Œ LLM ë¶„ì„ ìˆ˜í–‰ â–¼â–¼â–¼â–¼â–¼ ---
    if is_weekly_run or is_monthly_run:
        print(f"[INFO] Weekly/Monthly Run: Enriching topics and generating insights with LLM.")
        # 4-1. LLMìœ¼ë¡œ í† í”½ ì´ë¦„/ìš”ì•½ ìƒì„±
        topics_obj = _ensure_prob_payload(topics_obj, topn=10)
        topics_obj = enrich_topics_with_llm(topics_obj, api_key=api_key, model=model_name)

        # 4-2. í† í”½ ìš”ì•½ ì •ë³´ êµ¬ì„±
        top_topics = []
        for t in topics_obj.get("topics", []):
            top_topics.append({
                "topic_id": t.get("topic_id"), "topic_name": t.get("topic_name"),
                "summary": t.get("topic_summary", ""), "words": [w.get("word", "") for w in (t.get("top_words") or [])[:5]]
            })
        
        # 4-3. LLMìœ¼ë¡œ ì¢…í•© ì¸ì‚¬ì´íŠ¸ ìƒì„±
        summary = gemini_insight(
            api_key=api_key, model=model_name,
            context={"topics": top_topics, "keywords": top_keywords},
            max_tokens=int(LLM.get("max_output_tokens", 2048)),
            temperature=float(LLM.get("temperature", 0.3)),
        )
        insights_obj = {"summary": summary, "top_topics": top_topics, "evidence": {}}

    else: # ì¼ê°„ ì‹¤í–‰ì¼ ê²½ìš°
        print("[INFO] Daily Run: Skipping LLM enrichment and insights.")
        topics_obj = _ensure_prob_payload(topics_obj, topn=10) # LLM ì—†ì´ probë§Œ ë³´ì¥
        top_topics = []
        for t in topics_obj.get("topics", []): # ì´ë¦„ ì—†ëŠ” í† í”½ ì •ë³´ êµ¬ì„±
            top_topics.append({
                "topic_id": t.get("topic_id"), "topic_name": f"Topic #{t.get('topic_id')}",
                "summary": "", "words": [w.get("word", "") for w in (t.get("top_words") or [])[:5]]
            })
        # LLM ìš”ì•½ ëŒ€ì‹  í”Œë ˆì´ìŠ¤í™€ë” ë©”ì‹œì§€ ì €ì¥
        insights_obj = {
            "summary": "ì¼ê°„ ì‹¤í–‰ì—ì„œëŠ” LLM ìš”ì•½ì´ ìƒì„±ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
            "top_topics": top_topics, "evidence": {}
        }
    # --- â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–²â–² ---

    # 5. ê²°ê³¼ë¬¼ ì €ì¥
    save_json("outputs/topics.json", topics_obj)
    save_json("outputs/trend_insights.json", insights_obj)

    meta = {"module": "C", "mode": "PRO" if use_pro_mode() else "LITE", "time_utc": datetime.datetime.utcnow().isoformat() + "Z"}
    save_json("outputs/debug/run_meta_c.json", meta)

    print("[INFO] Module C done | topics=%d | LLM Called: %s" % (
        len(topics_obj.get("topics", [])), str(is_weekly_run or is_monthly_run)))
        

if __name__ == "__main__":
    main()
