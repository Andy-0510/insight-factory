import os
import json
import re
import glob
from pathlib import Path
from datetime import datetime, timedelta
import pandas as pd
from collections import defaultdict, Counter
from wordcloud import WordCloud
import matplotlib.font_manager as fm
import matplotlib.pyplot as plt
import seaborn as sns
from src.utils import load_json, save_json, latest
from src.config import load_config # LLM í˜¸ì¶œì„ ìœ„í•´ ì¶”ê°€

try:
    from .daily_report import (_fmt_int, _safe_read_csv, _to_markdown_table, _section_header, build_html_from_md_new, _exists, _insert_images)
except ImportError:
    from daily_report import (_fmt_int, _safe_read_csv, _to_markdown_table, _section_header, build_html_from_md_new, _exists, _insert_images)

# --- ì„¤ì • ---
ROOT_OUTPUT_DIR = "outputs"
DAILY_ARCHIVE_DIR = os.path.join(ROOT_OUTPUT_DIR, "daily")
FIG_DIR = os.path.join(ROOT_OUTPUT_DIR, "fig")
OUT_MD = os.path.join(ROOT_OUTPUT_DIR, "weekly_report.md")
OUT_HTML = os.path.join(ROOT_OUTPUT_DIR, "weekly_report.html")
TARGET_COMPETITORS = ["ì‚¼ì„±ë””ìŠ¤í”Œë ˆì´", "LGë””ìŠ¤í”Œë ˆì´", "BOE", "CSOT", "Visionox", "Tianma"]

# --- â–¼â–¼â–¼â–¼â–¼â–¼ ì£¼ê°„ ê²½ì˜ ìš”ì•½ì„ ìœ„í•œ LLM í˜¸ì¶œ í•¨ìˆ˜ â–¼â–¼â–¼â–¼â–¼â–¼ ---
def call_gemini_for_weekly_summary(context):
    """LLMì„ í˜¸ì¶œí•˜ì—¬ ì£¼ê°„ ê²½ì˜ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        import google.generativeai as genai
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key: raise RuntimeError("GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        genai.configure(api_key=api_key)
        cfg = load_config()
        model_name = cfg.get("llm", {}).get("model", "gemini-1.5-flash-001")
        model = genai.GenerativeModel(model_name)
        print(f"[INFO] Using Gemini model for weekly summary: {model_name}")

        prompt = f"""
        ë‹¹ì‹ ì€ ë””ìŠ¤í”Œë ˆì´ ì‚°ì—… ì „ë¬¸ ìˆ˜ì„ ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì•„ë˜ëŠ” ì§€ë‚œ í•œ ì£¼ê°„ì˜ ì‹œì¥ ë°ì´í„° ìš”ì•½ì…ë‹ˆë‹¤. ì´ ë°ì´í„°ë¥¼ ì¢…í•©í•˜ì—¬ ê²½ì˜ì§„ ë° íŒ€ ë¦¬ë”ë¥¼ ìœ„í•œ 'ì£¼ê°„ ì¸í…”ë¦¬ì „ìŠ¤ ìš”ì•½'ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

        ### ì£¼ê°„ ë°ì´í„° ìš”ì•½:
        {json.dumps(context, ensure_ascii=False, indent=2)}

        ### ì‘ì„± ê°€ì´ë“œ:
        1. **í•µì‹¬ ë§¥ë½**: ë°ì´í„°ë¥¼ ê´€í†µí•˜ëŠ” ê°€ì¥ ì¤‘ìš”í•œ ì‹œì¥ì˜ íë¦„ 1~2ê°€ì§€ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”.
        2. **ì „ëµì  ì¸ì‚¬ì´íŠ¸**: ì´ íë¦„ì´ ìš°ë¦¬ ë¹„ì¦ˆë‹ˆìŠ¤ì— ì£¼ëŠ” ê¸°íšŒ ë˜ëŠ” ìœ„í˜‘ ìš”ì†Œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”.
        3. **ì¶”ì²œ Action Items**: ë‹¤ìŒ ì£¼ì— íŒ€ì´ ìš°ì„ ì ìœ¼ë¡œ ì‹¤í–‰í•´ì•¼ í•  êµ¬ì²´ì ì¸ ì•¡ì…˜ ì•„ì´í…œ 2ê°€ì§€ë¥¼ ì œì•ˆí•´ì£¼ì„¸ìš”.
        4. ê° í•­ëª©ì„ ëª…í™•í•˜ê²Œ êµ¬ë¶„í•˜ê³ , ì „ë¬¸ê°€ì˜ ì‹œê°ì—ì„œ ê°„ê²°í•˜ê³  ëª…í™•í•œ í†¤ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.

        ### ì¶œë ¥ í˜•ì‹ (Markdown):
        #### í•µì‹¬ ë§¥ë½
        - (ë¶„ì„ ë‚´ìš©)

        #### ì „ëµì  ì¸ì‚¬ì´íŠ¸
        - (ë¶„ì„ ë‚´ìš©)

        #### ì¶”ì²œ Action Items
        - (ì‹¤í–‰ ì œì•ˆ 1)
        - (ì‹¤í–‰ ì œì•ˆ 2)
        """
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        print(f"[ERROR] Gemini ì£¼ê°„ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")
        return "LLM ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

# --- â–¼â–¼â–¼â–¼â–¼â–¼ ì£¼ê°„ ì•½í•œ ì‹ í˜¸ ë¶„ì„ì„ ìœ„í•œ LLM í˜¸ì¶œ í•¨ìˆ˜ â–¼â–¼â–¼â–¼â–¼â–¼ ---
def call_gemini_for_weekly_insight(weak_signals):
    """LLMì„ í˜¸ì¶œí•˜ì—¬ ì£¼ê°„ ì•½í•œ ì‹ í˜¸ì˜ ì˜ë¯¸ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
    if not weak_signals:
        return "ê¸ˆì£¼ì— ì£¼ëª©í•  ë§Œí•œ ì‹ ê·œ ì•½í•œ ì‹ í˜¸ê°€ í¬ì°©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    try:
        import google.generativeai as genai
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key: raise RuntimeError("GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        genai.configure(api_key=api_key)
        cfg = load_config()
        model_name = cfg.get("llm", {}).get("model", "gemini-1.5-flash-001")
        model = genai.GenerativeModel(model_name)
        print(f"[INFO] Using Gemini model for weekly weak signal insight: {model_name}")

        prompt = f"""
        ë‹¹ì‹ ì€ ë¯¸ë˜ ê¸°ìˆ  íŠ¸ë Œë“œ ë¶„ì„ê°€ì…ë‹ˆë‹¤. ì•„ë˜ëŠ” ì§€ë‚œ í•œ ì£¼ê°„ í¬ì°©ëœ ì´ˆê¸° ì‹ í˜¸(Weak Signals) ëª©ë¡ì…ë‹ˆë‹¤.

        ### ì£¼ê°„ ì´ˆê¸° ì‹ í˜¸ ëª©ë¡:
        {json.dumps(weak_signals, ensure_ascii=False, indent=2)}

        ### ë¶„ì„ ìš”ì²­:
        1. ëª©ë¡ì—ì„œ ê°€ì¥ ì¤‘ìš”í•˜ê³  ì ì¬ë ¥ ìˆëŠ” ì‹ í˜¸ 2~3ê°œë¥¼ ì„ ë³„í•´ì£¼ì„¸ìš”.
        2. ê° ì‹ í˜¸ê°€ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ì§€, ê·¸ë¦¬ê³  ì™œ ì§€ê¸ˆ ì£¼ëª©í•´ì•¼ í•˜ëŠ”ì§€ì— ëŒ€í•œ í•´ì„ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.
        3. ë¶„ì„ ê²°ê³¼ë¥¼ ì•„ë˜ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš”. ë‹¤ë¥¸ ì„¤ëª…ì€ í•„ìš” ì—†ìŠµë‹ˆë‹¤.

        ### ì¶œë ¥ í˜•ì‹ (Markdown):
        - **[ì‹ í˜¸ëª… 1]:** (ë¶„ì„ ë° í•´ì„ ìš”ì•½)
        - **[ì‹ í˜¸ëª… 2]:** (ë¶„ì„ ë° í•´ì„ ìš”ì•½)
        """
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        print(f"[ERROR] Gemini ì£¼ê°„ ì•½í•œ ì‹ í˜¸ ë¶„ì„ ì‹¤íŒ¨: {e}")
        return "LLM ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

def load_weekly_data(days=7):
    print(f"[INFO] Loading data from the last {days} days...")
    aggregated_data = {
        "start_date": "", "end_date": "", "total_articles": 0, "all_keywords": [],
        "all_events": pd.DataFrame(),
        "trend_strength_history": defaultdict(list),
        "all_weak_signals": pd.DataFrame() # ì•½í•œ ì‹ í˜¸ ë°ì´í„°í”„ë ˆì„ ì¶”ê°€
    }
    
    end_date = datetime.now()
    start_date = end_date - timedelta(days=days - 1)
    aggregated_data["start_date"] = start_date.strftime("%Y-%m-%d")
    aggregated_data["end_date"] = end_date.strftime("%Y-%m-%d")

    for i in range(days):
        current_date = start_date + timedelta(days=i)
        date_str = current_date.strftime("%Y-%m-%d")
        date_folders = sorted(glob.glob(os.path.join(DAILY_ARCHIVE_DIR, date_str, "*")))
        if not date_folders: continue
        
        latest_daily_folder = date_folders[-1]
        print(f"[DEBUG] Loading from: {latest_daily_folder}")

        ts_path = os.path.join(latest_daily_folder, "trend_timeseries.json")
        kw_path = os.path.join(latest_daily_folder, "keywords.json")
        events_path = os.path.join(latest_daily_folder, "export", "events.csv")
        trends_path = os.path.join(latest_daily_folder, "export", "trend_strength.csv")
        weak_signals_path = os.path.join(latest_daily_folder, "export", "weak_signals.csv")
        
        ts_data = load_json(ts_path, {"daily": []}) if os.path.exists(ts_path) else {"daily": []}
        keywords_data = load_json(kw_path, {"keywords": []}) if os.path.exists(kw_path) else {"keywords": []}
        events_df = _safe_read_csv(events_path)
        trends_df = _safe_read_csv(trends_path)
        trends_df = _safe_read_csv(os.path.join(latest_daily_folder, "export", "trend_strength.csv"))
        weak_signals_df = _safe_read_csv(weak_signals_path)

        if ts_data and ts_data.get("daily"):
            today_count = next((d['count'] for d in reversed(ts_data['daily']) if d['date'] == date_str), 0)
            aggregated_data["total_articles"] += today_count
        if keywords_data and keywords_data.get("keywords"):
            aggregated_data["all_keywords"].extend(keywords_data["keywords"])
        if not events_df.empty:
            aggregated_data["all_events"] = pd.concat([aggregated_data["all_events"], events_df], ignore_index=True)
        if not trends_df.empty:
            for _, row in trends_df.iterrows():
                aggregated_data["trend_strength_history"][row['term']].append({
                    "date": date_str,
                    "cur": row.get('cur', 0),
                    "z_like": row.get('z_like', 0.0)
                })
        if not weak_signals_df.empty:
            aggregated_data["all_weak_signals"] = pd.concat([aggregated_data["all_weak_signals"], weak_signals_df], ignore_index=True)
        return aggregated_data

def generate_weekly_visuals(data):
    """ì£¼ê°„ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹œê°í™” ìë£Œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    print("[INFO] Generating weekly visuals...")
    os.makedirs(FIG_DIR, exist_ok=True)
    
    # 1. ì£¼ê°„ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
    keyword_scores = defaultdict(float)
    
    # --- ğŸ’¡ [ìˆ˜ì •ëœ ë¡œì§] í‚¤ì›Œë“œ ì ìˆ˜ë¥¼ ì˜¬ë°”ë¥´ê²Œ í•©ì‚°í•©ë‹ˆë‹¤. ---
    for k_data in data['all_keywords']:
        keyword_scores[k_data['keyword']] += k_data.get('score', 0.0)
    
    if keyword_scores:
        # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ë¡œì§
        font_paths = fm.findSystemFonts(fontpaths=None, fontext='ttf')
        font_path = next((p for p in font_paths if 'NanumGothic' in p or 'NotoSansKR' in p), None)
        
        wc = WordCloud(
            width=1600, height=900, background_color="white",
            colormap="tab20c", font_path=font_path,
            relative_scaling=0.4, random_state=42,
            collocations=False
        ).generate_from_frequencies(dict(keyword_scores))
        
        output_path = os.path.join(FIG_DIR, "weekly_wordcloud.png")
        wc.to_file(output_path)
        print(f"[INFO] Weekly wordcloud saved.")

    # 2. ì£¼ê°„ ìƒìŠ¹/í•˜ê°• ì‹ í˜¸ ë°”ì°¨íŠ¸ ìƒì„± (ê¸°ì¡´ê³¼ ë™ì¼)
    weekly_trends = []
    for term, history in data["trend_strength_history"].items():
        if len(history) > 1:
            avg_z_like = sum(d['z_like'] for d in history) / len(history)
            weekly_trends.append({"term": term, "weekly_avg_z_like": avg_z_like})
    
    if weekly_trends:
        df_trends = pd.DataFrame(weekly_trends).sort_values(by="weekly_avg_z_like", ascending=False)
        rising = df_trends[df_trends['weekly_avg_z_like'] > 0].head(5)
        falling = df_trends[df_trends['weekly_avg_z_like'] < 0].tail(5)
        combined = pd.concat([rising, falling])
        
        plt.figure(figsize=(12, 8))
        font_paths = fm.findSystemFonts(fontpaths=None, fontext='ttf')
        font_path = next((path for path in font_paths if 'NanumGothic' in path or 'NotoSansKR' in path), None)
        if font_path: plt.rcParams['font.family'] = fm.FontProperties(fname=font_path).get_name()
        plt.rcParams['axes.unicode_minus'] = False

        sns.barplot(data=combined, y="term", x="weekly_avg_z_like",
                    palette=["#3b82f6" if x > 0 else "#ef4444" for x in combined['weekly_avg_z_like']])
        plt.title('ì£¼ê°„ í•µì‹¬ ì‹ í˜¸ ëª¨ë©˜í…€ (ìƒìŠ¹/í•˜ê°• Top 5)', fontsize=16)
        plt.xlabel('ì£¼ê°„ í‰ê·  ëª¨ë©˜í…€ (z_like)', fontsize=12)
        plt.ylabel('')
        plt.tight_layout()
        plt.savefig(os.path.join(FIG_DIR, "weekly_strong_signals_barchart.png"), dpi=150)
        plt.close()
        print(f"[INFO] Weekly strong signals barchart saved.")

# --- â–¼â–¼â–¼â–¼â–¼â–¼ ì£¼ê°„ ê²½ì˜ ìš”ì•½ ì„¹ì…˜ ìµœì¢… êµ¬í˜„ â–¼â–¼â–¼â–¼â–¼â–¼ ---
def _section_weekly_summary(data):
    """ì£¼ê°„ ê²½ì˜ ìš”ì•½ ì„¹ì…˜"""
    
    # LLMì— ì „ë‹¬í•  ì»¨í…ìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
    keyword_scores = defaultdict(float)
    for k in data['all_keywords']: keyword_scores[k['keyword']] += k.get('score', 0.0)
    top_keywords = [k for k, v in sorted(keyword_scores.items(), key=lambda item: item[1], reverse=True)[:5]]

    competitor_mentions = Counter()
    for term in TARGET_COMPETITORS:
        history = data["trend_strength_history"].get(term, [])
        competitor_mentions[term] = sum(d.get('cur', 0) for d in history)
    top_competitors = [c for c, v in competitor_mentions.most_common(3) if v > 0]
    
    top_weak_signals = data["all_weak_signals"].sort_values(by="z_like", ascending=False).drop_duplicates(subset=['term']).head(3)['term'].tolist()

    context = {
        "ë¶„ì„ ê¸°ê°„": f"{data['start_date']} ~ {data['end_date']}",
        "ì£¼ê°„ Top í‚¤ì›Œë“œ": top_keywords,
        "ì£¼ê°„ í™œë™ëŸ‰ Top ê²½ìŸì‚¬": top_competitors,
        "ì£¼ëª©í•  ë§Œí•œ ì•½í•œ ì‹ í˜¸": top_weak_signals
    }
    
    # LLM í˜¸ì¶œí•˜ì—¬ ìš”ì•½ ìƒì„±
    llm_summary = call_gemini_for_weekly_summary(context)
    
    # ê¸°ë³¸ í†µê³„ ì •ë³´
    basic_stats = f"""
- **ë¶„ì„ ê¸°ê°„:** {data['start_date']} ~ {data['end_date']}
- **ì´ ë¶„ì„ ê¸°ì‚¬ ìˆ˜:** {_fmt_int(data['total_articles'])}
- **ì£¼ìš” ì´ë²¤íŠ¸ ë°œìƒ ê±´ìˆ˜:** {_fmt_int(len(data['all_events'].drop_duplicates(subset=['title'])))}
"""
    
    return basic_stats + "\n" + llm_summary

# --- â–¼â–¼â–¼â–¼â–¼â–¼ ì‹œì¥ í…Œë§ˆ ë° ê±°ì‹œì  íë¦„ ë¶„ì„ ì„¹ì…˜ êµ¬í˜„ â–¼â–¼â–¼â–¼â–¼â–¼ ---
def _section_weekly_market_themes(data):
    keyword_scores = defaultdict(float)
    for k in data['all_keywords']: keyword_scores[k['keyword']] += k['score']
    top_10_keywords = sorted(keyword_scores.items(), key=lambda item: item[1], reverse=True)[:10]
    df_top_keywords = pd.DataFrame(top_10_keywords, columns=["í‚¤ì›Œë“œ", "ì£¼ê°„ ëˆ„ì  ì ìˆ˜"])
    df_top_keywords["ì£¼ê°„ ëˆ„ì  ì ìˆ˜"] = df_top_keywords["ì£¼ê°„ ëˆ„ì  ì ìˆ˜"].apply(lambda x: round(x, 2))
    lines = [_to_markdown_table(df_top_keywords)]
    image_path = os.path.join(FIG_DIR, "weekly_wordcloud.png")
    lines.append(_insert_images(image_path, OUT_MD, captions=["ì£¼ê°„ í‚¤ì›Œë“œ ì›Œë“œí´ë¼ìš°ë“œ"]))
    return "\n".join(lines)

# --- â–¼â–¼â–¼â–¼â–¼â–¼ ê²½ìŸì‚¬ ë™í–¥ ë¶„ì„ ì„¹ì…˜ êµ¬í˜„ â–¼â–¼â–¼â–¼â–¼â–¼ ---
def _section_weekly_competitor_trends(data):
    """ì£¼ê°„ ê²½ìŸ ë™í–¥ ë¶„ì„ ì„¹ì…˜"""
    competitor_stats = []
    
    for competitor in TARGET_COMPETITORS:
        history = data["trend_strength_history"].get(competitor, [])
        if history:
            weekly_mentions = sum(day.get('cur', 0) for day in history)
            avg_z_like = sum(day.get('z_like', 0.0) for day in history) / len(history)
            competitor_stats.append({
                "ê²½ìŸì‚¬": competitor,
                "ì£¼ê°„ ì´ ì–¸ê¸‰ëŸ‰": weekly_mentions,
                "ì£¼ê°„ í‰ê·  ëª¨ë©˜í…€": round(avg_z_like, 2)
            })

    if not competitor_stats:
        return "> ê¸ˆì£¼ì— ê°ì§€ëœ ì£¼ìš” ê²½ìŸì‚¬ í™œë™ì´ ì—†ìŠµë‹ˆë‹¤."

    df_competitors = pd.DataFrame(competitor_stats)
    df_competitors = df_competitors.sort_values(by="ì£¼ê°„ ì´ ì–¸ê¸‰ëŸ‰", ascending=False)
    
    return _to_markdown_table(df_competitors)

# --- â–¼â–¼â–¼â–¼â–¼â–¼ ì ì¬ì  ë¯¸ë˜ ì„±ì¥ ë™ë ¥ ì„¹ì…˜ êµ¬í˜„ â–¼â–¼â–¼â–¼â–¼â–¼ ---
def _section_weekly_future_signals(data):
    """ì£¼ê°„ ë¯¸ë˜ ì‹ í˜¸ ë°œê²¬ ì„¹ì…˜"""
    df_weak = data["all_weak_signals"]
    if df_weak.empty:
        return "> ê¸ˆì£¼ì— í¬ì°©ëœ ì‹ ê·œ ì•½í•œ ì‹ í˜¸ê°€ ì—†ìŠµë‹ˆë‹¤."
        
    # ì£¼ê°„ ë™ì•ˆ ë‚˜íƒ€ë‚œ ì•½í•œ ì‹ í˜¸ë¥¼ z_like ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ 5ê°œ ì„ ì •
    top_weak_signals = df_weak.sort_values(by="z_like", ascending=False).drop_duplicates(subset=['term']).head(5)
    
    # LLM ë¶„ì„ì„ ìœ„í•´ dict ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    weak_signals_for_llm = top_weak_signals[['term', 'z_like', 'total']].to_dict('records')
    
    # LLM í˜¸ì¶œí•˜ì—¬ ë¶„ì„ ê²°ê³¼ ë°›ê¸°
    llm_interpretation = call_gemini_for_weekly_insight(weak_signals_for_llm)
    
    lines = [_to_markdown_table(top_weak_signals[['term', 'cur', 'z_like', 'total']])]
    lines.append("\n**AI ê¸°ë°˜ ì£¼ìš” ì‹ í˜¸ í•´ì„:**\n")
    lines.append(llm_interpretation)
    
    return "\n".join(lines)

# --- â–¼â–¼â–¼â–¼â–¼â–¼ ëª¨ë©˜í…€ ë³€í™” ì„¹ì…˜ êµ¬í˜„ â–¼â–¼â–¼â–¼â–¼â–¼ ---
def _section_weekly_momentum_change(data):
    """ì£¼ìš” ì‹ í˜¸ ë³€í™” ì¶”ì´ ì„¹ì…˜"""
    
    # ì£¼ê°„ ì´ ì–¸ê¸‰ëŸ‰ ê¸°ì¤€ ìƒìœ„ 5ê°œ ì‹ í˜¸ ì„ ì •
    mention_counts = Counter()
    for term, history in data["trend_strength_history"].items():
        mention_counts[term] += sum(d['cur'] for d in history)
        
    top_5_terms = [term for term, count in mention_counts.most_common(5)]
    
    if not top_5_terms:
        return "> ê¸ˆì£¼ì— ì¶”ì í•  ë§Œí•œ í•µì‹¬ ì‹ í˜¸ê°€ ì—†ìŠµë‹ˆë‹¤."

    momentum_data = []
    for term in top_5_terms:
        history = data["trend_strength_history"].get(term, [])
        # ì¼ë³„ z_like ì ìˆ˜ë¥¼ ë¬¸ìì—´ë¡œ í‘œí˜„
        trend_str = " â†’ ".join([f"{d['z_like']:.1f}" for d in sorted(history, key=lambda x: x['date'])])
        momentum_data.append({
            "í•µì‹¬ ì‹ í˜¸": term,
            "ì£¼ê°„ ì´ ì–¸ê¸‰ëŸ‰": mention_counts[term],
            "ì¼ë³„ ëª¨ë©˜í…€(z_like) ì¶”ì´": trend_str
        })
        
    df_momentum = pd.DataFrame(momentum_data)

    lines = [_to_markdown_table(df_momentum)]
    lines.append(_insert_images(os.path.join(FIG_DIR, "weekly_strong_signals_barchart.png"), OUT_MD, captions=["ì£¼ê°„ ìƒìŠ¹/í•˜ê°• ì‹ í˜¸ Top 5"]))

    return "\n".join(lines)

def build_weekly_markdown():
    weekly_data = load_weekly_data(days=7)
    generate_weekly_visuals(weekly_data) # ì‹œê°í™” ìƒì„±ì€ ê·¸ëŒ€ë¡œ ìœ ì§€
    
    today_str = datetime.now().strftime("%Y-%m-%d")
    lines = [f"# Weekly Intelligence ({today_str})"]

    lines.append(_section_header("1. ì£¼ê°„ ê²½ì˜ ìš”ì•½ ë° ì „ëµì  ì‹œì‚¬ì ")); lines.append(_section_weekly_summary(weekly_data))
    lines.append(_section_header("2. ì£¼ê°„ ì‹œì¥ í…Œë§ˆ ë° ê±°ì‹œì  íë¦„ ë¶„ì„")); lines.append(_section_weekly_market_themes(weekly_data))
    lines.append(_section_header("3. ê²½ìŸì‚¬ í™œë™ ê°•ë„ ë° ì „ëµ ì „í™˜ ê²½ë³´")); lines.append(_section_weekly_competitor_trends(weekly_data))
    lines.append(_section_header("4. ì ì¬ì  ë¯¸ë˜ ì„±ì¥ ë™ë ¥ ë° ì´ˆê¸° ì‹ í˜¸ ë°œê²¬")); lines.append(_section_weekly_future_signals(weekly_data))
    lines.append(_section_header("5. í•µì‹¬ íŠ¸ë Œë“œ ëª¨ë©˜í…€ ë³€í™” ë° ìš°ì„ ìˆœìœ„ ê²€í† ")); lines.append(_section_weekly_momentum_change(weekly_data))
    with open(OUT_MD, "w", encoding="utf-8") as f: f.write("\n".join(lines))
    return OUT_MD

def main():
    try:
        md_path = build_weekly_markdown()
        build_html_from_md_new(md_path, OUT_HTML)
        print(f"[INFO] Weekly report generated: {md_path}, {OUT_HTML}")
    except Exception as e:
        import traceback; traceback.print_exc()
        print(f"[ERROR] Weekly report generation failed: {e}")

if __name__ == "__main__":
    main()