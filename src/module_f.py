import os
import json
import glob
import re
import datetime
from pathlib import Path
from scripts.plot_font import set_kr_font, get_kr_font_path
from wordcloud import WordCloud
from matplotlib import pyplot as plt
import pandas as pd
import matplotlib.dates as mdates
from datetime import datetime, timedelta
import seaborn as sns
import networkx as nx
from src.utils import load_json, save_json, latest


# 1) ì¼ë°˜ ê·¸ë˜í”„ í°íŠ¸ ì„¤ì •(ë‹¤ë¥¸ í”Œë¡¯ë„ í•œê¸€ OK)
_ = set_kr_font()

# 2) ì›Œë“œí´ë¼ìš°ë“œ ì „ìš© í°íŠ¸ ê²½ë¡œ
font_path = get_kr_font_path()
print(f"[INFO] wordcloud font_path: {font_path}")


def load_data():
    keywords = load_json("outputs/keywords.json", {"keywords": [], "stats": {}})
    topics = load_json("outputs/topics.json", {"topics": []})
    ts = load_json("outputs/trend_timeseries.json", {"daily": []})
    insights = load_json("outputs/trend_insights.json", {"summary": "", "top_topics": [], "evidence": {}})
    opps = load_json("outputs/biz_opportunities.json", {"ideas": []})
    meta_path = latest("data/news_meta_*.json")
    meta_items = load_json(meta_path, []) if meta_path else []
    return keywords, topics, ts, insights, opps, meta_items

def simple_tokenize_ko(text: str):
    toks = re.findall(r"[ê°€-í£A-Za-z0-9]+", text or "")
    toks = [t.lower() for t in toks if len(t) >= 2]
    return toks

def ensure_fonts():
    import os
    import matplotlib
    from matplotlib import font_manager
    candidates = [
        "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
        "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc",
        "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc",
        "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc",
    ]
    font_path = next((p for p in candidates if os.path.exists(p)), None)
    if font_path:
        font_manager.fontManager.addfont(font_path)
        font_name = font_manager.FontProperties(fname=font_path).get_name()
    else:
        font_name = "NanumGothic"
    matplotlib.rcParams["font.family"] = font_name
    matplotlib.rcParams["font.sans-serif"] = [font_name, "NanumGothic", "Noto Sans CJK KR", "Malgun Gothic", "AppleGothic", "DejaVu Sans"]
    matplotlib.rcParams["axes.unicode_minus"] = False
    try:
        from matplotlib import font_manager as fm
        fm._rebuild()
    except Exception:
        pass
    return font_name

def apply_plot_style():
    plt.rcParams.update({
        "figure.dpi": 150,
        "savefig.dpi": 150,
        "axes.titlesize": 12,
        "axes.labelsize": 11,
        "xtick.labelsize": 9,
        "ytick.labelsize": 9,
        "legend.fontsize": 9,
        "axes.grid": True,
        "grid.alpha": 0.25,
        "grid.linestyle": "--",
        "grid.linewidth": 0.6,
        "axes.edgecolor": "#999",
        "axes.linewidth": 0.8,
    })

def plot_wordcloud_from_keywords(keywords_obj, out_path="outputs/fig/wordcloud.png"):
    from wordcloud import WordCloud
    import matplotlib.pyplot as plt
    import os
    ensure_fonts()
    apply_plot_style()
    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    items = (keywords_obj or {}).get("keywords") or []
    if not items:
        plt.figure(figsize=(8, 5))
        plt.text(0.5, 0.5, "ì›Œë“œí´ë¼ìš°ë“œ ë°ì´í„° ì—†ìŒ", ha="center", va="center")
        plt.axis("off")
        plt.savefig(out_path, dpi=150, bbox_inches="tight")
        plt.close()
        return
    freqs = {}
    for it in items[:200]:
        w = (it.get("keyword") or "").strip()
        s = float(it.get("score", 0) or 0)
        if w:
            freqs[w] = freqs.get(w, 0.0) + max(s, 0.0)
    font_path = get_kr_font_path()
    if not font_path:
        print("[WARN] font_pathë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”. assets/fontsì— TTF/OTF ë„£ê±°ë‚˜ scripts/plot_font.py í™•ì¸ í”Œë¦¬ì¦ˆ.")
    wc = WordCloud(
        width=1200,
        height=600,
        background_color="white",
        font_path=font_path,
        colormap="tab20",
        prefer_horizontal=0.9,
        min_font_size=10,
        max_words=200,
        relative_scaling=0.5,
        normalize_plurals=False
    ).generate_from_frequencies(freqs)
    wc.to_file(out_path)

def plot_top_keywords(keywords, out_path="outputs/fig/top_keywords.png", topn=15):
    import matplotlib.pyplot as plt
    import seaborn as sns
    ensure_fonts()
    apply_plot_style()
    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    data = keywords.get("keywords", [])[:topn]
    if not data:
        plt.figure(figsize=(8, 5))
        plt.text(0.5, 0.5, "í‚¤ì›Œë“œ ë°ì´í„° ì—†ìŒ", ha="center")
        plt.axis("off")
        plt.savefig(out_path, dpi=150, bbox_inches="tight")
        plt.close()
        return
    labels = [d["keyword"] for d in data][::-1]
    scores = [d["score"] for d in data][::-1]
    plt.figure(figsize=(10, 6))
    sns.barplot(x=scores, y=labels, color="#3b82f6")
    plt.title("Top Keywords")
    plt.xlabel("Score")
    plt.ylabel("")
    plt.tight_layout()
    plt.savefig(out_path, dpi=150)
    plt.close()

def plot_topics(topics, out_path="outputs/fig/topics.png", topn_words=6):
    import os, math
    import matplotlib.pyplot as plt
    import seaborn as sns
    ensure_fonts()
    apply_plot_style()
    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    tps = topics.get("topics", [])
    if not tps:
        plt.figure(figsize=(8, 5))
        plt.text(0.5, 0.5, "í† í”½ ë°ì´í„° ì—†ìŒ", ha="center")
        plt.axis("off")
        plt.savefig(out_path, dpi=150, bbox_inches="tight")
        plt.close()
        return
    k = len(tps)
    cols = 2
    rows = math.ceil(k / cols)
    fig, axes = plt.subplots(rows, cols, figsize=(12, 4 * rows))
    axes = axes.flatten() if k > 1 else [axes]
    for i, t in enumerate(tps):
        ax = axes[i]
        words = (t.get("top_words") or [])[:topn_words]
        labels = [str((w.get("word") or "")) for w in words][::-1]
        probs = []
        for w in words[::-1]:
            pw = w.get("prob", 1.0)
            try:
                p = float(pw)
                if p <= 0:
                    p = 1.0
            except Exception:
                p = 1.0
            probs.append(p)
        sns.barplot(x=probs, y=labels, ax=ax, color="#10b981")
        ax.set_title(f"Topic #{t.get('topic_id')}")
        ax.set_xlabel("Weight")
        ax.set_ylabel("")
    for j in range(i + 1, len(axes)):
        axes[j].axis("off")
    plt.tight_layout()
    plt.savefig(out_path, dpi=150)
    plt.close()

def plot_timeseries(ts, out_path="outputs/fig/timeseries.png"):
    import matplotlib.pyplot as plt
    import pandas as pd
    import matplotlib.dates as mdates
    from datetime import datetime, timedelta
    ensure_fonts()
    apply_plot_style()
    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    daily = ts.get("daily", [])
    if not daily:
        plt.figure(figsize=(10, 5))
        plt.title("Articles per Day (no data)")
        plt.xlabel("Date")
        plt.ylabel("Count")
        plt.tight_layout()
        plt.savefig(out_path, dpi=150, bbox_inches="tight")
        plt.close()
        return
    df = pd.DataFrame(daily).copy()
    df["date"] = pd.to_datetime(df["date"], errors="coerce", utc=False)
    df["count"] = pd.to_numeric(df.get("count", 0), errors="coerce").fillna(0).astype(int)
    df = df.dropna(subset=["date"]).sort_values("date")
    now_year = datetime.now().year
    y_min, y_max = now_year - 3, now_year + 1
    df = df[(df["date"].dt.year >= y_min) & (df["date"].dt.year <= y_max)]
    if df.empty:
        plt.figure(figsize=(10, 5))
        plt.title("Articles per Day (empty after filtering)")
        plt.xlabel("Date")
        plt.ylabel("Count")
        plt.tight_layout()
        plt.savefig(out_path, dpi=150, bbox_inches="tight")
        plt.close()
        return
    df = df.set_index("date")
    full_idx = pd.date_range(df.index.min().normalize(), df.index.max().normalize(), freq="D")
    df = df.reindex(full_idx).fillna(0)
    df.index.name = "date"
    df["count"] = df["count"].astype(int)
    if len(df) == 1:
        d0 = df.index[0]
        y = float(df["count"].iloc[0])
        plt.figure(figsize=(12, 4.5))
        plt.xlim(d0 - timedelta(days=1), d0 + timedelta(days=1))
        ypad = max(1, y * 0.15)
        plt.ylim(0, y + ypad)
        ax = plt.gca()
        ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))
        ax.xaxis.set_major_formatter(mdates.DateFormatter("%Y-%m-%d"))
        plt.plot([d0], [y], marker="o", color="#6366f1", label="Daily")
        plt.annotate(f"{int(y)}", (d0, y), textcoords="offset points", xytext=(0, -14), ha="center",
                     fontsize=9, bbox=dict(boxstyle="round,pad=0.2", fc="white", ec="none", alpha=0.7))
        plt.title(f"Articles per Day ({d0.strftime('%Y-%m-%d')})")
        plt.xlabel("Date")
        plt.ylabel("Count")
        plt.legend(loc="upper right")
        plt.grid(alpha=0.25, linestyle="--")
        plt.tight_layout()
        plt.savefig(out_path, dpi=150, bbox_inches="tight")
        plt.close()
        return
    plt.figure(figsize=(12, 4.5))
    ax = plt.gca()
    ax.xaxis.set_major_locator(mdates.AutoDateLocator())
    ax.xaxis.set_major_formatter(mdates.DateFormatter("%Y-%m-%d"))
    plt.plot(df.index, df["count"], marker="o", markersize=3, linewidth=1, color="#6366f1", label="Daily")
    if len(df) >= 7:
        df["ma7"] = df["count"].rolling(window=7, min_periods=1).mean()
        plt.plot(df.index, df["ma7"], linestyle="--", linewidth=2, color="#f43f5e", label="7-day MA")
    plt.title("Articles per Day")
    plt.xlabel("Date")
    plt.ylabel("Count")
    plt.legend(loc="upper right")
    plt.grid(alpha=0.25, linestyle="--")
    plt.tight_layout()
    plt.savefig(out_path, dpi=150, bbox_inches="tight")
    plt.close()

def plot_keyword_network(keywords, docs, out_path="outputs/fig/keyword_network.png",
                         topn=50, min_cooccur=2, max_edges=100, label_top=25):
    """ í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ìƒì„± (PageRank ê¸°ë°˜ ë¼ë²¨ë§ + ì»¤ë®¤ë‹ˆí‹° íƒì§€ ì‹œê°í™” ë²„ì „) """
    import matplotlib.pyplot as plt
    import networkx as nx
    import os
    from networkx.algorithms import community
    from adjustText import adjust_text
    from matplotlib.patches import Patch

    ensure_fonts()
    apply_plot_style()
    os.makedirs(os.path.dirname(out_path), exist_ok=True)

    # 1. í‚¤ì›Œë“œ ì ìˆ˜ ê°€ì ¸ì˜¤ê¸°
    keyword_scores = {
        it.get("keyword"): float(it.get("score", 0))
        for it in keywords.get("keywords", [])[:topn]
        if it.get("keyword")
    }

    if not keyword_scores or not docs:
        return {"nodes": 0, "edges": 0}

    # 2. ê³µë™ ì¶œí˜„ ê³„ì‚°
    cooccur = {}
    for doc in docs:
        words_in_doc = set(simple_tokenize_ko(doc)).intersection(keyword_scores.keys())
        words_in_doc = sorted(words_in_doc)
        for i in range(len(words_in_doc)):
            for j in range(i + 1, len(words_in_doc)):
                pair = tuple(sorted((words_in_doc[i], words_in_doc[j])))
                cooccur[pair] = cooccur.get(pair, 0) + 1

    # 3. ìƒìœ„ ì—£ì§€ ì„ íƒ
    edges = sorted(
        [(u, v, w) for (u, v), w in cooccur.items() if w >= min_cooccur],
        key=lambda x: x[2],
        reverse=True
    )[:max_edges]

    # 4. ê·¸ë˜í”„ êµ¬ì„±
    G = nx.Graph()
    for word, score in keyword_scores.items():
        G.add_node(word, score=score)
    G.add_weighted_edges_from(edges)

    # ê³ ë¦½ ë…¸ë“œ ì œê±°
    G.remove_nodes_from(list(nx.isolates(G)))
    if G.number_of_nodes() == 0:
        return {"nodes": 0, "edges": 0}

    # 5. ì»¤ë®¤ë‹ˆí‹° íƒì§€
    communities = list(community.greedy_modularity_communities(G))
    for i, comm in enumerate(communities):
        for node in comm:
            G.nodes[node]['community'] = i

    # 6. PageRank ê³„ì‚°
    pagerank = nx.pagerank(G, weight='weight')

    # 7. ì»¤ë®¤ë‹ˆí‹° ëŒ€í‘œ ë¼ë²¨ ì„¤ì •
    community_labels = {}
    for i, comm in enumerate(communities):
        top_node = max(comm, key=lambda node: pagerank.get(node, 0))
        community_labels[i] = top_node

    # 8. ì‹œê°í™”
    fig, ax = plt.subplots(figsize=(14, 10))
    pos = nx.spring_layout(G, k=5.0, iterations=50, seed=42)

    node_sizes = [100 + 2500 * G.nodes[n]['score'] for n in G.nodes()]
    community_ids = [G.nodes[n]['community'] for n in G.nodes()]
    node_colors = [plt.cm.tab20(c % 20) for c in community_ids]

    nx.draw_networkx_nodes(
        G, pos,
        node_size=node_sizes,
        node_color=node_colors,
        edgecolors='#000000',   # ë” ì§„í•œ ì™¸ê³½ì„ 
        linewidths=1.0,
        alpha=0.9,
        ax=ax
    )


    edge_weights = [d['weight'] for _, _, d in G.edges(data=True)]
    w_max = max(edge_weights, default=1)
    edge_widths = [0.3 + 2.0 * (w / w_max) for w in edge_weights]
    nx.draw_networkx_edges(G, pos, width=edge_widths, edge_color='grey', alpha=0.5, ax=ax)

    # 9. ìƒìœ„ PageRank ë…¸ë“œ ë¼ë²¨ë§
    top_nodes = sorted(pagerank, key=pagerank.get, reverse=True)[:label_top]
    texts = [
        ax.text(pos[n][0], pos[n][1], n,
                fontsize=9, ha='center', va='center',
                fontweight='bold', color="#111111",  # ì§„í•œ ê¸€ì”¨
                bbox=dict(boxstyle='round,pad=0.2', fc='white', ec='gray', alpha=0.8))
        for n in top_nodes if n in pos
    ]

    if texts:
        adjust_text(texts, arrowprops=dict(arrowstyle='-', color='gray', lw=0.5))

    # 10. ë²”ë¡€
    legend_elements = [
        Patch(facecolor=plt.cm.tab20(i % 20), edgecolor='black', label=f'Group: {comm_label}')
        for i, comm_label in community_labels.items()
    ]
    ax.legend(
        handles=legend_elements,
        title="ì£¼ì œ ê·¸ë£¹ (Topic Groups)",
        loc='lower left',           # â† ë‚´ë¶€ë¡œ ì´ë™
        bbox_to_anchor=(0.01, 0.01),  # â† ì¢Œì¸¡ í•˜ë‹¨ ì‚´ì§ ì•ˆìª½
        frameon=True,
        framealpha=0.9,
        edgecolor='gray',
        fontsize=9,
        title_fontsize=10
    )

    ax.set_title("í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ë° ì£¼ì œ ê·¸ë£¹ ë¶„ì„", fontsize=16)
    plt.tight_layout()
    plt.savefig(out_path, dpi=150, bbox_inches='tight')
    plt.close()

    print("[INFO] Saved keyword_network.png")
    return {"nodes": G.number_of_nodes(), "edges": G.number_of_edges()}


def export_csvs(ts_obj, keywords_obj, topics_obj, out_dir="outputs/export"):
    import pandas as pd
    import os
    os.makedirs(out_dir, exist_ok=True)
    daily = (ts_obj or {}).get("daily", [])
    df_ts = pd.DataFrame(daily) if daily else pd.DataFrame(columns=["date", "count"])
    df_ts.to_csv(os.path.join(out_dir, "timeseries_daily.csv"), index=False, encoding="utf-8")
    kws = (keywords_obj or {}).get("keywords", [])[:20]
    df_kw = pd.DataFrame(kws) if kws else pd.DataFrame(columns=["keyword", "score"])
    df_kw.to_csv(os.path.join(out_dir, "keywords_top20.csv"), index=False, encoding="utf-8")
    topics = (topics_obj or {}).get("topics", [])
    rows = []
    for t in topics:
        tid = t.get("topic_id")
        for w in (t.get("top_words") or [])[:10]:
            pw = w.get("prob", None)
            try:
                p = float(pw)
                if p == 0.0:
                    p = 1e-6
            except Exception:
                p = 1e-6
            rows.append({"topic_id": tid, "word": w.get("word", ""), "prob": p})
    df_tw = pd.DataFrame(rows) if rows else pd.DataFrame(columns=["topic_id", "word", "prob"])
    df_tw.to_csv(os.path.join(out_dir, "topics_top_words.csv"), index=False, encoding="utf-8")
    print("[INFO] export CSVs -> outputs/export/*.csv")

# ===== [PATCH] ê´€ê³„Â·ê²½ìŸ ì‹¬í™” ë¶„ì„ ì„¹ì…˜ =====
def _generate_relationship_competition_section(fig_dir="fig",
                                               net_path="outputs/company_network.json",
                                               summary_path="outputs/analysis_summary.json"):
    import os, json
    import pandas as pd
    lines = []
    lines.append("\n## ê´€ê³„Â·ê²½ìŸ ì‹¬í™” ë¶„ì„\n")

    # ë„¤íŠ¸ì›Œí¬ ë¡œë“œ
    net = {}
    try:
        with open(net_path, "r", encoding="utf-8") as f:
            net = json.load(f) or {}
    except Exception:
        lines.append("- (ë„¤íŠ¸ì›Œí¬ ë°ì´í„°ê°€ ì—†ì–´ ë³¸ ì„¹ì…˜ì„ ìƒëµí•©ë‹ˆë‹¤.)\n")
        return "\n".join(lines)

    edges = net.get("edges", [])
    nodes = net.get("nodes", [])
    top_pairs = net.get("top_pairs", [])
    central = net.get("centrality", [])
    betw = net.get("betweenness", [])
    comms = net.get("communities", [])

    # í•µì‹¬ ìš”ì•½
    num_nodes = len(nodes)
    num_edges = len(edges)
    lines.append("**í•µì‹¬ ìš”ì•½**\n")
    lines.append(f"- **ê´€ê³„ë§ ê·œëª¨:** ë…¸ë“œ {num_nodes}ê°œ / ì—£ì§€ {num_edges}ê°œ\n")
    if top_pairs:
        tp = top_pairs[0]
        lines.append(f"- **ê°€ì¥ ê°•í•œ ê´€ê³„:** {tp.get('source')} â†” {tp.get('target')} (ê°€ì¤‘ì¹˜ {tp.get('weight')}, ìœ í˜• {tp.get('rel_type')})\n")
    if central:
        lines.append(f"- **í—ˆë¸Œ í›„ë³´:** {central[0].get('org')} (Degree {central[0].get('degree_centrality')})\n")
    if betw:
        lines.append(f"- **ë¸Œë¡œì»¤ í›„ë³´:** {betw[0].get('org')} (Betweenness {betw[0].get('betweenness')})\n")

    # ìƒìœ„ ê´€ê³„ìŒ
    if top_pairs:
        df_pairs = pd.DataFrame(top_pairs)[["source","target","weight","rel_type"]]
        lines.append("\n### ìƒìœ„ ê´€ê³„ìŒ(Edge)\n")
        # ì„¤ëª… ì¸ìš© ì¶”ê°€
        lines.append("> ë™ì¼ ë¬¸ì„œ/ë¬¸ì¥ ë‚´ì—ì„œ í•¨ê»˜ ì–¸ê¸‰ëœ ê¸°ì—… ìŒì´ë©°, ê°€ì¤‘ì¹˜ëŠ” ë™ì‹œì¶œí˜„ ë¹ˆë„ì…ë‹ˆë‹¤. ê°’ì´ ë†’ì„ìˆ˜ë¡ ìƒí˜¸ ê´€ë ¨ì„±ì´ ê°•í•˜ê³ , ìœ í˜•ì€ í‚¤ì›Œë“œ ê·œì¹™ìœ¼ë¡œ ê²½ìŸ/í˜‘ë ¥/ì¤‘ë¦½ì„ ì¶”ì •í•©ë‹ˆë‹¤.\n")
        lines.append(df_pairs.rename(columns={
            "source": "Source", "target": "Target", "weight": "Weight", "rel_type": "Type"
        }).to_markdown(index=False))
        lines.append("\n")

    # ì¤‘ì‹¬ì„± ìƒìœ„
    if central:
        df_c = pd.DataFrame(central)[["org","degree_centrality"]]
        lines.append("\n### ì¤‘ì‹¬ì„± ìƒìœ„(ì—°ê²° í—ˆë¸Œ)\n")
        # ì„¤ëª… ì¸ìš© ì¶”ê°€
        lines.append("> Degree ì¤‘ì‹¬ì„±ì€ í•œ ë…¸ë“œê°€ ì—°ê²°ëœ ìƒëŒ€ ìˆ˜ì˜ ë¹„ìœ¨ë¡œ, ê°’ì´ ë†’ì„ìˆ˜ë¡ ë‹¤ìˆ˜ì˜ ê¸°ì—…ê³¼ ì§ì ‘ ì—°ê²°ëœ í—ˆë¸Œ ì„±ê²©ì„ ê°€ì§‘ë‹ˆë‹¤. í—ˆë¸ŒëŠ” ì´ìŠˆ í™•ì‚°ê³¼ ì •ë³´ ì ‘ê·¼ì„±ì´ ë†’ìŠµë‹ˆë‹¤.\n")
        lines.append(df_c.rename(columns={"org": "Org", "degree_centrality": "DegreeCentrality"}).to_markdown(index=False))
        lines.append("\n")
    if betw:
        df_b = pd.DataFrame(betw)[["org","betweenness"]]
        lines.append("\n### ë§¤ê°œ ì¤‘ì‹¬ì„± ìƒìœ„(ì •ë³´ ë¸Œë¡œì»¤)\n")
        # ì„¤ëª… ì¸ìš© ì¶”ê°€
        lines.append("> BetweennessëŠ” ë„¤íŠ¸ì›Œí¬ ê²½ë¡œì˜ â€˜ë‹¤ë¦¬â€™ ì—­í•  ì •ë„ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ê°’ì´ ë†’ì„ìˆ˜ë¡ ì„œë¡œ ë‹¤ë¥¸ ì§‘ë‹¨ì„ ì—°ê²°í•˜ëŠ” ì¤‘ê°œì(ë¸Œë¡œì»¤)ë¡œ í•´ì„ë˜ë©°, ê±°ë˜Â·í˜‘ìƒë ¥ê³¼ ì •ë³´ íë¦„ ì¥ì•…ë ¥ì´ í½ë‹ˆë‹¤.\n")
        lines.append(df_b.rename(columns={"org": "Org", "betweenness": "Betweenness"}).to_markdown(index=False))
        lines.append("\n")

    # ì»¤ë®¤ë‹ˆí‹° ë¯¸ë¦¬ë³´ê¸°
    if comms:
        lines.append("\n### ì»¤ë®¤ë‹ˆí‹°(ê´€ê³„ í´ëŸ¬ìŠ¤í„°)\n")
        # ì„¤ëª… ì¸ìš© ì¶”ê°€
        lines.append("> ëª¨ë“ˆëŸ¬ë¦¬í‹° ê¸°ë°˜ìœ¼ë¡œ ìë™ ì¶”ì¶œí•œ ê´€ê³„ ì§‘ë‹¨ì…ë‹ˆë‹¤. ê°™ì€ ì§‘ë‹¨ ë‚´ ê¸°ì—…ë“¤ì€ ìœ ì‚¬ ì£¼ì œë‚˜ ê³µê¸‰ë§ í™œë™ì„ ê³µìœ í•  ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.\n")
        preview = []
        for c in comms[:5]:
            members = c.get("members", [])[:6]
            theme = (c.get("interpretation", "") or "").strip()

            # í•´ì„ì´ ë¹„ì–´ ìˆìœ¼ë©´ ìë™ ìƒì„±: ëŒ€í‘œ ë©¤ë²„ í‚¤ì›Œë“œ ê¸°ë°˜ ìš”ì•½
            if not theme:
                # ê°„ë‹¨ ê·œì¹™: ëŒ€í‘œ ë©¤ë²„ëª…ì„ ë‚˜ì—´í•´ ìš”ì•½(í•„ìš” ì‹œ ë„ë©”ì¸ í‚¤ì›Œë“œì™€ ê²°í•© ê°€ëŠ¥)
                if members:
                    theme = f"{members[0]} ì¤‘ì‹¬ì˜ ì—°ê´€ í´ëŸ¬ìŠ¤í„°"
                else:
                    theme = "ì£¼ìš” ê¸°ì—… ì—°ê´€ í´ëŸ¬ìŠ¤í„°"

            preview.append(f"- C{c.get('community_id')}: {', '.join(members)} | í•´ì„: {theme}")
        lines.extend(preview)
        lines.append("\n")

    # ì´ë¯¸ì§€ ì²¨ë¶€(ì´ë¯¸ D/generate_visualsì—ì„œ ìƒì„±í–ˆë‹¤ê³  ê°€ì •)
    img_rel = f"outputs/{fig_dir}/company_network.png"
    if os.path.exists(img_rel):
        lines.append("### ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”\n")
        lines.append(f"![Company Network]({fig_dir}/company_network.png)\n")

    # ì¢…í•© ì½”ë©˜íŠ¸
    lines.append("> ë™ì‹œì¶œí˜„ì´ ë†’ì€ ìŒì€ ì§ì ‘ ê²½ìŸ ë˜ëŠ” ê³µê¸‰ë§ í•µì‹¬ í˜‘ë ¥ ê°€ëŠ¥ì„±ì„ ì‹œì‚¬í•˜ë©°, í—ˆë¸Œ/ë¸Œë¡œì»¤ëŠ” ì‹œì¥ ì˜í–¥ë ¥ ë° ì¤‘ê°œ í¬ì§€ì…˜ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì»¤ë®¤ë‹ˆí‹°ëŠ” ì „ëµÂ·ë°¸ë¥˜ì²´ì¸ ë‹¨ìœ„ì˜ ë™ì¡° í´ëŸ¬ìŠ¤í„°ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n")

    return "\n".join(lines)


def build_docs_from_meta(meta_items):
    docs = []
    for it in meta_items:
        title = (it.get("title") or it.get("title_og") or "").strip()
        desc = (it.get("description") or it.get("description_og") or "").strip()
        doc = (title + " " + desc).strip()
        if doc:
            docs.append(doc)
    return docs

def _fmt_int(x):
    try:
        return f"{int(x):,}"
    except Exception:
        return str(x)

def _fmt_score(x, nd=3):
    try:
        return f"{float(x):.{nd}f}"
    except Exception:
        return str(x)

def _truncate(s, n=80):
    s = (s or "").strip().replace("\n", " ")
    return s if len(s) <= n else s[:n-1] + "â€¦"

def build_markdown(keywords, topics, ts, insights, opps, fig_dir="fig", out_md="outputs/report.md"):
    import pandas as pd
    import glob

    # --- âœ¨âœ¨âœ¨ ì‹ ê·œ í—¬í¼ í•¨ìˆ˜ 1: ê¸°ìˆ  ì„±ìˆ™ë„ ì„¹ì…˜ ìƒì„± âœ¨âœ¨âœ¨ ---
    def _generate_tech_maturity_section(fig_dir="fig"):
        section_lines = ["\n## ê¸°ìˆ  ì„±ìˆ™ë„ ë¶„ì„ (Technology Maturity Analysis)\n"]
        try:
            maturity_data = load_json("outputs/tech_maturity.json", {"results": []})
            if not maturity_data.get("results"):
                section_lines.append("- (ë¶„ì„ëœ ê¸°ìˆ  ì„±ìˆ™ë„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.)\n")
                return "\n".join(section_lines)

            # --- âœ¨âœ¨âœ¨ ì°¨íŠ¸ ì´ë¯¸ì§€ì™€ ì„¤ëª… ì¶”ê°€ âœ¨âœ¨âœ¨ ---
            section_lines.append(f"![Technology Maturity Map]({fig_dir}/tech_maturity_map.png)\n")
            section_lines.append("> ê° ê¸°ìˆ ì˜ ì‹œì¥ ë‚´ ìœ„ì¹˜(Xì¶•: ê´€ì‹¬ë„, Yì¶•: ê¸ì •ì„±)ì™€ ì‚¬ì—… í™œë°œë„(ë²„ë¸” í¬ê¸°)ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.\n")

            section_lines.append("| ê¸°ìˆ  (Technology) | ì„±ìˆ™ë„ ë‹¨ê³„ (Stage) | íŒë‹¨ ê·¼ê±° (Rationale) |")
            section_lines.append("|:---|:---|:---|")
            for item in maturity_data["results"]:
                tech = item.get("technology", "N/A")
                analysis = item.get("analysis", {})
                stage = analysis.get("stage", "N/A")
                reason = analysis.get("reason", "-")
                section_lines.append(f"| {tech} | **{stage}** | {reason} |")
            section_lines.append("\n")
        except Exception as e:
            section_lines.append(f"- (ê¸°ìˆ  ì„±ìˆ™ë„ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e})\n")
        
        return "\n".join(section_lines)

    # --- âœ¨âœ¨âœ¨ ì‹ ê·œ í—¬í¼ í•¨ìˆ˜ 2: ì•½í•œ ì‹ í˜¸ ì„¹ì…˜ ìƒì„± âœ¨âœ¨âœ¨ ---
    def _generate_weak_signal_section():
        section_lines = ["\n## í¬ì°©ëœ ì•½í•œ ì‹ í˜¸ ë° í•´ì„ (Emerging Signals & Interpretation)\n"]
        try:
            weak_signal_data = load_json("outputs/weak_signal_insights.json", {"results": []})
            if not weak_signal_data.get("results"):
                section_lines.append("- (í¬ì°©ëœ ì•½í•œ ì‹ í˜¸ê°€ ì—†ìŠµë‹ˆë‹¤.)\n")
                return "\n".join(section_lines)

            for item in weak_signal_data["results"]:
                signal = item.get("signal", "N/A")
                interpretation = item.get("interpretation", "-")
                section_lines.append(f"- **{signal}**")
                section_lines.append(f"  - **í•´ì„:** {interpretation}")
            section_lines.append("\n")
        except Exception as e:
            section_lines.append(f"- (ì•½í•œ ì‹ í˜¸ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e})\n")

        return "\n".join(section_lines)

    def _generate_matrix_section(topics_obj):
        try:
            csv_path = "outputs/export/company_topic_matrix_wide.csv"
            if not os.path.exists(csv_path):
                return "\n## ê¸°ì—…Ã—í† í”½ ì§‘ì¤‘ë„ ë§¤íŠ¸ë¦­ìŠ¤ (ì£¼ê°„)\n\n- (ë¶„ì„í•  ìœ íš¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.)\n"
            df = pd.read_csv(csv_path, encoding='utf-8-sig')
            topic_cols = [col for col in df.columns if col.startswith('topic_')]
            if df.empty or not topic_cols:
                return "\n## ê¸°ì—…Ã—í† í”½ ì§‘ì¤‘ë„ ë§¤íŠ¸ë¦­ìŠ¤ (ì£¼ê°„)\n\n- (ë¶„ì„í•  ìœ íš¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.)\n"
            df_numeric = df.copy()
            for col in topic_cols:
                df_numeric[col] = df[col].fillna('').astype(str).str.split(' ').str[0].replace('', '0').astype(float)
            competitive_scores = df_numeric[topic_cols].gt(0).sum()
            top_competitive_topic_id = competitive_scores.idxmax() if not competitive_scores.empty and competitive_scores.max() > 0 else "N/A"
            df_numeric['total_score'] = df_numeric[topic_cols].sum(axis=1)
            top_focused_org = df_numeric.loc[df_numeric['total_score'].idxmax()]['org'] if not df_numeric['total_score'].empty and df_numeric['total_score'].max() > 0 else "N/A"
            max_score = 0
            rising_star_info = "N/A"
            if df_numeric[topic_cols].max().max() > 0:
                for col in topic_cols:
                    if df_numeric[col].max() > max_score:
                        max_score = df_numeric[col].max()
                        org_name = df_numeric.loc[df_numeric[col].idxmax()]['org']
                        rising_star_info = f"{org_name} @ {col}"
            topic_map = {f"topic_{t.get('topic_id')}": ", ".join([w.get('word', '') for w in t.get('top_words', [])[:2]]) for t in topics_obj.get('topics', [])}
            section_lines = ["\n## ê¸°ì—…Ã—í† í”½ ì§‘ì¤‘ë„ ë§¤íŠ¸ë¦­ìŠ¤ (ì£¼ê°„)\n"]
            section_lines.append("**í•µì‹¬ ìš”ì•½:**\n")
            section_lines.append(f"- **ê°€ì¥ ê²½ìŸì´ ì¹˜ì—´í•œ í† í”½:** **{topic_map.get(top_competitive_topic_id, top_competitive_topic_id)}** (ê°€ì¥ ë§ì€ ê¸°ì—…ë“¤ì´ ì£¼ëª©)\n")
            section_lines.append(f"- **ê°€ì¥ ì§‘ì¤‘ë„ê°€ ë†’ì€ ê¸°ì—…:** **{top_focused_org}** (ë‹¤ì–‘í•œ í† í”½ì— ê±¸ì³ ë†’ì€ ê´€ë ¨ì„±)\n")
            section_lines.append(f"- **ì£¼ëª©í•  ë§Œí•œ ì¡°í•©:** **{rising_star_info}** (ê°€ì¥ ë†’ì€ ë‹¨ì¼ ì—°ê´€ ì ìˆ˜ ê¸°ë¡)\n")
            section_lines.append("ê° ê¸°ì—…ë³„ ìƒìœ„ 8ê°œ í† í”½ì˜ ì—°ê´€ ì ìˆ˜ì™€ í•´ë‹¹ í† í”½ ë‚´ì—ì„œì˜ ì ìœ ìœ¨(%)ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n")
            section_lines.append(df.to_markdown(index=False))
            section_lines.append("\n**ì½”ë©˜íŠ¸ ë° ì•¡ì…˜ íŒíŠ¸:**\n")
            section_lines.append(f"> íŠ¹ì • í† í”½ì—ì„œ ë†’ì€ ì ìœ ìœ¨ì„ ë³´ì´ëŠ” ê¸°ì—…ì€ í•´ë‹¹ ë¶„ì•¼ì˜ 'ì£¼ë„ì(Leader)'ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤. ë°˜ë©´, íŠ¹ì • ê¸°ì—…ì´ ì†Œìˆ˜ì˜ í† í”½ì— ë†’ì€ ì ìˆ˜ë¥¼ ì§‘ì¤‘í•˜ê³  ìˆë‹¤ë©´, ì´ëŠ” í•´ë‹¹ ê¸°ì—…ì˜ 'í•µì‹¬ ì „ëµ ë¶„ì•¼'ë¥¼ ì‹œì‚¬í•©ë‹ˆë‹¤. ê²½ìŸì‚¬ ë° íŒŒíŠ¸ë„ˆì‚¬ì˜ ì§‘ì¤‘ ë¶„ì•¼ë¥¼ íŒŒì•…í•˜ì—¬ ìš°ë¦¬ì˜ ì „ëµì„ ì ê²€í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n")
            return "\n".join(section_lines)
        except Exception as e:
            return f"\n## ê¸°ì—…Ã—í† í”½ ì§‘ì¤‘ë„ ë§¤íŠ¸ë¦­ìŠ¤ (ì£¼ê°„)\n\n- (ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e})\n"

    def _generate_visual_analysis_section(fig_dir="fig"):
        section_lines = ["\n## ê¸°ì—…Ã—í† í”½ ì‹œê°ì  ë¶„ì„\n"]
        has_content = False
        heatmap_path = f"outputs/{fig_dir}/matrix_heatmap.png"
        if os.path.exists(heatmap_path):
            section_lines.append("### ì „ì²´ ì‹œì¥ êµ¬ë„ (Heatmap)\n")
            section_lines.append(f"![Heatmap]({fig_dir}/matrix_heatmap.png)\n")
            section_lines.append("> ì „ì²´ ê¸°ì—…ê³¼ í† í”½ ê°„ì˜ ê´€ê³„ë¥¼ í•œëˆˆì— ë³´ì—¬ì¤ë‹ˆë‹¤. ìƒ‰ì´ ì§„í• ìˆ˜ë¡ ì—°ê´€ì„±ì´ ë†’ìŠµë‹ˆë‹¤.\n")
            has_content = True
        share_images = sorted(glob.glob(f"outputs/{fig_dir}/topic_share_*.png"))
        if share_images:
            section_lines.append("### ì£¼ìš” í† í”½ë³„ ê²½ìŸ êµ¬ë„ (Pie Charts)\n")
            section_lines.append("> ê°€ì¥ ëœ¨ê±°ìš´ ì£¼ì œë¥¼ ë‘ê³  ì–´ë–¤ ê¸°ì—…ë“¤ì´ ê²½ìŸí•˜ëŠ”ì§€ ì ìœ ìœ¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n")
            for img_path in share_images:
                img_name = os.path.basename(img_path)
                section_lines.append(f"![Topic Share]({fig_dir}/{img_name})")
            section_lines.append("\n")
            has_content = True
        focus_images = sorted(glob.glob(f"outputs/{fig_dir}/company_focus_*.png"))
        if focus_images:
            section_lines.append("### ì£¼ìš” ê¸°ì—…ë³„ ì „ëµ ë¶„ì„ (Bar Charts)\n")
            section_lines.append("> ì‹œì¥ì„ ì£¼ë„í•˜ëŠ” ì£¼ìš” ê¸°ì—…ë“¤ì´ ì–´ë–¤ í† í”½ì— ì§‘ì¤‘í•˜ê³  ìˆëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤.\n")
            for img_path in focus_images:
                img_name = os.path.basename(img_path)
                section_lines.append(f"![Company Focus]({fig_dir}/{img_name})")
            section_lines.append("\n")
            has_content = True
        if not has_content:
            return ""
        return "\n".join(section_lines)
    
    def _generate_signals_section(fig_dir="fig"): # fig_dir ì¸ì ì¶”ê°€
        section_lines = ["\n## ì£¼ìš” ì‹œê·¸ë„ ë¶„ì„ (Key Signal Analysis)\n"]
        has_content = False

        # 1. ê°•í•œ ì‹ í˜¸ (Strong Signals) í…Œì´ë¸” ë° ì°¨íŠ¸ ìƒì„±
        try:
            strong_df = pd.read_csv("outputs/export/trend_strength.csv")
            if not strong_df.empty:
                section_lines.append("### ê°•í•œ ì‹ í˜¸ (Strong Signals)\n")
                section_lines.append("> ìµœê·¼ ë‰´ìŠ¤ì—ì„œ ê°€ì¥ ì£¼ëª©ë°›ì€ ìƒìœ„ í‚¤ì›Œë“œë“¤ì…ë‹ˆë‹¤.\n")
                
                # --- âœ¨âœ¨âœ¨ ì°¨íŠ¸ ì´ë¯¸ì§€ ì‚½ì… âœ¨âœ¨âœ¨ ---
                section_lines.append(f"![Strong Signals Chart]({fig_dir}/strong_signals_barchart.png)\n")
                
                report_df = strong_df.head(10)[['term', 'cur', 'z_like']].copy()
                report_df.rename(columns={'term': 'ê°•í•œ ì‹ í˜¸ (Term)', 'cur': 'ìµœê·¼ ì–¸ê¸‰ëŸ‰ (cur)', 'z_like': 'ì„íŒ©íŠ¸ (z_like)'}, inplace=True)
                report_df.insert(0, 'ìˆœìœ„', range(1, 1 + len(report_df)))

                section_lines.append(report_df.to_markdown(index=False))
                section_lines.append("\n")
                has_content = True
        except FileNotFoundError:
            pass
        except Exception as e:
            section_lines.append(f"- (ê°•í•œ ì‹ í˜¸ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e})\n")

        # 2. ì•½í•œ ì‹ í˜¸ (Weak Signals) í…Œì´ë¸” ë° ì°¨íŠ¸ ìƒì„±
        try:
            weak_df = pd.read_csv("outputs/export/weak_signals.csv")
            weak_insights = load_json("outputs/weak_signal_insights.json", {"results": []})
            
            if not weak_df.empty:
                section_lines.append("### ì•½í•œ ì‹ í˜¸ (Weak Signals)\n")
                section_lines.append("> ì´ ì–¸ê¸‰ëŸ‰ì€ ì ì§€ë§Œ ìµœê·¼ ê¸‰ë¶€ìƒí•˜ì—¬ ë¯¸ë˜ê°€ ê¸°ëŒ€ë˜ëŠ” 'í‹ˆìƒˆ í‚¤ì›Œë“œ'ë“¤ì…ë‹ˆë‹¤.\n")

                # --- âœ¨âœ¨âœ¨ ì°¨íŠ¸ ì´ë¯¸ì§€ ì‚½ì… âœ¨âœ¨âœ¨ ---
                section_lines.append(f"![Weak Signal Radar]({fig_dir}/weak_signal_radar.png)\n")

                if weak_insights.get("results"):
                    insights_map = {item['signal']: item['interpretation'] for item in weak_insights["results"]}
                    report_rows = []
                    for _, row in weak_df.iterrows():
                        term = row['term']
                        report_rows.append({
                            "ì•½í•œ ì‹ í˜¸ (Signal)": term,
                            "ì§€í‘œ (cur / z_like)": f"{row['cur']} / {row['z_like']:.2f}",
                            "LLMì˜ 1ì¤„ ìš”ì•½ (Interpretation)": insights_map.get(term, "-")
                        })
                    section_lines.append(pd.DataFrame(report_rows).to_markdown(index=False))
                
                section_lines.append("\n")
                has_content = True
        except FileNotFoundError:
            pass
        except Exception as e:
            section_lines.append(f"- (ì•½í•œ ì‹ í˜¸ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e})\n")

        if not has_content:
            return ""
            
        return "\n".join(section_lines)

    klist = keywords.get("keywords", [])[:15]
    tlist = topics.get("topics", [])
    daily = ts.get("daily", [])
    summary = (insights.get("summary", "") or "").strip()
    n_days = len(daily)
    total_cnt = sum(int(x.get("count", 0)) for x in daily)
    date_range = f"{daily[0].get('date', '?')} ~ {daily[-1].get('date', '?')}" if n_days > 0 else "-"
    today = datetime.now().strftime("%Y-%m-%d")
    lines = []
    lines.append(f"# Weekly/New Biz Report ({today})\n")
    lines.append("## Executive Summary\n")
    lines.append("- ì´ë²ˆ ê¸°ê°„ í•µì‹¬ í† í”½ê³¼ í‚¤ì›Œë“œ, ì£¼ìš” ì‹œì‚¬ì ì„ ìš”ì•½í•©ë‹ˆë‹¤.\n")
    if summary:
        lines.append(summary + "\n")
    lines.append("## Key Metrics\n")
    num_docs = keywords.get("stats", {}).get("num_docs", "N/A")
    num_docs_disp = _fmt_int(num_docs) if isinstance(num_docs, (int, float)) or str(num_docs).isdigit() else str(num_docs)
    lines.append(f"- ê¸°ê°„: {date_range}")
    lines.append(f"- ì´ ê¸°ì‚¬ ìˆ˜: {_fmt_int(total_cnt)}")
    lines.append(f"- ë¬¸ì„œ ìˆ˜: {num_docs_disp}")
    lines.append(f"- í‚¤ì›Œë“œ ìˆ˜(ìƒìœ„): {len(klist)}")
    lines.append(f"- í† í”½ ìˆ˜: {len(tlist)}")
    lines.append(f"- ì‹œê³„ì—´ ë°ì´í„° ì¼ì ìˆ˜: {n_days}\n")
    lines.append("## Top Keywords\n")
    lines.append(f"![Word Cloud]({fig_dir}/wordcloud.png)\n")
    if klist:
        kw_all = sorted((keywords.get("keywords") or []), key=lambda x: x.get("score", 0), reverse=True)
        lines.append("| Rank | Keyword | Score |")
        lines.append("|---:|---|---:|")
        for i, k in enumerate(kw_all[:15], 1):
            kw = (k.get("keyword", "") or "").replace("|", r"\|")
            sc = _fmt_score(k.get("score", 0), nd=3)
            lines.append(f"| {i} | {kw} | {sc} |")
    else:
        lines.append("- (ë°ì´í„° ì—†ìŒ)")
    lines.append(f"\n![Top Keywords]({fig_dir}/top_keywords.png)\n")
    lines.append(f"![Keyword Network]({fig_dir}/keyword_network.png)\n")
    lines.append("## Topics\n")
    if tlist:
        for t in tlist:
            tid = t.get("topic_id")
            top_words = [w.get("word", "") for w in t.get("top_words", []) if w.get("word")]
            head = (t.get("topic_name") or ", ".join(top_words[:3]) or f"Topic #{tid}")
            words_preview = ", ".join(top_words[:6])
            lines.append(f"- {head} (#{tid})")
            if words_preview:
                lines.append(f"  - ëŒ€í‘œ ë‹¨ì–´: {words_preview}")
            if t.get("insight"):
                one_liner = (t.get("insight") or "").replace("\n", " ").strip()
                lines.append(f"  - ìš”ì•½: {one_liner}")
    else:
        lines.append("- (ë°ì´í„° ì—†ìŒ)")
    lines.append(f"\n![Topics]({fig_dir}/topics.png)\n")
    lines.append(_generate_matrix_section(topics))
    lines.append(_generate_visual_analysis_section(fig_dir))
  
    lines.append(_generate_relationship_competition_section(fig_dir))

    lines.append("\n## Trend\n")
    lines.append("- ìµœê·¼ ê¸°ì‚¬ ìˆ˜ ì¶”ì„¸ì™€ 7ì¼ ì´ë™í‰ê· ì„ ì„ ì œê³µí•©ë‹ˆë‹¤.")
    lines.append(f"\n![Timeseries]({fig_dir}/timeseries.png)\n")
    lines.append("## Insights\n")
    if summary:
        lines.append(summary + "\n")
    else:
        lines.append("- (ìš”ì•½ ì—†ìŒ)\n")

    lines.append(_generate_signals_section())

    lines.append(_generate_tech_maturity_section())
    lines.append(_generate_weak_signal_section())

    lines.append("## Opportunities (Top 5)\n")
    ideas_all = (opps.get("ideas", []) or [])
    if ideas_all:
        ideas_sorted = sorted(
            ideas_all,
            key=lambda it: float(it.get("score", it.get("priority_score", 0)) or 0),
            reverse=True
        )[:5]
        _do_trunc = os.getenv("TRUNCATE_OPP", "").lower() in ("1", "true", "yes", "y")
        lines.append("| Idea | Target | Value Prop | Score (Market / Urgency / Feasibility / Risk) |")
        lines.append("|---|---|---|---|")
        for it in ideas_sorted:
            idea_raw = (it.get('idea', '') or it.get('title', '') or '')
            tgt_raw = it.get('target_customer', '') or ''
            vp_raw = (it.get('value_prop', '') or '').replace("\n", " ")
            if _do_trunc:
                idea = _truncate(idea_raw, 120).replace("|", r"\|")
                tgt = _truncate(tgt_raw, 80).replace("|", r"\|")
                vp = _truncate(vp_raw, 280).replace("|", r"\|")
            else:
                idea = idea_raw.replace("|", r"\|")
                tgt = tgt_raw.replace("|", r"\|")
                vp = vp_raw.replace("|", r"\|")
            score_val = it.get("score", "")
            bd = it.get("score_breakdown", {})
            mkt = bd.get("market", "")
            urg = bd.get("urgency", "")
            feas = bd.get("feasibility", "")
            risk = bd.get("risk", "")
            score_str = f"{score_val} ({mkt} / {urg} / {feas} / {risk})" if score_val != "" else ""
            lines.append(f"| {idea} | {tgt} | {vp} | {score_str} |")
    else:
        lines.append("- (ì•„ì´ë””ì–´ ì—†ìŒ)")
    chart_path = "outputs/fig/idea_score_distribution.png"
    if os.path.exists(chart_path):
        lines.append("\n### ğŸ“Š ì•„ì´ë””ì–´ ì ìˆ˜ ë¶„í¬")
        lines.append(f"![ì•„ì´ë””ì–´ ì ìˆ˜ ë¶„í¬](fig/idea_score_distribution.png)\n")
    else:
        print(f"[WARN] Chart image not found at {chart_path}")
    lines.append("\n## Appendix\n")
    lines.append("- ë°ì´í„°: keywords.json, topics.json, trend_timeseries.json, trend_insights.json, biz_opportunities.json")
    Path(out_md).parent.mkdir(parents=True, exist_ok=True)
    with open(out_md, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))

def build_html_from_md(md_path="outputs/report.md", out_html="outputs/report.html"):
    try:
        import markdown
        with open(md_path, "r", encoding="utf-8") as f:
            md = f.read()
        html = markdown.markdown(md, extensions=["extra", "tables", "toc"])
        html_tpl = f"""<!doctype html>
<html lang="ko">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Auto Report</title>
<link rel="preconnect" href="https://fonts.gstatic.com">
<style>
  body {{ font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Noto Sans KR', sans-serif; line-height: 1.6; padding: 24px; color: #222; }}
  img {{ max-width: 100%; height: auto; }}
  table {{ border-collapse: collapse; width: 100%; }}
  th, td {{ border: 1px solid #ddd; padding: 8px; vertical-align: top; }}
  th {{ background: #f7f7f7; }}
  code {{ background: #f1f5f9; padding: 2px 4px; border-radius: 4px; }}
  td, th {{ overflow-wrap: anywhere; word-break: break-word; white-space: normal; }}
</style>
</head>
<body>
{html}
</body>
</html>"""
        with open(out_html, "w", encoding="utf-8") as f:
            f.write(html_tpl)
    except Exception as e:
        print("[WARN] HTML ë³€í™˜ ì‹¤íŒ¨:", e)

def main():
    keywords, topics, ts, insights, opps, meta_items = load_data()
    os.makedirs("outputs/fig", exist_ok=True)
    try:
        plot_top_keywords(keywords)
    except Exception as e:
        print("[WARN] top_keywords ê·¸ë¦¼ ì‹¤íŒ¨:", e)
    try:
        plot_topics(topics)
    except Exception as e:
        print("[WARN] topics ê·¸ë¦¼ ì‹¤íŒ¨:", e)
    try:
        plot_wordcloud_from_keywords(keywords)
    except Exception as e:
        print("[WARN] wordcloud ìƒì„± ì‹¤íŒ¨:", e)
    try:
        plot_timeseries(ts)
    except Exception as e:
        print("[WARN] timeseries ê·¸ë¦¼ ì‹¤íŒ¨:", e)
    try:
        docs = build_docs_from_meta(meta_items)
        kw_list = keywords.get("keywords", [])
        n_kw = len(kw_list)
        label_cap = 25
        plot_keyword_network(
            keywords, docs,
            out_path="outputs/fig/keyword_network.png",
            topn=n_kw,
            min_cooccur=1,
            max_edges=200,
            label_top=(None if n_kw <= label_cap else label_cap)
        )
    except Exception as e:
        print("[WARN] í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ì‹¤íŒ¨:", e)
    try:
        export_csvs(ts, keywords, topics)
    except Exception as e:
        print("[WARN] CSV ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨:", e)
    try:
        build_markdown(keywords, topics, ts, insights, opps)
        build_html_from_md()
    except Exception as e:
        print("[WARN] ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨(í´ë°± ìƒì„±ìœ¼ë¡œ ëŒ€ì²´):", e)
        try:
            skeleton = """# Weekly/New Biz Report (fallback)
## Executive Summary
- (ìƒì„± ì‹¤íŒ¨ í´ë°±) ìš”ì•½ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.
## Key Metrics
- ê¸°ê°„: -
- ì´ ê¸°ì‚¬ ìˆ˜: 0
- ë¬¸ì„œ ìˆ˜: 0
- í‚¤ì›Œë“œ ìˆ˜(ìƒìœ„): 0
- í† í”½ ìˆ˜: 0
- ì‹œê³„ì—´ ë°ì´í„° ì¼ì ìˆ˜: 0
## Top Keywords
- (ë°ì´í„° ì—†ìŒ)
## Topics
- (ë°ì´í„° ì—†ìŒ)
## Trend
- (ë°ì´í„° ì—†ìŒ)
## Insights
- (ìš”ì•½ ì—†ìŒ)
## Opportunities (Top 5)
- (ì•„ì´ë””ì–´ ì—†ìŒ)
## Appendix
- ë°ì´í„°: keywords.json, topics.json, trend_timeseries.json, trend_insights.json, biz_opportunities.json
"""
            with open("outputs/report.md", "w", encoding="utf-8") as f:
                f.write(skeleton)
            try:
                build_html_from_md()
            except Exception as e2:
                print("[WARN] HTML í´ë°± ë³€í™˜ ì‹¤íŒ¨:", e2)
        except Exception as e3:
            print("[ERROR] í´ë°± ë¦¬í¬íŠ¸ ìƒì„±ë„ ì‹¤íŒ¨:", e3)
    print("[INFO] Module F ì™„ë£Œ | report.md, report.html ìƒì„±(ë˜ëŠ” í´ë°± ìƒì„±)")

if __name__ == "__main__":
    main()