import os
import json
import glob
import re
import datetime
from pathlib import Path
from wordcloud import WordCloud
from matplotlib import pyplot as plt
import pandas as pd
import matplotlib.dates as mdates
from datetime import datetime, timedelta
import seaborn as sns
import networkx as nx
from src.utils import load_json, save_json, latest


### ê¸°ë³¸ ì„¤ì •
def load_data():
    keywords = load_json("outputs/keywords.json", {"keywords": [], "stats": {}})
    topics = load_json("outputs/topics.json", {"topics": []})
    ts = load_json("outputs/trend_timeseries.json", {"daily": []})
    insights = load_json("outputs/trend_insights.json", {"summary": "", "top_topics": [], "evidence": {}})
    opps = load_json("outputs/biz_opportunities.json", {"ideas": []})
    meta_path = latest("data/news_meta_*.json")
    meta_items = load_json(meta_path, []) if meta_path else []
    return keywords, topics, ts, insights, opps, meta_items

def simple_tokenize_ko(text: str):
    toks = re.findall(r"[ê°€-í£A-Za-z0-9]+", text or "")
    toks = [t.lower() for t in toks if len(t) >= 2]
    return toks

def export_csvs(ts_obj, keywords_obj, topics_obj, out_dir="outputs/export"):
    import pandas as pd
    import os
    os.makedirs(out_dir, exist_ok=True)
    daily = (ts_obj or {}).get("daily", [])
    df_ts = pd.DataFrame(daily) if daily else pd.DataFrame(columns=["date", "count"])
    df_ts.to_csv(os.path.join(out_dir, "timeseries_daily.csv"), index=False, encoding="utf-8")
    kws = (keywords_obj or {}).get("keywords", [])[:20]
    df_kw = pd.DataFrame(kws) if kws else pd.DataFrame(columns=["keyword", "score"])
    df_kw.to_csv(os.path.join(out_dir, "keywords_top20.csv"), index=False, encoding="utf-8")
    topics = (topics_obj or {}).get("topics", [])
    rows = []
    for t in topics:
        tid = t.get("topic_id")
        for w in (t.get("top_words") or [])[:10]:
            pw = w.get("prob", None)
            try:
                p = float(pw)
                if p == 0.0:
                    p = 1e-6
            except Exception:
                p = 1e-6
            rows.append({"topic_id": tid, "word": w.get("word", ""), "prob": p})
    df_tw = pd.DataFrame(rows) if rows else pd.DataFrame(columns=["topic_id", "word", "prob"])
    df_tw.to_csv(os.path.join(out_dir, "topics_top_words.csv"), index=False, encoding="utf-8")
    print("[INFO] export CSVs -> outputs/export/*.csv")

# ===== ê´€ê³„Â·ê²½ìŸ ì‹¬í™” ë¶„ì„ ì„¹ì…˜ =====
def _generate_relationship_competition_section(fig_dir="fig",
                                               net_path="outputs/company_network.json",
                                               summary_path="outputs/analysis_summary.json"):
    import os, json
    import pandas as pd
    lines = []
    lines.append("\n## ê´€ê³„Â·ê²½ìŸ ì‹¬í™” ë¶„ì„\n")

    # ë„¤íŠ¸ì›Œí¬ ë¡œë“œ
    net = {}
    try:
        with open(net_path, "r", encoding="utf-8") as f:
            net = json.load(f) or {}
    except Exception:
        lines.append("- (ë„¤íŠ¸ì›Œí¬ ë°ì´í„°ê°€ ì—†ì–´ ë³¸ ì„¹ì…˜ì„ ìƒëµí•©ë‹ˆë‹¤.)\n")
        return "\n".join(lines)

    edges = net.get("edges", [])
    nodes = net.get("nodes", [])
    top_pairs = net.get("top_pairs", [])
    central = net.get("centrality", [])
    betw = net.get("betweenness", [])
    comms = net.get("communities", [])

    # í•µì‹¬ ìš”ì•½
    num_nodes = len(nodes)
    num_edges = len(edges)
    lines.append("**í•µì‹¬ ìš”ì•½**\n")
    lines.append(f"- **ê´€ê³„ë§ ê·œëª¨:** ë…¸ë“œ {num_nodes}ê°œ / ì—£ì§€ {num_edges}ê°œ\n")
    if top_pairs:
        tp = top_pairs[0]
        lines.append(f"- **ê°€ì¥ ê°•í•œ ê´€ê³„:** {tp.get('source')} â†” {tp.get('target')} (ê°€ì¤‘ì¹˜ {tp.get('weight')}, ìœ í˜• {tp.get('rel_type')})\n")
    if central:
        lines.append(f"- **í—ˆë¸Œ í›„ë³´:** {central[0].get('org')} (Degree {central[0].get('degree_centrality')})\n")
    if betw:
        lines.append(f"- **ë¸Œë¡œì»¤ í›„ë³´:** {betw[0].get('org')} (Betweenness {betw[0].get('betweenness')})\n")

    # ìƒìœ„ ê´€ê³„ìŒ
    if top_pairs:
        df_pairs = pd.DataFrame(top_pairs)[["source","target","weight","rel_type"]]
        lines.append("\n### ìƒìœ„ ê´€ê³„ìŒ(Edge)\n")
        # ì„¤ëª… ì¸ìš© ì¶”ê°€
        lines.append("> ë™ì¼ ë¬¸ì„œ/ë¬¸ì¥ ë‚´ì—ì„œ í•¨ê»˜ ì–¸ê¸‰ëœ ê¸°ì—… ìŒì´ë©°, ê°€ì¤‘ì¹˜ëŠ” ë™ì‹œì¶œí˜„ ë¹ˆë„ì…ë‹ˆë‹¤. ê°’ì´ ë†’ì„ìˆ˜ë¡ ìƒí˜¸ ê´€ë ¨ì„±ì´ ê°•í•˜ê³ , ìœ í˜•ì€ í‚¤ì›Œë“œ ê·œì¹™ìœ¼ë¡œ ê²½ìŸ/í˜‘ë ¥/ì¤‘ë¦½ì„ ì¶”ì •í•©ë‹ˆë‹¤.\n")
        lines.append(df_pairs.rename(columns={
            "source": "Source", "target": "Target", "weight": "Weight", "rel_type": "Type"
        }).to_markdown(index=False))
        lines.append("\n")

    # ì¤‘ì‹¬ì„± ìƒìœ„
    if central:
        df_c = pd.DataFrame(central)[["org","degree_centrality"]]
        lines.append("\n### ì¤‘ì‹¬ì„± ìƒìœ„(ì—°ê²° í—ˆë¸Œ)\n")
        # ì„¤ëª… ì¸ìš© ì¶”ê°€
        lines.append("> Degree ì¤‘ì‹¬ì„±ì€ í•œ ë…¸ë“œê°€ ì—°ê²°ëœ ìƒëŒ€ ìˆ˜ì˜ ë¹„ìœ¨ë¡œ, ê°’ì´ ë†’ì„ìˆ˜ë¡ ë‹¤ìˆ˜ì˜ ê¸°ì—…ê³¼ ì§ì ‘ ì—°ê²°ëœ í—ˆë¸Œ ì„±ê²©ì„ ê°€ì§‘ë‹ˆë‹¤. í—ˆë¸ŒëŠ” ì´ìŠˆ í™•ì‚°ê³¼ ì •ë³´ ì ‘ê·¼ì„±ì´ ë†’ìŠµë‹ˆë‹¤.\n")
        lines.append(df_c.rename(columns={"org": "Org", "degree_centrality": "DegreeCentrality"}).to_markdown(index=False))
        lines.append("\n")
    if betw:
        df_b = pd.DataFrame(betw)[["org","betweenness"]]
        lines.append("\n### ë§¤ê°œ ì¤‘ì‹¬ì„± ìƒìœ„(ì •ë³´ ë¸Œë¡œì»¤)\n")
        # ì„¤ëª… ì¸ìš© ì¶”ê°€
        lines.append("> BetweennessëŠ” ë„¤íŠ¸ì›Œí¬ ê²½ë¡œì˜ â€˜ë‹¤ë¦¬â€™ ì—­í•  ì •ë„ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ê°’ì´ ë†’ì„ìˆ˜ë¡ ì„œë¡œ ë‹¤ë¥¸ ì§‘ë‹¨ì„ ì—°ê²°í•˜ëŠ” ì¤‘ê°œì(ë¸Œë¡œì»¤)ë¡œ í•´ì„ë˜ë©°, ê±°ë˜Â·í˜‘ìƒë ¥ê³¼ ì •ë³´ íë¦„ ì¥ì•…ë ¥ì´ í½ë‹ˆë‹¤.\n")
        lines.append(df_b.rename(columns={"org": "Org", "betweenness": "Betweenness"}).to_markdown(index=False))
        lines.append("\n")

    # ì»¤ë®¤ë‹ˆí‹° ë¯¸ë¦¬ë³´ê¸°
    if comms:
        lines.append("\n### ì»¤ë®¤ë‹ˆí‹°(ê´€ê³„ í´ëŸ¬ìŠ¤í„°)\n")
        # ì„¤ëª… ì¸ìš© ì¶”ê°€
        lines.append("> ëª¨ë“ˆëŸ¬ë¦¬í‹° ê¸°ë°˜ìœ¼ë¡œ ìë™ ì¶”ì¶œí•œ ê´€ê³„ ì§‘ë‹¨ì…ë‹ˆë‹¤. ê°™ì€ ì§‘ë‹¨ ë‚´ ê¸°ì—…ë“¤ì€ ìœ ì‚¬ ì£¼ì œë‚˜ ê³µê¸‰ë§ í™œë™ì„ ê³µìœ í•  ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.\n")
        preview = []
        for c in comms[:5]:
            members = c.get("members", [])[:6]
            theme = (c.get("interpretation", "") or "").strip()

            # í•´ì„ì´ ë¹„ì–´ ìˆìœ¼ë©´ ìë™ ìƒì„±: ëŒ€í‘œ ë©¤ë²„ í‚¤ì›Œë“œ ê¸°ë°˜ ìš”ì•½
            if not theme:
                # ê°„ë‹¨ ê·œì¹™: ëŒ€í‘œ ë©¤ë²„ëª…ì„ ë‚˜ì—´í•´ ìš”ì•½(í•„ìš” ì‹œ ë„ë©”ì¸ í‚¤ì›Œë“œì™€ ê²°í•© ê°€ëŠ¥)
                if members:
                    theme = f"{members[0]} ì¤‘ì‹¬ì˜ ì—°ê´€ í´ëŸ¬ìŠ¤í„°"
                else:
                    theme = "ì£¼ìš” ê¸°ì—… ì—°ê´€ í´ëŸ¬ìŠ¤í„°"

            preview.append(f"- C{c.get('community_id')}: {', '.join(members)} | í•´ì„: {theme}")
        lines.extend(preview)
        lines.append("\n")

    # ì´ë¯¸ì§€ ì²¨ë¶€(ì´ë¯¸ D/generate_visualsì—ì„œ ìƒì„±í–ˆë‹¤ê³  ê°€ì •)
    img_rel = f"outputs/{fig_dir}/company_network.png"
    if os.path.exists(img_rel):
        lines.append("### ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”\n")
        lines.append(f"![Company Network]({fig_dir}/company_network.png)\n")

    # ì¢…í•© ì½”ë©˜íŠ¸
    lines.append("> ë™ì‹œì¶œí˜„ì´ ë†’ì€ ìŒì€ ì§ì ‘ ê²½ìŸ ë˜ëŠ” ê³µê¸‰ë§ í•µì‹¬ í˜‘ë ¥ ê°€ëŠ¥ì„±ì„ ì‹œì‚¬í•˜ë©°, í—ˆë¸Œ/ë¸Œë¡œì»¤ëŠ” ì‹œì¥ ì˜í–¥ë ¥ ë° ì¤‘ê°œ í¬ì§€ì…˜ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì»¤ë®¤ë‹ˆí‹°ëŠ” ì „ëµÂ·ë°¸ë¥˜ì²´ì¸ ë‹¨ìœ„ì˜ ë™ì¡° í´ëŸ¬ìŠ¤í„°ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n")

    return "\n".join(lines)


def _fmt_int(x):
    try:
        return f"{int(x):,}"
    except Exception:
        return str(x)

def _fmt_score(x, nd=3):
    try:
        return f"{float(x):.{nd}f}"
    except Exception:
        return str(x)

def _truncate(s, n=80):
    s = (s or "").strip().replace("\n", " ")
    return s if len(s) <= n else s[:n-1] + "â€¦"

def build_markdown(keywords, topics, ts, insights, opps, fig_dir="fig", out_md="outputs/report.md"):
    import pandas as pd
    import glob

    # --- âœ¨âœ¨âœ¨ ì‹ ê·œ í—¬í¼ í•¨ìˆ˜ 1: ê¸°ìˆ  ì„±ìˆ™ë„ ì„¹ì…˜ ìƒì„± âœ¨âœ¨âœ¨ ---
    def _generate_tech_maturity_section(fig_dir="fig"):
        section_lines = ["\n## ê¸°ìˆ  ì„±ìˆ™ë„ ë¶„ì„ (Technology Maturity Analysis)\n"]
        try:
            maturity_data = load_json("outputs/tech_maturity.json", {"results": []})
            if not maturity_data.get("results"):
                section_lines.append("- (ë¶„ì„ëœ ê¸°ìˆ  ì„±ìˆ™ë„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.)\n")
                return "\n".join(section_lines)

            # --- âœ¨âœ¨âœ¨ ì°¨íŠ¸ ì´ë¯¸ì§€ì™€ ì„¤ëª… ì¶”ê°€ âœ¨âœ¨âœ¨ ---
            section_lines.append(f"![Technology Maturity Map]({fig_dir}/tech_maturity_map.png)\n")
            section_lines.append("> ê° ê¸°ìˆ ì˜ ì‹œì¥ ë‚´ ìœ„ì¹˜(Xì¶•: ê´€ì‹¬ë„, Yì¶•: ê¸ì •ì„±)ì™€ ì‚¬ì—… í™œë°œë„(ë²„ë¸” í¬ê¸°)ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.\n")

            section_lines.append("| ê¸°ìˆ  (Technology) | ì„±ìˆ™ë„ ë‹¨ê³„ (Stage) | íŒë‹¨ ê·¼ê±° (Rationale) |")
            section_lines.append("|:---|:---|:---|")
            for item in maturity_data["results"]:
                tech = item.get("technology", "N/A")
                analysis = item.get("analysis", {})
                stage = analysis.get("stage", "N/A")
                reason = analysis.get("reason", "-")
                section_lines.append(f"| {tech} | **{stage}** | {reason} |")
            section_lines.append("\n")
        except Exception as e:
            section_lines.append(f"- (ê¸°ìˆ  ì„±ìˆ™ë„ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e})\n")
        
        return "\n".join(section_lines)

    # --- âœ¨âœ¨âœ¨ ì‹ ê·œ í—¬í¼ í•¨ìˆ˜ 2: ì•½í•œ ì‹ í˜¸ ì„¹ì…˜ ìƒì„± âœ¨âœ¨âœ¨ ---
    def _generate_weak_signal_section():
        section_lines = ["\n## í¬ì°©ëœ ì•½í•œ ì‹ í˜¸ ë° í•´ì„ (Emerging Signals & Interpretation)\n"]
        try:
            weak_signal_data = load_json("outputs/weak_signal_insights.json", {"results": []})
            if not weak_signal_data.get("results"):
                section_lines.append("- (í¬ì°©ëœ ì•½í•œ ì‹ í˜¸ê°€ ì—†ìŠµë‹ˆë‹¤.)\n")
                return "\n".join(section_lines)

            for item in weak_signal_data["results"]:
                signal = item.get("signal", "N/A")
                interpretation = item.get("interpretation", "-")
                section_lines.append(f"- **{signal}**")
                section_lines.append(f"  - **í•´ì„:** {interpretation}")
            section_lines.append("\n")
        except Exception as e:
            section_lines.append(f"- (ì•½í•œ ì‹ í˜¸ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e})\n")

        return "\n".join(section_lines)

    def _generate_matrix_section(topics_obj):
        try:
            csv_path = "outputs/export/company_topic_matrix_wide.csv"
            if not os.path.exists(csv_path):
                return "\n## ê¸°ì—…Ã—í† í”½ ì§‘ì¤‘ë„ ë§¤íŠ¸ë¦­ìŠ¤ (ì£¼ê°„)\n\n- (ë¶„ì„í•  ìœ íš¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.)\n"
            df = pd.read_csv(csv_path, encoding='utf-8-sig')
            topic_cols = [col for col in df.columns if col.startswith('topic_')]
            if df.empty or not topic_cols:
                return "\n## ê¸°ì—…Ã—í† í”½ ì§‘ì¤‘ë„ ë§¤íŠ¸ë¦­ìŠ¤ (ì£¼ê°„)\n\n- (ë¶„ì„í•  ìœ íš¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.)\n"
            df_numeric = df.copy()
            for col in topic_cols:
                df_numeric[col] = df[col].fillna('').astype(str).str.split(' ').str[0].replace('', '0').astype(float)
            competitive_scores = df_numeric[topic_cols].gt(0).sum()
            top_competitive_topic_id = competitive_scores.idxmax() if not competitive_scores.empty and competitive_scores.max() > 0 else "N/A"
            df_numeric['total_score'] = df_numeric[topic_cols].sum(axis=1)
            top_focused_org = df_numeric.loc[df_numeric['total_score'].idxmax()]['org'] if not df_numeric['total_score'].empty and df_numeric['total_score'].max() > 0 else "N/A"
            max_score = 0
            rising_star_info = "N/A"
            if df_numeric[topic_cols].max().max() > 0:
                for col in topic_cols:
                    if df_numeric[col].max() > max_score:
                        max_score = df_numeric[col].max()
                        org_name = df_numeric.loc[df_numeric[col].idxmax()]['org']
                        rising_star_info = f"{org_name} @ {col}"
            topic_map = {f"topic_{t.get('topic_id')}": ", ".join([w.get('word', '') for w in t.get('top_words', [])[:2]]) for t in topics_obj.get('topics', [])}
            section_lines = ["\n## ê¸°ì—…Ã—í† í”½ ì§‘ì¤‘ë„ ë§¤íŠ¸ë¦­ìŠ¤ (ì£¼ê°„)\n"]
            section_lines.append("**í•µì‹¬ ìš”ì•½:**\n")
            section_lines.append(f"- **ê°€ì¥ ê²½ìŸì´ ì¹˜ì—´í•œ í† í”½:** **{topic_map.get(top_competitive_topic_id, top_competitive_topic_id)}** (ê°€ì¥ ë§ì€ ê¸°ì—…ë“¤ì´ ì£¼ëª©)\n")
            section_lines.append(f"- **ê°€ì¥ ì§‘ì¤‘ë„ê°€ ë†’ì€ ê¸°ì—…:** **{top_focused_org}** (ë‹¤ì–‘í•œ í† í”½ì— ê±¸ì³ ë†’ì€ ê´€ë ¨ì„±)\n")
            section_lines.append(f"- **ì£¼ëª©í•  ë§Œí•œ ì¡°í•©:** **{rising_star_info}** (ê°€ì¥ ë†’ì€ ë‹¨ì¼ ì—°ê´€ ì ìˆ˜ ê¸°ë¡)\n")
            section_lines.append("ê° ê¸°ì—…ë³„ ìƒìœ„ 8ê°œ í† í”½ì˜ ì—°ê´€ ì ìˆ˜ì™€ í•´ë‹¹ í† í”½ ë‚´ì—ì„œì˜ ì ìœ ìœ¨(%)ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n")
            section_lines.append(df.to_markdown(index=False))
            section_lines.append("\n**ì½”ë©˜íŠ¸ ë° ì•¡ì…˜ íŒíŠ¸:**\n")
            section_lines.append(f"> íŠ¹ì • í† í”½ì—ì„œ ë†’ì€ ì ìœ ìœ¨ì„ ë³´ì´ëŠ” ê¸°ì—…ì€ í•´ë‹¹ ë¶„ì•¼ì˜ 'ì£¼ë„ì(Leader)'ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤. ë°˜ë©´, íŠ¹ì • ê¸°ì—…ì´ ì†Œìˆ˜ì˜ í† í”½ì— ë†’ì€ ì ìˆ˜ë¥¼ ì§‘ì¤‘í•˜ê³  ìˆë‹¤ë©´, ì´ëŠ” í•´ë‹¹ ê¸°ì—…ì˜ 'í•µì‹¬ ì „ëµ ë¶„ì•¼'ë¥¼ ì‹œì‚¬í•©ë‹ˆë‹¤. ê²½ìŸì‚¬ ë° íŒŒíŠ¸ë„ˆì‚¬ì˜ ì§‘ì¤‘ ë¶„ì•¼ë¥¼ íŒŒì•…í•˜ì—¬ ìš°ë¦¬ì˜ ì „ëµì„ ì ê²€í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n")
            return "\n".join(section_lines)
        except Exception as e:
            return f"\n## ê¸°ì—…Ã—í† í”½ ì§‘ì¤‘ë„ ë§¤íŠ¸ë¦­ìŠ¤ (ì£¼ê°„)\n\n- (ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e})\n"

    def _generate_visual_analysis_section(fig_dir="fig"):
        section_lines = ["\n## ê¸°ì—…Ã—í† í”½ ì‹œê°ì  ë¶„ì„\n"]
        has_content = False
        heatmap_path = f"outputs/{fig_dir}/matrix_heatmap.png"
        if os.path.exists(heatmap_path):
            section_lines.append("### ì „ì²´ ì‹œì¥ êµ¬ë„ (Heatmap)\n")
            section_lines.append(f"![Heatmap]({fig_dir}/matrix_heatmap.png)\n")
            section_lines.append("> ì „ì²´ ê¸°ì—…ê³¼ í† í”½ ê°„ì˜ ê´€ê³„ë¥¼ í•œëˆˆì— ë³´ì—¬ì¤ë‹ˆë‹¤. ìƒ‰ì´ ì§„í• ìˆ˜ë¡ ì—°ê´€ì„±ì´ ë†’ìŠµë‹ˆë‹¤.\n")
            has_content = True
        share_images = sorted(glob.glob(f"outputs/{fig_dir}/topic_share_*.png"))
        if share_images:
            section_lines.append("### ì£¼ìš” í† í”½ë³„ ê²½ìŸ êµ¬ë„ (Pie Charts)\n")
            section_lines.append("> ê°€ì¥ ëœ¨ê±°ìš´ ì£¼ì œë¥¼ ë‘ê³  ì–´ë–¤ ê¸°ì—…ë“¤ì´ ê²½ìŸí•˜ëŠ”ì§€ ì ìœ ìœ¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n")
            for img_path in share_images:
                img_name = os.path.basename(img_path)
                section_lines.append(f"![Topic Share]({fig_dir}/{img_name})")
            section_lines.append("\n")
            has_content = True
        focus_images = sorted(glob.glob(f"outputs/{fig_dir}/company_focus_*.png"))
        if focus_images:
            section_lines.append("### ì£¼ìš” ê¸°ì—…ë³„ ì „ëµ ë¶„ì„ (Bar Charts)\n")
            section_lines.append("> ì‹œì¥ì„ ì£¼ë„í•˜ëŠ” ì£¼ìš” ê¸°ì—…ë“¤ì´ ì–´ë–¤ í† í”½ì— ì§‘ì¤‘í•˜ê³  ìˆëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤.\n")
            for img_path in focus_images:
                img_name = os.path.basename(img_path)
                section_lines.append(f"![Company Focus]({fig_dir}/{img_name})")
            section_lines.append("\n")
            has_content = True
        if not has_content:
            return ""
        return "\n".join(section_lines)
    
    def _generate_signals_section(fig_dir="fig"): # fig_dir ì¸ì ì¶”ê°€
        section_lines = ["\n## ì£¼ìš” ì‹œê·¸ë„ ë¶„ì„ (Key Signal Analysis)\n"]
        has_content = False

        # 1. ê°•í•œ ì‹ í˜¸ (Strong Signals) í…Œì´ë¸” ë° ì°¨íŠ¸ ìƒì„±
        try:
            strong_df = pd.read_csv("outputs/export/trend_strength.csv")
            if not strong_df.empty:
                section_lines.append("### ê°•í•œ ì‹ í˜¸ (Strong Signals)\n")
                section_lines.append("> ìµœê·¼ ë‰´ìŠ¤ì—ì„œ ê°€ì¥ ì£¼ëª©ë°›ì€ ìƒìœ„ í‚¤ì›Œë“œë“¤ì…ë‹ˆë‹¤.\n")
                
                # --- âœ¨âœ¨âœ¨ ì°¨íŠ¸ ì´ë¯¸ì§€ ì‚½ì… âœ¨âœ¨âœ¨ ---
                section_lines.append(f"![Strong Signals Chart]({fig_dir}/strong_signals_barchart.png)\n")
                
                report_df = strong_df.head(10)[['term', 'cur', 'z_like']].copy()
                report_df.rename(columns={'term': 'ê°•í•œ ì‹ í˜¸ (Term)', 'cur': 'ìµœê·¼ ì–¸ê¸‰ëŸ‰ (cur)', 'z_like': 'ì„íŒ©íŠ¸ (z_like)'}, inplace=True)
                report_df.insert(0, 'ìˆœìœ„', range(1, 1 + len(report_df)))

                section_lines.append(report_df.to_markdown(index=False))
                section_lines.append("\n")
                has_content = True
        except FileNotFoundError:
            pass
        except Exception as e:
            section_lines.append(f"- (ê°•í•œ ì‹ í˜¸ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e})\n")

        # 2. ì•½í•œ ì‹ í˜¸ (Weak Signals) í…Œì´ë¸” ë° ì°¨íŠ¸ ìƒì„±
        try:
            weak_df = pd.read_csv("outputs/export/weak_signals.csv")
            weak_insights = load_json("outputs/weak_signal_insights.json", {"results": []})
            
            if not weak_df.empty:
                section_lines.append("### ì•½í•œ ì‹ í˜¸ (Weak Signals)\n")
                section_lines.append("> ì´ ì–¸ê¸‰ëŸ‰ì€ ì ì§€ë§Œ ìµœê·¼ ê¸‰ë¶€ìƒí•˜ì—¬ ë¯¸ë˜ê°€ ê¸°ëŒ€ë˜ëŠ” 'í‹ˆìƒˆ í‚¤ì›Œë“œ'ë“¤ì…ë‹ˆë‹¤.\n")

                # --- âœ¨âœ¨âœ¨ ì°¨íŠ¸ ì´ë¯¸ì§€ ì‚½ì… âœ¨âœ¨âœ¨ ---
                section_lines.append(f"![Weak Signal Radar]({fig_dir}/weak_signal_radar.png)\n")

                if weak_insights.get("results"):
                    insights_map = {item['signal']: item['interpretation'] for item in weak_insights["results"]}
                    report_rows = []
                    for _, row in weak_df.iterrows():
                        term = row['term']
                        report_rows.append({
                            "ì•½í•œ ì‹ í˜¸ (Signal)": term,
                            "ì§€í‘œ (cur / z_like)": f"{row['cur']} / {row['z_like']:.2f}",
                            "LLMì˜ 1ì¤„ ìš”ì•½ (Interpretation)": insights_map.get(term, "-")
                        })
                    section_lines.append(pd.DataFrame(report_rows).to_markdown(index=False))
                
                section_lines.append("\n")
                has_content = True
        except FileNotFoundError:
            pass
        except Exception as e:
            section_lines.append(f"- (ì•½í•œ ì‹ í˜¸ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e})\n")

        if not has_content:
            return ""
            
        return "\n".join(section_lines)

    klist = keywords.get("keywords", [])[:15]
    tlist = topics.get("topics", [])
    daily = ts.get("daily", [])
    summary = (insights.get("summary", "") or "").strip()
    n_days = len(daily)
    total_cnt = sum(int(x.get("count", 0)) for x in daily)
    date_range = f"{daily[0].get('date', '?')} ~ {daily[-1].get('date', '?')}" if n_days > 0 else "-"
    today = datetime.now().strftime("%Y-%m-%d")
    lines = []
    lines.append(f"# Weekly/New Biz Report ({today})\n")
    lines.append("## Executive Summary\n")
    lines.append("- ì´ë²ˆ ê¸°ê°„ í•µì‹¬ í† í”½ê³¼ í‚¤ì›Œë“œ, ì£¼ìš” ì‹œì‚¬ì ì„ ìš”ì•½í•©ë‹ˆë‹¤.\n")
    if summary:
        lines.append(summary + "\n")
    lines.append("## Key Metrics\n")
    num_docs = keywords.get("stats", {}).get("num_docs", "N/A")
    num_docs_disp = _fmt_int(num_docs) if isinstance(num_docs, (int, float)) or str(num_docs).isdigit() else str(num_docs)
    lines.append(f"- ê¸°ê°„: {date_range}")
    lines.append(f"- ì´ ê¸°ì‚¬ ìˆ˜: {_fmt_int(total_cnt)}")
    lines.append(f"- ë¬¸ì„œ ìˆ˜: {num_docs_disp}")
    lines.append(f"- í‚¤ì›Œë“œ ìˆ˜(ìƒìœ„): {len(klist)}")
    lines.append(f"- í† í”½ ìˆ˜: {len(tlist)}")
    lines.append(f"- ì‹œê³„ì—´ ë°ì´í„° ì¼ì ìˆ˜: {n_days}\n")
    lines.append("## Top Keywords\n")
    lines.append(f"![Word Cloud]({fig_dir}/wordcloud.png)\n")
    if klist:
        kw_all = sorted((keywords.get("keywords") or []), key=lambda x: x.get("score", 0), reverse=True)
        lines.append("| Rank | Keyword | Score |")
        lines.append("|---:|---|---:|")
        for i, k in enumerate(kw_all[:15], 1):
            kw = (k.get("keyword", "") or "").replace("|", r"\|")
            sc = _fmt_score(k.get("score", 0), nd=3)
            lines.append(f"| {i} | {kw} | {sc} |")
    else:
        lines.append("- (ë°ì´í„° ì—†ìŒ)")
    lines.append(f"\n![Top Keywords]({fig_dir}/top_keywords.png)\n")
    lines.append(f"![Keyword Network]({fig_dir}/keyword_network.png)\n")
    lines.append("## Topics\n")
    if tlist:
        for t in tlist:
            tid = t.get("topic_id")
            top_words = [w.get("word", "") for w in t.get("top_words", []) if w.get("word")]
            head = (t.get("topic_name") or ", ".join(top_words[:3]) or f"Topic #{tid}")
            words_preview = ", ".join(top_words[:6])
            lines.append(f"- {head} (#{tid})")
            if words_preview:
                lines.append(f"  - ëŒ€í‘œ ë‹¨ì–´: {words_preview}")
            if t.get("insight"):
                one_liner = (t.get("insight") or "").replace("\n", " ").strip()
                lines.append(f"  - ìš”ì•½: {one_liner}")
    else:
        lines.append("- (ë°ì´í„° ì—†ìŒ)")
    lines.append(f"\n![Topics]({fig_dir}/topics.png)\n")
    lines.append(_generate_matrix_section(topics))
    lines.append(_generate_visual_analysis_section(fig_dir))
  
    lines.append(_generate_relationship_competition_section(fig_dir))

    lines.append("\n## Trend\n")
    lines.append("- ìµœê·¼ ê¸°ì‚¬ ìˆ˜ ì¶”ì„¸ì™€ 7ì¼ ì´ë™í‰ê· ì„ ì„ ì œê³µí•©ë‹ˆë‹¤.")
    lines.append(f"\n![Timeseries]({fig_dir}/timeseries.png)\n")
    lines.append("## Insights\n")
    if summary:
        lines.append(summary + "\n")
    else:
        lines.append("- (ìš”ì•½ ì—†ìŒ)\n")

    lines.append(_generate_signals_section())

    lines.append(_generate_tech_maturity_section())
    lines.append(_generate_weak_signal_section())

    lines.append("## Opportunities (Top 5)\n")
    ideas_all = (opps.get("ideas", []) or [])
    if ideas_all:
        ideas_sorted = sorted(
            ideas_all,
            key=lambda it: float(it.get("score", it.get("priority_score", 0)) or 0),
            reverse=True
        )[:5]
        _do_trunc = os.getenv("TRUNCATE_OPP", "").lower() in ("1", "true", "yes", "y")
        lines.append("| Idea | Target | Value Prop | Score (Market / Urgency / Feasibility / Risk) |")
        lines.append("|---|---|---|---|")
        for it in ideas_sorted:
            idea_raw = (it.get('idea', '') or it.get('title', '') or '')
            tgt_raw = it.get('target_customer', '') or ''
            vp_raw = (it.get('value_prop', '') or '').replace("\n", " ")
            if _do_trunc:
                idea = _truncate(idea_raw, 120).replace("|", r"\|")
                tgt = _truncate(tgt_raw, 80).replace("|", r"\|")
                vp = _truncate(vp_raw, 280).replace("|", r"\|")
            else:
                idea = idea_raw.replace("|", r"\|")
                tgt = tgt_raw.replace("|", r"\|")
                vp = vp_raw.replace("|", r"\|")
            score_val = it.get("score", "")
            bd = it.get("score_breakdown", {})
            mkt = bd.get("market", "")
            urg = bd.get("urgency", "")
            feas = bd.get("feasibility", "")
            risk = bd.get("risk", "")
            score_str = f"{score_val} ({mkt} / {urg} / {feas} / {risk})" if score_val != "" else ""
            lines.append(f"| {idea} | {tgt} | {vp} | {score_str} |")
    else:
        lines.append("- (ì•„ì´ë””ì–´ ì—†ìŒ)")
    chart_path = "outputs/fig/idea_score_distribution.png"
    if os.path.exists(chart_path):
        lines.append("\n### ğŸ“Š ì•„ì´ë””ì–´ ì ìˆ˜ ë¶„í¬")
        lines.append(f"![ì•„ì´ë””ì–´ ì ìˆ˜ ë¶„í¬](fig/idea_score_distribution.png)\n")
    else:
        print(f"[WARN] Chart image not found at {chart_path}")
    lines.append("\n## Appendix\n")
    lines.append("- ë°ì´í„°: keywords.json, topics.json, trend_timeseries.json, trend_insights.json, biz_opportunities.json")
    Path(out_md).parent.mkdir(parents=True, exist_ok=True)
    with open(out_md, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))

def build_html_from_md(md_path="outputs/report.md", out_html="outputs/report.html"):
    try:
        import markdown
        with open(md_path, "r", encoding="utf-8") as f:
            md = f.read()
        html = markdown.markdown(md, extensions=["extra", "tables", "toc"])
        html_tpl = f"""<!doctype html>
<html lang="ko">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Auto Report</title>
<link rel="preconnect" href="https://fonts.gstatic.com">
<style>
  body {{ font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Noto Sans KR', sans-serif; line-height: 1.6; padding: 24px; color: #222; }}
  img {{ max-width: 100%; height: auto; }}
  table {{ border-collapse: collapse; width: 100%; }}
  th, td {{ border: 1px solid #ddd; padding: 8px; vertical-align: top; }}
  th {{ background: #f7f7f7; }}
  code {{ background: #f1f5f9; padding: 2px 4px; border-radius: 4px; }}
  td, th {{ overflow-wrap: anywhere; word-break: break-word; white-space: normal; }}
</style>
</head>
<body>
{html}
</body>
</html>"""
        with open(out_html, "w", encoding="utf-8") as f:
            f.write(html_tpl)
    except Exception as e:
        print("[WARN] HTML ë³€í™˜ ì‹¤íŒ¨:", e)

def main():
    keywords, topics, ts, insights, opps, meta_items = load_data()

    try:
        export_csvs(ts, keywords, topics)
    except Exception as e:
        print("[WARN] CSV ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨:", e)
    try:
        build_markdown(keywords, topics, ts, insights, opps)
        build_html_from_md()
    except Exception as e:
        print("[WARN] ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨(í´ë°± ìƒì„±ìœ¼ë¡œ ëŒ€ì²´):", e)
        try:
            skeleton = """# Weekly/New Biz Report (fallback)
## Executive Summary
- (ìƒì„± ì‹¤íŒ¨ í´ë°±) ìš”ì•½ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.
## Key Metrics
- ê¸°ê°„: -
- ì´ ê¸°ì‚¬ ìˆ˜: 0
- ë¬¸ì„œ ìˆ˜: 0
- í‚¤ì›Œë“œ ìˆ˜(ìƒìœ„): 0
- í† í”½ ìˆ˜: 0
- ì‹œê³„ì—´ ë°ì´í„° ì¼ì ìˆ˜: 0
## Top Keywords
- (ë°ì´í„° ì—†ìŒ)
## Topics
- (ë°ì´í„° ì—†ìŒ)
## Trend
- (ë°ì´í„° ì—†ìŒ)
## Insights
- (ìš”ì•½ ì—†ìŒ)
## Opportunities (Top 5)
- (ì•„ì´ë””ì–´ ì—†ìŒ)
## Appendix
- ë°ì´í„°: keywords.json, topics.json, trend_timeseries.json, trend_insights.json, biz_opportunities.json
"""
            with open("outputs/report.md", "w", encoding="utf-8") as f:
                f.write(skeleton)
            try:
                build_html_from_md()
            except Exception as e2:
                print("[WARN] HTML í´ë°± ë³€í™˜ ì‹¤íŒ¨:", e2)
        except Exception as e3:
            print("[ERROR] í´ë°± ë¦¬í¬íŠ¸ ìƒì„±ë„ ì‹¤íŒ¨:", e3)
    print("[INFO] Module F ì™„ë£Œ | report.md, report.html ìƒì„±(ë˜ëŠ” í´ë°± ìƒì„±)")

if __name__ == "__main__":
    main()