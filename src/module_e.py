import os
import json
import glob
import re
import datetime
from pathlib import Path
from scripts.plot_font import set_kr_font, get_kr_font_path
from wordcloud import WordCloud
from matplotlib import pyplot as plt


# 1) ì¼ë°˜ ê·¸ë˜í”„ í°íŠ¸ ì„¤ì •(ë‹¤ë¥¸ í”Œë¡¯ë„ í•œê¸€ OK)
_ = set_kr_font()

# 2) ì›Œë“œí´ë¼ìš°ë“œ ì „ìš© í°íŠ¸ ê²½ë¡œ
font_path = get_kr_font_path()
print(f"[INFO] wordcloud font_path: {font_path}")


def load_json(path, default=None):
    if default is None:
        default ={}
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return default

def latest(globpat: str):
    files = sorted(glob.glob(globpat))
    return files[-1] if files else None

def load_data():
    keywords = load_json("outputs/keywords.json", {"keywords": [], "stats":{}})
    topics   = load_json("outputs/topics.json", {"topics":[]})
    ts       = load_json("outputs/trend_timeseries.json", {"daily":[]})
    insights = load_json("outputs/trend_insights.json", {"summary": "", "top_topics": [], "evidence":{}})
    opps     = load_json("outputs/biz_opportunities.json", {"ideas":[]})
    meta_path = latest("data/news_meta_*.json")
    meta_items = load_json(meta_path, []) if meta_path else[]
    return keywords, topics, ts, insights, opps, meta_items

# ---------- ê°„ë‹¨ í† í¬ë‚˜ì´ì € ----------
def simple_tokenize_ko(text: str):
    toks = re.findall(r"[ê°€-í£A-Za-z0-9]+", text or "")
    toks = [t.lower() for t in toks if len(t) >= 2]
    return toks


def ensure_fonts():
    import os
    import matplotlib
    from matplotlib import font_manager
    candidates = [
        "/usr/share/fonts/truetype/nanum/NanumGothic.ttf",
        "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc",
        "/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc",
        "/usr/share/fonts/truetype/noto/NotoSansCJK-Regular.ttc",
    ]
    font_path = next((p for p in candidates if os.path.exists(p)), None)
    if font_path:
        font_manager.fontManager.addfont(font_path)
        font_name = font_manager.FontProperties(fname=font_path).get_name()
    else:
        font_name = "NanumGothic"
    matplotlib.rcParams["font.family"] = font_name
    matplotlib.rcParams["font.sans-serif"] = [font_name, "NanumGothic", "Noto Sans CJK KR", "Malgun Gothic", "AppleGothic", "DejaVu Sans"]
    matplotlib.rcParams["axes.unicode_minus"] = False
    try:
        from matplotlib import font_manager as fm
        fm._rebuild()
    except Exception:
        pass
    return font_name

def apply_plot_style():
    import matplotlib.pyplot as plt
    plt.rcParams.update({
        "figure.dpi": 150,
        "savefig.dpi": 150,
        "axes.titlesize": 12,
        "axes.labelsize": 11,
        "xtick.labelsize": 9,
        "ytick.labelsize": 9,
        "legend.fontsize": 9,
        "axes.grid": True,
        "grid.alpha": 0.25,
        "grid.linestyle": "--",
        "grid.linewidth": 0.6,
        "axes.edgecolor": "#999",
        "axes.linewidth": 0.8,
    })

def plot_wordcloud_from_keywords(keywords_obj, out_path="outputs/fig/wordcloud.png"):
    from wordcloud import WordCloud
    import matplotlib.pyplot as plt
    import os

    # ë„¤ ìŠ¤íƒ€ì¼/í°íŠ¸ ì´ˆê¸°í™” ë¡œì§ì€ ê·¸ëŒ€ë¡œ ìœ ì§€
    ensure_fonts()
    apply_plot_style()

    os.makedirs(os.path.dirname(out_path), exist_ok=True)

    items = (keywords_obj or {}).get("keywords") or []
    if not items:
        plt.figure(figsize=(8, 5))
        plt.text(0.5, 0.5, "ì›Œë“œí´ë¼ìš°ë“œ ë°ì´í„° ì—†ìŒ", ha="center", va="center")
        plt.axis("off")
        plt.savefig(out_path, dpi=150, bbox_inches="tight")
        plt.close()
        return

    # ìƒìœ„ Nê°œë§Œ ì‚¬ìš©(ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    freqs = {}
    for it in items[:200]:
        w = (it.get("keyword") or "").strip()
        s = float(it.get("score", 0) or 0)
        if w:
            freqs[w] = freqs.get(w, 0.0) + max(s, 0.0)

    # í•µì‹¬: ì›Œë“œí´ë¼ìš°ë“œìš© í°íŠ¸ íŒŒì¼ ê²½ë¡œ í™•ë³´ (ë²ˆë“¤/ì‹œìŠ¤í…œ ìë™ íƒìƒ‰)
    font_path = get_kr_font_path()
    if not font_path:
        # ìµœí›„ì˜ ë³´ë£¨(í™˜ê²½ ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆì–´ì„œ ê²½ê³ ë§Œ)
        print("[WARN] font_pathë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”. assets/fontsì— TTF/OTF ë„£ê±°ë‚˜ scripts/plot_font.py í™•ì¸ í”Œë¦¬ì¦ˆ.")

    wc = WordCloud(
        width=1200,
        height=600,
        background_color="white",
        font_path=font_path,           # ì—¬ê¸°ë§Œ ì¶”ê°€ë˜ë©´ í•œê¸€ ì•ˆ ê¹¨ì§!
        colormap="tab20",
        prefer_horizontal=0.9,
        min_font_size=10,
        max_words=200,
        relative_scaling=0.5,
        normalize_plurals=False
    ).generate_from_frequencies(freqs)

    wc.to_file(out_path)

def plot_top_keywords(keywords, out_path="outputs/fig/top_keywords.png", topn=15):
    import matplotlib.pyplot as plt
    import seaborn as sns
    ensure_fonts(); apply_plot_style()
    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    data = keywords.get("keywords", [])[:topn]
    if not data:
        plt.figure(figsize=(8,5)); plt.text(0.5,0.5,"í‚¤ì›Œë“œ ë°ì´í„° ì—†ìŒ", ha="center")
        plt.axis("off"); plt.savefig(out_path, dpi=150, bbox_inches="tight"); plt.close(); return
    labels = [d["keyword"] for d in data][::-1]
    scores = [d["score"] for d in data][::-1]
    plt.figure(figsize=(10,6)); sns.barplot(x=scores, y=labels, color="#3b82f6")
    plt.title("Top Keywords"); plt.xlabel("Score"); plt.ylabel("")
    plt.tight_layout(); plt.savefig(out_path, dpi=150); plt.close()

def plot_topics(topics, out_path="outputs/fig/topics.png", topn_words=6):
    import os, math
    import matplotlib.pyplot as plt
    import seaborn as sns
    ensure_fonts(); apply_plot_style()
    os.makedirs(os.path.dirname(out_path), exist_ok=True)

    tps = topics.get("topics", [])
    if not tps:
        plt.figure(figsize=(8,5)); plt.text(0.5,0.5,"í† í”½ ë°ì´í„° ì—†ìŒ", ha="center")
        plt.axis("off"); plt.savefig(out_path, dpi=150, bbox_inches="tight"); plt.close(); return

    k = len(tps); cols = 2; rows = math.ceil(k / cols)
    fig, axes = plt.subplots(rows, cols, figsize=(12, 4 * rows))
    axes = axes.flatten() if k > 1 else [axes]

    for i, t in enumerate(tps):
        ax = axes[i]
        words = (t.get("top_words") or [])[:topn_words]
        labels = [str((w.get("word") or "")) for w in words][::-1]
        probs = []
        for w in words[::-1]:
            pw = w.get("prob", 1.0)
            try:
                p = float(pw)
                if p <= 0:
                    p = 1.0
            except Exception:
                p = 1.0
            probs.append(p)
        # ë³´ê¸°ìš© ìŠ¤ì¼€ì¼(ì›í•˜ë©´ ì£¼ì„ í•´ì œ)
        # probs = [p*100.0 for p in probs]
        sns.barplot(x=probs, y=labels, ax=ax, color="#10b981")
        ax.set_title(f"Topic #{t.get('topic_id')}")
        ax.set_xlabel("Weight"); ax.set_ylabel("")
    for j in range(i + 1, len(axes)):
        axes[j].axis("off")
    plt.tight_layout(); plt.savefig(out_path, dpi=150); plt.close()
    

def plot_timeseries(ts, out_path="outputs/fig/timeseries.png"):
    import matplotlib.pyplot as plt
    import pandas as pd
    import matplotlib.dates as mdates
    from datetime import datetime, timedelta
    ensure_fonts(); apply_plot_style()
    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    daily = ts.get("daily", [])
    if not daily:
        plt.figure(figsize=(10, 5)); plt.title("Articles per Day (no data)")
        plt.xlabel("Date"); plt.ylabel("Count")
        plt.tight_layout(); plt.savefig(out_path, dpi=150, bbox_inches="tight"); plt.close(); return
    df = pd.DataFrame(daily).copy()
    df["date"] = pd.to_datetime(df["date"], errors="coerce", infer_datetime_format=True, utc=False)
    df["count"] = pd.to_numeric(df.get("count", 0), errors="coerce").fillna(0).astype(int)
    df = df.dropna(subset=["date"]).sort_values("date")
    now_year = datetime.now().year; y_min, y_max = now_year - 3, now_year + 1
    df = df[(df["date"].dt.year >= y_min) & (df["date"].dt.year <= y_max)]
    if df.empty:
        plt.figure(figsize=(10, 5)); plt.title("Articles per Day (empty after filtering)")
        plt.xlabel("Date"); plt.ylabel("Count")
        plt.tight_layout(); plt.savefig(out_path, dpi=150, bbox_inches="tight"); plt.close(); return
    df = df.set_index("date")
    full_idx = pd.date_range(df.index.min().normalize(), df.index.max().normalize(), freq="D")
    df = df.reindex(full_idx).fillna(0); df.index.name = "date"; df["count"] = df["count"].astype(int)
    if len(df) == 1:
        d0 = df.index[0]; y = float(df["count"].iloc[0])
        plt.figure(figsize=(12, 4.5))
        plt.xlim(d0 - timedelta(days=1), d0 + timedelta(days=1))
        ypad = max(1, y * 0.15); plt.ylim(0, y + ypad)
        ax = plt.gca(); ax.xaxis.set_major_locator(mdates.DayLocator(interval=1))
        ax.xaxis.set_major_formatter(mdates.DateFormatter("%Y-%m-%d"))
        plt.plot([d0], [y], marker="o", color="#6366f1", label="Daily")
        plt.annotate(f"{int(y)}", (d0, y), textcoords="offset points", xytext=(0, -14), ha="center",
                     fontsize=9, bbox=dict(boxstyle="round,pad=0.2", fc="white", ec="none", alpha=0.7))
        plt.title(f"Articles per Day ({d0.strftime('%Y-%m-%d')})")
        plt.xlabel("Date"); plt.ylabel("Count"); plt.legend(loc="upper right")
        plt.grid(alpha=0.25, linestyle="--", linewidth=0.6)
        plt.tight_layout(); plt.savefig(out_path, dpi=150, bbox_inches="tight"); plt.close()
        try:
            os.makedirs("outputs/debug", exist_ok=True)
            df.reset_index().rename(columns={"index": "date"}).to_csv("outputs/debug/timeseries_preview.csv", index=False, encoding="utf-8")
        except Exception:
            pass
        return
    df["ma7"] = df["count"].rolling(window=7, min_periods=1).mean()
    focus_days = 120
    if len(df) > focus_days:
        df = df.iloc[-focus_days:]
    plt.figure(figsize=(12, 4.5))
    plt.plot(df.index, df["count"], label="Daily", color="#6366f1", linewidth=1.6)
    plt.plot(df.index, df["ma7"], label="7d MA", color="#f59e0b", linewidth=1.6)
    ax = plt.gca(); locator = mdates.AutoDateLocator(minticks=5, maxticks=10)
    formatter = mdates.ConciseDateFormatter(locator); ax.xaxis.set_major_locator(locator); ax.xaxis.set_major_formatter(formatter)
    dmin = df.index.min().strftime("%Y-%m-%d"); dmax = df.index.max().strftime("%Y-%m-%d")
    plt.title(f"Articles per Day ({dmin} ~ {dmax})")
    plt.xlabel("Date"); plt.ylabel("Count"); plt.legend(loc="upper right")
    plt.grid(alpha=0.25, linestyle="--", linewidth=0.6)
    plt.tight_layout(); plt.savefig(out_path, dpi=150, bbox_inches="tight"); plt.close()


def plot_keyword_network(keywords, docs, out_path="outputs/fig/keyword_network.png",
                         topn=50, min_cooccur=2, max_edges=200, label_top=None):
    import os, math, re
    import matplotlib.pyplot as plt
    import seaborn as sns
    import networkx as nx
    import numpy as np

    font_name = ensure_fonts()
    try:
        apply_plot_style()
    except Exception:
        pass

    os.makedirs(os.path.dirname(out_path), exist_ok=True)

    # í‚¤ì›Œë“œ ì •ê·œí™”(ë¹ˆ ë¬¸ìì—´/1ê¸€ì ì œê±°)
    def norm_kw(w: str) -> str:
        w = (w or "").strip().lower()
        w = re.sub(r"\s+", " ", w)
        return w

    raw_kw = [norm_kw(k.get("keyword")) for k in (keywords.get("keywords", [])[:topn])]
    kw = [w for w in raw_kw if w and len(w) >= 2]
    vocab = set(kw)

    # ë°ì´í„° ë¶€ì¡± ì²˜ë¦¬
    if not docs or not vocab:
        import matplotlib.pyplot as plt
        plt.figure(figsize=(8, 5))
        plt.text(0.5, 0.5, "í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ìƒì„± ë¶ˆê°€(ë°ì´í„° ë¶€ì¡±)", ha="center", va="center")
        plt.axis("off")
        plt.savefig(out_path, dpi=150, bbox_inches="tight")
        plt.close()
        return {"nodes": 0, "edges": 0}

    # ë¬¸ì„œ í† í°í™”(ë¬¸ì„œ ë‚´ ì¤‘ë³µ í† í° ì œê±°) + ì •ê·œí™”
    def _tok(x: str):
        return re.findall(r"[ê°€-í£A-Za-z0-9]+", x or "")

    token_docs = []
    for d in docs:
        toks = set(norm_kw(t) for t in _tok(d))
        toks = [t for t in toks if t and len(t) >= 2]
        token_docs.append(toks)

    # ë™ì‹œì¶œí˜„/ë¹ˆë„ ê³„ì‚°
    from collections import Counter
    co = Counter()
    freq = Counter()
    for toks in token_docs:
        toks_in = [t for t in toks if t in vocab]
        for w in toks_in:
            freq[w] += 1
        for i in range(len(toks_in)):
            for j in range(i + 1, len(toks_in)):
                a, b = sorted((toks_in[i], toks_in[j]))
                co[(a, b)] += 1

    edges = [(a, b, c) for (a, b), c in co.items() if c >= min_cooccur]
    edges = sorted(edges, key=lambda x: x[2], reverse=True)[:max_edges]

    # ê·¸ë˜í”„ êµ¬ì„±
    G = nx.Graph()
    for w, f in freq.items():
        if f > 0 and (w in vocab) and (w and len(w) >= 2):
            G.add_node(w, freq=f)
    for a, b, c in edges:
        if a in G.nodes and b in G.nodes:
            G.add_edge(a, b, weight=c)

    if G.number_of_nodes() == 0 or G.number_of_edges() == 0:
        plt.figure(figsize=(8, 5))
        plt.text(0.5, 0.5, "í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ìƒì„± ë¶ˆê°€(ì—£ì§€ ì—†ìŒ)", ha="center", va="center")
        plt.axis("off")
        plt.savefig(out_path, dpi=150, bbox_inches="tight")
        plt.close()
        return {"nodes": 0, "edges": 0}

    # ì»¤ë®¤ë‹ˆí‹° ìƒ‰ìƒ
    try:
        comms = list(nx.algorithms.community.greedy_modularity_communities(G))
        comm_map = {}
        for ci, com in enumerate(comms):
            for n in com:
                comm_map[n] = ci
        num_comm = max(comm_map.values()) + 1 if comm_map else 1
    except Exception:
        comm_map = {n: 0 for n in G.nodes()}
        num_comm = 1

    # ë ˆì´ì•„ì›ƒ
    pos = nx.spring_layout(G, seed=42, k=0.9)

    # ìŠ¤íƒ€ì¼(ë…¸ë“œ)
    palette = sns.color_palette("tab10", n_colors=max(10, num_comm))
    node_sizes = [300 + 50 * math.sqrt(G.nodes[n].get("freq", 1)) for n in G.nodes()]
    node_colors = [palette[comm_map.get(n, 0)] for n in G.nodes()]

    # ì „ì—­ ì •ê·œí™”: ì—£ì§€ ë‘ê»˜ ê³ ì • ë²”ìœ„ë¡œ ë§¤í•‘(ë¦¬í¬íŠ¸ ê°„ ë¹„êµ ìš©ì´)
    edges_all = list(G.edges(data=True))
    w_raw = np.array([float(d.get("weight", 1.0) or 1.0) for _, _, d in edges_all], dtype=float)
    # ì´ìƒì¹˜ ì™„í™”(ì„ íƒ)
    if len(w_raw) > 5:
        q95 = np.quantile(w_raw, 0.95)
        w_raw = np.minimum(w_raw, q95)
    W_MIN, W_MAX = 0.8, 2.2
    den = (w_raw.max() - w_raw.min()) or 1.0
    w_norm = (W_MIN + (W_MAX - W_MIN) * (w_raw - w_raw.min()) / den).tolist()

    # ê·¸ë¦¬ê¸°: ì—£ì§€ â†’ ë…¸ë“œ â†’ ë¼ë²¨
    plt.figure(figsize=(10, 7))
    ax = plt.gca()
    ax.set_axis_off()

    nx.draw_networkx_edges(
        G, pos,
        edgelist=[(u, v) for u, v, _ in edges_all],
        width=w_norm,
        edge_color="#666", alpha=0.25
    )
    nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=node_colors,
                           alpha=0.9, linewidths=0.5, edgecolors="#333")

    # ë¼ë²¨ ëŒ€ìƒ
    if label_top is None:
        label_nodes = list(G.nodes())
    else:
        label_nodes = [w for w, _ in sorted(freq.items(), key=lambda x: x[1], reverse=True)[:label_top]]
        label_nodes = [n for n in label_nodes if n in G.nodes()]

    # ë¼ë²¨ ìˆ˜ë™ í…ìŠ¤íŠ¸(ê²¹ì¹¨ ë°©ì§€ ë°•ìŠ¤)
    for n in label_nodes:
        txt = (n or "").strip()
        if not txt:
            continue
        x, y = pos[n]
        ax.text(
            x, y, txt,
            ha="center", va="center",
            fontsize=8, color="#111111",
            zorder=5, clip_on=False,
            fontname=font_name,
            bbox=dict(boxstyle="round,pad=0.20", fc="white", ec="none", alpha=0.80)
        )

    # ë¼ë²¨ ì˜ë¦¼ ë°©ì§€ íŒ¨ë”©
    xs = [p[0] for p in pos.values()]
    ys = [p[1] for p in pos.values()]
    if xs and ys:
        pad_x = (max(xs) - min(xs)) * 0.08 + 0.05
        pad_y = (max(ys) - min(ys)) * 0.08 + 0.05
        ax.set_xlim(min(xs) - pad_x, max(xs) + pad_x)
        ax.set_ylim(min(ys) - pad_y, max(ys) + pad_y)

    plt.title("Keyword Co-occurrence Network")
    plt.tight_layout()
    plt.savefig(out_path, dpi=150, bbox_inches="tight")
    plt.close()

    return {"nodes": G.number_of_nodes(), "edges": G.number_of_edges()}


def export_csvs(ts_obj, keywords_obj, topics_obj, out_dir="outputs/export"):
    import pandas as pd, os
    os.makedirs(out_dir, exist_ok=True)
    daily = (ts_obj or {}).get("daily", [])
    df_ts = pd.DataFrame(daily) if daily else pd.DataFrame(columns=["date","count"])
    df_ts.to_csv(os.path.join(out_dir, "timeseries_daily.csv"), index=False, encoding="utf-8")

    kws = (keywords_obj or {}).get("keywords", [])[:20]
    df_kw = pd.DataFrame(kws) if kws else pd.DataFrame(columns=["keyword","score"])
    df_kw.to_csv(os.path.join(out_dir, "keywords_top20.csv"), index=False, encoding="utf-8")

    topics = (topics_obj or {}).get("topics", [])
    rows = []
    for t in topics:
        tid = t.get("topic_id")
        for w in (t.get("top_words") or [])[:10]:
            pw = w.get("prob", None)
            try:
                p = float(pw)
                if p == 0.0:
                    p = 1e-6
            except Exception:
                p = 1e-6
            rows.append({"topic_id": tid, "word": w.get("word", ""), "prob": p})
    import pandas as pd
    df_tw = pd.DataFrame(rows) if rows else pd.DataFrame(columns=["topic_id","word","prob"])
    df_tw.to_csv(os.path.join(out_dir, "topics_top_words.csv"), index=False, encoding="utf-8")
    print("[INFO] export CSVs -> outputs/export/*.csv")

def build_docs_from_meta(meta_items):
    docs =[]
    for it in meta_items:
        title = (it.get("title") or it.get("title_og") or "").strip()
        desc  = (it.get("description") or it.get("description_og") or "").strip()
        doc = (title + " " + desc).strip()
        if doc: docs.append(doc)
    return docs

def _fmt_int(x):
    try:
        return f"{int(x):,}"
    except Exception:
        return str(x)

def _fmt_score(x, nd=3):
    try:
        return f"{float(x):.{nd}f}"
    except Exception:
        return str(x)

def _truncate(s, n=80):
    s = (s or "").strip().replace("\n", " ")
    return s if len(s) <= n else s[:n-1] + "â€¦"


def build_markdown(keywords, topics, ts, insights, opps, fig_dir="fig", out_md="outputs/report.md"):
    import pandas as pd
    import glob

    # --- ìƒˆë¡œìš´ ë§¤íŠ¸ë¦­ìŠ¤ ì„¹ì…˜ì„ ìƒì„±í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜ (ì¸ì½”ë”© ë¬¸ì œ í•´ê²° ë²„ì „) ---
    def _generate_matrix_section(topics_obj):
        try:
            csv_path = "outputs/export/company_topic_matrix_wide.csv"
            if not os.path.exists(csv_path):
                return ""

            # --- â–¼â–¼â–¼ ë°”ë¡œ ì´ ë¶€ë¶„ì— encoding='utf-8-sig' ì˜µì…˜ì„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤ â–¼â–¼â–¼ ---
            df = pd.read_csv(csv_path, encoding='utf-8-sig')
            
            # --- ì•ˆì „ì¥ì¹˜ 1: ë°ì´í„°í”„ë ˆì„ì´ ë¹„ì—ˆê±°ë‚˜, í† í”½ ì»¬ëŸ¼ì´ ì—†ëŠ” ê²½ìš° ì¦‰ì‹œ ì¢…ë£Œ ---
            topic_cols = [col for col in df.columns if col.startswith('topic_')]
            if df.empty or not topic_cols:
                return "\n## ê¸°ì—…Ã—í† í”½ ì§‘ì¤‘ë„ ë§¤íŠ¸ë¦­ìŠ¤ (ì£¼ê°„)\n\n- (ë¶„ì„í•  ìœ íš¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.)\n"

            # --- í‚¤ í•˜ì´ë¼ì´íŠ¸ ìë™ ìƒì„± ---
            df_numeric = df.copy()
            for col in topic_cols:
                # fillna('')ë¥¼ ì¶”ê°€í•˜ì—¬ ë¹ˆ ê°’(NaN)ìœ¼ë¡œ ì¸í•œ ì˜¤ë¥˜ ë°©ì§€
                df_numeric[col] = df[col].fillna('').astype(str).str.split(' ').str[0].replace('', '0').astype(float)

            # --- ì•ˆì „ì¥ì¹˜ 2: ê° ë¶„ì„ ë‹¨ê³„ë³„ ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬ ì¶”ê°€ ---
            
            # 1. ê°€ì¥ ê²½ìŸì´ ì¹˜ì—´í•œ í† í”½ ì°¾ê¸°
            competitive_scores = df_numeric[topic_cols].gt(0).sum()
            top_competitive_topic_id = competitive_scores.idxmax() if not competitive_scores.empty and competitive_scores.max() > 0 else "N/A"

            # 2. ê°€ì¥ ì§‘ì¤‘ë„ê°€ ë†’ì€ ê¸°ì—… ì°¾ê¸°
            df_numeric['total_score'] = df_numeric[topic_cols].sum(axis=1)
            top_focused_org = df_numeric.loc[df_numeric['total_score'].idxmax()]['org'] if not df_numeric['total_score'].empty and df_numeric['total_score'].max() > 0 else "N/A"

            # 3. ê°€ì¥ ë†’ì€ ë‹¨ì¼ ì ìˆ˜ë¥¼ ê¸°ë¡í•œ í† í”½-ê¸°ì—… ì¡°í•© ì°¾ê¸°
            max_score = 0
            rising_star_info = "N/A"
            if df_numeric[topic_cols].max().max() > 0:
                for col in topic_cols:
                    if df_numeric[col].max() > max_score:
                        max_score = df_numeric[col].max()
                        org_name = df_numeric.loc[df_numeric[col].idxmax()]['org']
                        rising_star_info = f"{org_name} @ {col}"

            topic_map = {f"topic_{t.get('topic_id')}": ", ".join([w.get('word', '') for w in t.get('top_words', [])[:2]]) for t in topics_obj.get('topics', [])}

            # --- ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ ìƒì„± ---
            section_lines = ["\n## ê¸°ì—…Ã—í† í”½ ì§‘ì¤‘ë„ ë§¤íŠ¸ë¦­ìŠ¤ (ì£¼ê°„)\n"]
            section_lines.append("**í•µì‹¬ ìš”ì•½:**\n")
            section_lines.append(f"- **ê°€ì¥ ê²½ìŸì´ ì¹˜ì—´í•œ í† í”½:** **{topic_map.get(top_competitive_topic_id, top_competitive_topic_id)}** (ê°€ì¥ ë§ì€ ê¸°ì—…ë“¤ì´ ì£¼ëª©)\n")
            section_lines.append(f"- **ê°€ì¥ ì§‘ì¤‘ë„ê°€ ë†’ì€ ê¸°ì—…:** **{top_focused_org}** (ë‹¤ì–‘í•œ í† í”½ì— ê±¸ì³ ë†’ì€ ê´€ë ¨ì„±)\n")
            section_lines.append(f"- **ì£¼ëª©í•  ë§Œí•œ ì¡°í•©:** **{rising_star_info}** (ê°€ì¥ ë†’ì€ ë‹¨ì¼ ì—°ê´€ ì ìˆ˜ ê¸°ë¡)\n")
            
            section_lines.append("ê° ê¸°ì—…ë³„ ìƒìœ„ 8ê°œ í† í”½ì˜ ì—°ê´€ ì ìˆ˜ì™€ í•´ë‹¹ í† í”½ ë‚´ì—ì„œì˜ ì ìœ ìœ¨(%)ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n")
            section_lines.append(df.to_markdown(index=False))
            section_lines.append("\n**ì½”ë©˜íŠ¸ ë° ì•¡ì…˜ íŒíŠ¸:**\n")
            section_lines.append(f"> íŠ¹ì • í† í”½ì—ì„œ ë†’ì€ ì ìœ ìœ¨ì„ ë³´ì´ëŠ” ê¸°ì—…ì€ í•´ë‹¹ ë¶„ì•¼ì˜ 'ì£¼ë„ì(Leader)'ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤. ë°˜ë©´, íŠ¹ì • ê¸°ì—…ì´ ì†Œìˆ˜ì˜ í† í”½ì— ë†’ì€ ì ìˆ˜ë¥¼ ì§‘ì¤‘í•˜ê³  ìˆë‹¤ë©´, ì´ëŠ” í•´ë‹¹ ê¸°ì—…ì˜ 'í•µì‹¬ ì „ëµ ë¶„ì•¼'ë¥¼ ì‹œì‚¬í•©ë‹ˆë‹¤. ê²½ìŸì‚¬ ë° íŒŒíŠ¸ë„ˆì‚¬ì˜ ì§‘ì¤‘ ë¶„ì•¼ë¥¼ íŒŒì•…í•˜ì—¬ ìš°ë¦¬ì˜ ì „ëµì„ ì ê²€í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n")

            return "\n".join(section_lines)

        except Exception as e:
            return f"\n## ê¸°ì—…Ã—í† í”½ ì§‘ì¤‘ë„ ë§¤íŠ¸ë¦­ìŠ¤ (ì£¼ê°„)\n\n- (ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e})\n"


    # --- âœ¨âœ¨ ìƒˆë¡œìš´ ì‹œê°í™” ì„¹ì…˜ ìƒì„± í•¨ìˆ˜ (ì‹ ê·œ ì¶”ê°€) âœ¨âœ¨ ---
    def _generate_visual_analysis_section(fig_dir="fig"):
        section_lines = ["\n## ê¸°ì—…Ã—í† í”½ ì‹œê°ì  ë¶„ì„\n"]
        has_content = False

        # 1. íˆíŠ¸ë§µ ì´ë¯¸ì§€ ì¶”ê°€
        heatmap_path = f"outputs/{fig_dir}/matrix_heatmap.png"
        if os.path.exists(heatmap_path):
            section_lines.append("### ì „ì²´ ì‹œì¥ êµ¬ë„ (Heatmap)\n")
            section_lines.append(f"![Heatmap]({fig_dir}/matrix_heatmap.png)\n")
            section_lines.append("> ì „ì²´ ê¸°ì—…ê³¼ í† í”½ ê°„ì˜ ê´€ê³„ë¥¼ í•œëˆˆì— ë³´ì—¬ì¤ë‹ˆë‹¤. ìƒ‰ì´ ì§„í• ìˆ˜ë¡ ì—°ê´€ì„±ì´ ë†’ìŠµë‹ˆë‹¤.\n")
            has_content = True

        # 2. í† í”½ ì ìœ ìœ¨ íŒŒì´ì°¨íŠ¸ ì¶”ê°€
        share_images = sorted(glob.glob(f"outputs/{fig_dir}/topic_share_*.png"))
        if share_images:
            section_lines.append("### ì£¼ìš” í† í”½ë³„ ê²½ìŸ êµ¬ë„ (Pie Charts)\n")
            section_lines.append("> ê°€ì¥ ëœ¨ê±°ìš´ ì£¼ì œë¥¼ ë‘ê³  ì–´ë–¤ ê¸°ì—…ë“¤ì´ ê²½ìŸí•˜ëŠ”ì§€ ì ìœ ìœ¨ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n")
            for img_path in share_images:
                img_name = os.path.basename(img_path)
                section_lines.append(f"![Topic Share]({fig_dir}/{img_name})")
            section_lines.append("\n")
            has_content = True

        # 3. ê¸°ì—… ì§‘ì¤‘ë„ ë°”ì°¨íŠ¸ ì¶”ê°€
        focus_images = sorted(glob.glob(f"outputs/{fig_dir}/company_focus_*.png"))
        if focus_images:
            section_lines.append("### ì£¼ìš” ê¸°ì—…ë³„ ì „ëµ ë¶„ì„ (Bar Charts)\n")
            section_lines.append("> ì‹œì¥ì„ ì£¼ë„í•˜ëŠ” ì£¼ìš” ê¸°ì—…ë“¤ì´ ì–´ë–¤ í† í”½ì— ì§‘ì¤‘í•˜ê³  ìˆëŠ”ì§€ ë³´ì—¬ì¤ë‹ˆë‹¤.\n")
            for img_path in focus_images:
                img_name = os.path.basename(img_path)
                section_lines.append(f"![Company Focus]({fig_dir}/{img_name})")
            section_lines.append("\n")
            has_content = True

        if not has_content:
            return "" # ìƒì„±ëœ ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ ì„¹ì…˜ ìì²´ë¥¼ ì¶”ê°€í•˜ì§€ ì•ŠìŒ
        
        return "\n".join(section_lines)

    klist = keywords.get("keywords", [])[:15]
    tlist = topics.get("topics", [])
    daily = ts.get("daily", [])
    summary = (insights.get("summary", "") or "").strip()
    n_days = len(daily)
    total_cnt = sum(int(x.get("count", 0)) for x in daily)
    date_range = f"{daily[0].get('date','?')} ~ {daily[-1].get('date','?')}" if n_days > 0 else "-"
    today = datetime.datetime.utcnow().strftime("%Y-%m-%d")
    
    lines = []
    lines.append(f"# Weekly/New Biz Report ({today})\n")
    lines.append("## Executive Summary\n")
    lines.append("- ì´ë²ˆ ê¸°ê°„ í•µì‹¬ í† í”½ê³¼ í‚¤ì›Œë“œ, ì£¼ìš” ì‹œì‚¬ì ì„ ìš”ì•½í•©ë‹ˆë‹¤.\n")
    if summary: lines.append(summary + "\n")
    
    lines.append("## Key Metrics\n")
    num_docs = keywords.get("stats", {}).get("num_docs", "N/A")
    num_docs_disp = _fmt_int(num_docs) if isinstance(num_docs, (int, float)) or str(num_docs).isdigit() else str(num_docs)
    lines.append(f"- ê¸°ê°„: {date_range}")
    lines.append(f"- ì´ ê¸°ì‚¬ ìˆ˜: {_fmt_int(total_cnt)}")
    lines.append(f"- ë¬¸ì„œ ìˆ˜: {num_docs_disp}")
    lines.append(f"- í‚¤ì›Œë“œ ìˆ˜(ìƒìœ„): {len(klist)}")
    lines.append(f"- í† í”½ ìˆ˜: {len(tlist)}")
    lines.append(f"- ì‹œê³„ì—´ ë°ì´í„° ì¼ì ìˆ˜: {n_days}\n")
    
    lines.append("## Top Keywords\n")
    lines.append(f"![Word Cloud]({fig_dir}/wordcloud.png)\n")
    if klist:
        kw_all = sorted((keywords.get("keywords") or []), key=lambda x: x.get("score", 0), reverse=True)
        lines.append("| Rank | Keyword | Score |"); lines.append("|---:|---|---:|")
        for i, k in enumerate(kw_all[:15], 1):
            kw = (k.get("keyword", "") or "").replace("|", r"\|")
            sc = _fmt_score(k.get("score", 0), nd=3)
            lines.append(f"| {i} | {kw} | {sc} |")
    else:
        lines.append("- (ë°ì´í„° ì—†ìŒ)")
    lines.append(f"\n![Top Keywords]({fig_dir}/top_keywords.png)\n")
    lines.append(f"![Keyword Network]({fig_dir}/keyword_network.png)\n")
    
    lines.append("## Topics\n")
    if tlist:
        for t in tlist:
            tid = t.get("topic_id")
            top_words = [w.get("word", "") for w in t.get("top_words", []) if w.get("word")]
            head = (t.get("topic_name") or ", ".join(top_words[:3]) or f"Topic #{tid}")
            words_preview = ", ".join(top_words[:6])
            lines.append(f"- {head} (#{tid})")
            if words_preview:
                lines.append(f"  - ëŒ€í‘œ ë‹¨ì–´: {words_preview}")
            if t.get("insight"):
                one_liner = (t.get("insight") or "").replace("\n", " ").strip()
                lines.append(f"  - ìš”ì•½: {one_liner}")
    else:
        lines.append("- (ë°ì´í„° ì—†ìŒ)")
    lines.append(f"\n![Topics]({fig_dir}/topics.png)\n")

    # --- âœ¨ ìƒˆë¡œìš´ ë§¤íŠ¸ë¦­ìŠ¤ ì„¹ì…˜ ì¶”ê°€ ---
    lines.append(_generate_matrix_section(topics))
    lines.append(_generate_visual_analysis_section(fig_dir))

    # --- ê¸°ì—… ê²½ìŸ/í˜‘ë ¥ ë„¤íŠ¸ì›Œí¬(ì‹ ê·œ) ---
    net_json = "outputs/company_network.json"
    net_png = "outputs/fig/company_network.png"

    if os.path.exists(net_png):
        lines.append("\n### ê¸°ì—… ê²½ìŸ/í˜‘ë ¥ ë„¤íŠ¸ì›Œí¬\n")
        lines.append(f"![ê¸°ì—… ë„¤íŠ¸ì›Œí¬](fig/company_network.png)\n")
        lines.append("<sub>â€» ë¹¨ê°„ìƒ‰ ë…¸ë“œëŠ” ë„¤íŠ¸ì›Œí¬ ì¤‘ì‹¬ ê¸°ì—…(í—ˆë¸Œ)ì„ ì˜ë¯¸í•˜ë©°, íŒŒë€ìƒ‰ì€ ì¼ë°˜ ê¸°ì—…ì…ë‹ˆë‹¤. "
                    "ì„ ì´ ë‘êº¼ìš¸ìˆ˜ë¡ ê¸°ì‚¬ì—ì„œ í•¨ê»˜ ì–¸ê¸‰ëœ ë¹ˆë„ê°€ ë†’ìŒì„ ëœ»í•©ë‹ˆë‹¤.</sub>\n")


    else:
        print("[WARN] company_network.png not found")

    # JSONì—ì„œ Top 5 ê¸°ì—…ìŒ + ì¤‘ì‹¬ì„± ìƒìœ„ ê¸°ì—… í‘œ ì‚½ì…
    try:
        if os.path.exists(net_json):
            with open(net_json, "r", encoding="utf-8") as f:
                net = json.load(f) or {}
            top_pairs = net.get("top_pairs", [])
            central = net.get("centrality", [])

            if top_pairs:
                lines.append("#### í•¨ê»˜ ê°€ì¥ ë§ì´ ì–¸ê¸‰ëœ ê¸°ì—… ìŒ (Top 5)\n")
                lines.append("| Rank | Pair | Co-occurrence |\n|---:|---|---|---:|\n")
                for i, e in enumerate(top_pairs, 1):
                    pair = f"{e.get('source','')} â€“ {e.get('target','')}"
                    lines.append(f"| {i} | {pair} | {int(e.get('weight',0))} |\n")
                lines.append("<sub>â€» ë‘ ê¸°ì—…ì´ ê°™ì€ ê¸°ì‚¬ì—ì„œ ìì£¼ í•¨ê»˜ ì–¸ê¸‰ë ìˆ˜ë¡ ê²½ìŸÂ·í˜‘ë ¥ ê´€ê³„ê°€ ë°€ì ‘í•˜ë‹¤ëŠ” ì‹ í˜¸ë¡œ í•´ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.</sub>\n")

            if central:
                lines.append("\n#### ì‚°ì—… ë‚´ í—ˆë¸Œ ê¸°ì—… (ì¤‘ì‹¬ì„± ê¸°ì¤€ Top 5)\n")
                lines.append("| Rank | Org | Degree | Betweenness |\n|---:|---|---:|---:|\n")
                for i, c in enumerate(central, 1):
                    lines.append(f"| {i} | {c.get('org','')} | {c.get('degree_centrality',0)} | {c.get('betweenness',0)} |\n")
                lines.append("<sub>â€» DegreeëŠ” ì—°ê²°ëœ ê¸°ì—… ìˆ˜, BetweennessëŠ” ë„¤íŠ¸ì›Œí¬ ë‚´ â€˜ì¤‘ê°œìâ€™ ì—­í•  ì •ë„ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. "
                             "ê°’ì´ ë†’ì„ìˆ˜ë¡ ì‚°ì—… ë‚´ ì˜í–¥ë ¥ì´ í¬ë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.</sub>\n")
    except Exception as e:
        print(f"[WARN] company_network.json read failed: {repr(e)}")

    
    lines.append("\n## Trend\n")
    lines.append("- ìµœê·¼ ê¸°ì‚¬ ìˆ˜ ì¶”ì„¸ì™€ 7ì¼ ì´ë™í‰ê· ì„ ì„ ì œê³µí•©ë‹ˆë‹¤.")
    lines.append(f"\n![Timeseries]({fig_dir}/timeseries.png)\n")
    
    lines.append("## Insights\n")
    if summary: lines.append(summary + "\n")
    else: lines.append("- (ìš”ì•½ ì—†ìŒ)\n")
    
    lines.append("## Opportunities (Top 5)\n")
    ideas_all = (opps.get("ideas", []) or [])
    if ideas_all:
        # ì •ë ¬ ê¸°ì¤€: score ìš°ì„ , ì—†ìœ¼ë©´ priority_score ì‚¬ìš©
        ideas_sorted = sorted(
            ideas_all,
            key=lambda it: float(it.get("score", it.get("priority_score", 0)) or 0),
            reverse=True
        )[:5]

        _do_trunc = os.getenv("TRUNCATE_OPP", "").lower() in ("1", "true", "yes", "y")
        lines.append("| Idea | Target | Value Prop | Score (Market / Urgency / Feasibility / Risk) |")
        lines.append("|---|---|---|---|")

        for it in ideas_sorted:
            idea_raw = (it.get('idea', '') or it.get('title', '') or '')
            tgt_raw  = it.get('target_customer', '') or ''
            vp_raw   = (it.get('value_prop', '') or '').replace("\n", " ")

            if _do_trunc:
                idea = _truncate(idea_raw, 120).replace("|", r"\|")
                tgt  = _truncate(tgt_raw, 80).replace("|", r"\|")
                vp   = _truncate(vp_raw, 280).replace("|", r"\|")
            else:
                idea = idea_raw.replace("|", r"\|")
                tgt  = tgt_raw.replace("|", r"\|")
                vp   = vp_raw.replace("|", r"\|")

            # ì ìˆ˜ í‘œì‹œ: score + breakdown
            score_val = it.get("score", "")
            bd = it.get("score_breakdown", {})
            mkt = bd.get("market", "")
            urg = bd.get("urgency", "")
            feas = bd.get("feasibility", "")
            risk = bd.get("risk", "")
            score_str = f"{score_val} ({mkt} / {urg} / {feas} / {risk})" if score_val != "" else ""

            lines.append(f"| {idea} | {tgt} | {vp} | {score_str} |")
    else:
        lines.append("- (ì•„ì´ë””ì–´ ì—†ìŒ)")

    chart_path = "outputs/fig/idea_score_distribution.png"
    if os.path.exists(chart_path):
        lines.append("\n### ğŸ“Š ì•„ì´ë””ì–´ ì ìˆ˜ ë¶„í¬")
        lines.append(f"![ì•„ì´ë””ì–´ ì ìˆ˜ ë¶„í¬](fig/idea_score_distribution.png)\n")
    else:
        print(f"[WARN] Chart image not found at {chart_path}")

    lines.append("\n## Appendix\n")
    lines.append("- ë°ì´í„°: keywords.json, topics.json, trend_timeseries.json, trend_insights.json, biz_opportunities.json")
    Path(out_md).parent.mkdir(parents=True, exist_ok=True)
    with open(out_md, "w", encoding="utf-8") as f:
        f.write("\n".join(lines))

def build_html_from_md(md_path="outputs/report.md", out_html="outputs/report.html"):
    try:
        import markdown
        with open(md_path, "r", encoding="utf-8") as f:
            md = f.read()
        html = markdown.markdown(md, extensions=["extra", "tables", "toc"])
        html_tpl = f"""<!doctype html>
<html lang="ko">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Auto Report</title>
<link rel="preconnect" href="https://fonts.gstatic.com">
<style>
  body {{ font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Noto Sans KR', sans-serif; line-height: 1.6; padding: 24px; color: #222; }}
  img {{ max-width: 100%; height: auto; }}
  table {{ border-collapse: collapse; width: 100%; }}
  th, td {{ border: 1px solid #ddd; padding: 8px; vertical-align: top; }}
  th {{ background: #f7f7f7; }}
  code {{ background: #f1f5f9; padding: 2px 4px; border-radius: 4px; }}
  td, th {{ overflow-wrap: anywhere; word-break: break-word; white-space: normal; }}
</style>
</head>
<body>
{html}
</body>
</html>"""
        with open(out_html, "w", encoding="utf-8") as f:
            f.write(html_tpl)
    except Exception as e:
        print("[WARN] HTML ë³€í™˜ ì‹¤íŒ¨:", e)

def export_csvs(ts_obj, keywords_obj, topics_obj, out_dir="outputs/export"):
    import pandas as pd, os
    os.makedirs(out_dir, exist_ok=True)
    daily = (ts_obj or {}).get("daily", [])
    df_ts = pd.DataFrame(daily) if daily else pd.DataFrame(columns=["date","count"])
    df_ts.to_csv(os.path.join(out_dir, "timeseries_daily.csv"), index=False, encoding="utf-8")
    kws = (keywords_obj or {}).get("keywords", [])[:20]
    df_kw = pd.DataFrame(kws) if kws else pd.DataFrame(columns=["keyword","score"])
    df_kw.to_csv(os.path.join(out_dir, "keywords_top20.csv"), index=False, encoding="utf-8")
    topics = (topics_obj or {}).get("topics", [])
    rows = []
    for t in topics:
        tid = t.get("topic_id")
        for w in (t.get("top_words") or [])[:10]:
            pw = w.get("prob", None)
            try:
                p = float(pw)
                if p == 0.0:
                    p = 1e-6
            except Exception:
                p = 1e-6
            rows.append({"topic_id": tid, "word": w.get("word", ""), "prob": p})
    df_tw = pd.DataFrame(rows) if rows else pd.DataFrame(columns=["topic_id","word","prob"])
    df_tw.to_csv(os.path.join(out_dir, "topics_top_words.csv"), index=False, encoding="utf-8")
    print("[INFO] export CSVs -> outputs/export/*.csv")


def main():
    keywords, topics, ts, insights, opps, meta_items = load_data()
    os.makedirs("outputs/fig", exist_ok=True)

    # ê·¸ë¦¼ë“¤
    try:
        plot_top_keywords(keywords)
    except Exception as e:
        print("[WARN] top_keywords ê·¸ë¦¼ ì‹¤íŒ¨:", e)
    try:
        plot_topics(topics)
    except Exception as e:
        print("[WARN] topics ê·¸ë¦¼ ì‹¤íŒ¨:", e)
    try:
        plot_wordcloud_from_keywords(keywords)
    except Exception as e:
        print("[WARN] wordcloud ìƒì„± ì‹¤íŒ¨:", e)
    try:
        plot_timeseries(ts)
    except Exception as e:
        print("[WARN] timeseries ê·¸ë¦¼ ì‹¤íŒ¨:", e)

    # ë„¤íŠ¸ì›Œí¬ ìƒì„± (module_e.py main ë‚´ë¶€)
    try:
        docs = build_docs_from_meta(meta_items)
        kw_list = keywords.get("keywords", [])
        n_kw = len(kw_list)
        label_cap = 25  # ë…¸ë“œ ìˆ˜ê°€ ì ìœ¼ë©´ ì „ë¶€, ë§ìœ¼ë©´ ìµœëŒ€ 25ê°œ ë¼ë²¨
        plot_keyword_network(
            keywords, docs,
            out_path="outputs/fig/keyword_network.png",
            topn=n_kw,                 # ìƒìœ„ í‚¤ì›Œë“œ ìˆ˜ë§Œí¼ ë…¸ë“œ
            min_cooccur=1,             # ë°ì´í„° ì ì€ ë‚  ì•ˆì •ì„±â†‘
            max_edges=200,
            label_top=(None if n_kw <= label_cap else label_cap)
        )
    except Exception as e:
        print("[WARN] í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ì‹¤íŒ¨:", e)
        

    # CSV ë‚´ë³´ë‚´ê¸°
    try:
        export_csvs(ts, keywords, topics)
    except Exception as e:
        print("[WARN] CSV ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨:", e)

    # ë¦¬í¬íŠ¸ ìƒì„±(ë°˜ë“œì‹œ íŒŒì¼ì„ ë‚¨ê¸°ëŠ” ë³´ìˆ˜ì  ë¡œì§)
    try:
        build_markdown(keywords, topics, ts, insights, opps)
        build_html_from_md()
    except Exception as e:
        print("[WARN] ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨(í´ë°± ìƒì„±ìœ¼ë¡œ ëŒ€ì²´):", e)
        try:
            # ìµœì†Œ ë³´ê³ ì„œ(ì²´í¬ ìŠ¤í¬ë¦½íŠ¸ê°€ ì°¾ëŠ” ì„¹ì…˜ í—¤ë” í¬í•¨)
            skeleton = """# Weekly/New Biz Report (fallback)

## Executive Summary
- (ìƒì„± ì‹¤íŒ¨ í´ë°±) ìš”ì•½ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.

## Key Metrics
- ê¸°ê°„: -
- ì´ ê¸°ì‚¬ ìˆ˜: 0
- ë¬¸ì„œ ìˆ˜: 0
- í‚¤ì›Œë“œ ìˆ˜(ìƒìœ„): 0
- í† í”½ ìˆ˜: 0
- ì‹œê³„ì—´ ë°ì´í„° ì¼ì ìˆ˜: 0

## Top Keywords
- (ë°ì´í„° ì—†ìŒ)

## Topics
- (ë°ì´í„° ì—†ìŒ)

## Trend
- (ë°ì´í„° ì—†ìŒ)

## Insights
- (ë°ì´í„° ì—†ìŒ)

## Opportunities (Top 5)
- (ë°ì´í„° ì—†ìŒ)

## Appendix
- ë°ì´í„°: keywords.json, topics.json, trend_timeseries.json, trend_insights.json, biz_opportunities.json
"""
            with open("outputs/report.md", "w", encoding="utf-8") as f:
                f.write(skeleton)
            # ê°„ë‹¨ HTML ë³€í™˜
            try:
                build_html_from_md()
            except Exception as e2:
                print("[WARN] HTML í´ë°± ë³€í™˜ ì‹¤íŒ¨:", e2)
        except Exception as e3:
            print("[ERROR] í´ë°± ë¦¬í¬íŠ¸ ìƒì„±ë„ ì‹¤íŒ¨:", e3)

    print("[INFO] Module E ì™„ë£Œ | report.md, report.html ìƒì„±(ë˜ëŠ” í´ë°± ìƒì„±)")
    
    
if __name__ == "__main__":
    main()
