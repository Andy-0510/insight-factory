name: Daily Pipeline

permissions:
  contents: write

on:
  schedule:
    - cron: "0 22 * * *" # UTC 22:00 = KST 07:00
  workflow_dispatch:
    inputs:
      dry_run:
        description: "드라이 런"
        required: true
        default: "true"
      pro_mode:
        description: "Pro 모드로 실행할까요? (true/false)"
        required: false
        default: "false"

concurrency:
  group: daily-${{ github.ref_name }}
  cancel-in-progress: false

jobs:
  pipeline:
    runs-on: ubuntu-latest
    env:
      # 수동 실행 시 입력값(pro_mode) 사용, 스케줄/기타 이벤트는 기본 false(Lite)
      USE_PRO: ${{ github.event_name == 'workflow_dispatch' && inputs.pro_mode || 'false' }}
      GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
      HF_HOME: ~/.cache/huggingface # 임베딩 모델 캐시 디렉터리

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      # Python 3.11 고정 + pip 캐시
      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"
          cache: "pip"
          cache-dependency-path: "requirements.txt"

      # 의존성 설치 (venv 사용)
      - name: Install dependencies
        shell: bash
        run: |
          python -m venv .venv
          source .venv/bin/activate
          python -m pip install --upgrade pip
          python -m pip install -r requirements.txt

      # Pro 모드용 모델 캐시 (선택) - 캐시가 없으면 그냥 건너뜀
      - name: Prepare HuggingFace cache (optional)
        if: env.USE_PRO == 'true'
        shell: bash
        run: |
          mkdir -p "${HF_HOME}"
          echo "HF_HOME=${HF_HOME}"

      # trafilatura 임포트 확인(같은 파이썬 보장)
      - name: Verify deps (quick)
        shell: bash
        run: |
          source .venv/bin/activate
          python - <<'PY'
          import sys
          print("PY:", sys.executable)
          try:
              import trafilatura
              print("trafilatura:", trafilatura.__version__)
          except Exception as e:
              print("trafilatura import failed:", repr(e))
              raise
          PY

      # 폰트 설치
      - name: Install viz deps
        shell: bash
        run: |
          source .venv/bin/activate
          sudo apt-get update
          sudo apt-get install -y fonts-nanum fonts-noto-cjk
          fc-list | grep -i -E "Nanum|Noto|CJK" || true
          fc-cache -f -v

      # 시크릿 체크
      - name: Preflight | Check secrets
        shell: bash
        run: |
          test -n "${{ secrets.GEMINI_API_KEY }}" || (echo "GEMINI_API_KEY 없음"; exit 1)
          if [ -z "${{ secrets.NAVER_CLIENT_ID }}" ] || [ -z "${{ secrets.NAVER_CLIENT_SECRET }}" ]; then
            echo "[WARN] NAVER API 키가 없습니다(옵션?)."
          fi

      # Module A
      - name: Module A - Fetch & Preprocess
        shell: bash
        env:
          NAVER_CLIENT_ID: ${{ secrets.NAVER_CLIENT_ID }}
          NAVER_CLIENT_SECRET: ${{ secrets.NAVER_CLIENT_SECRET }}
          DRY_RUN: ${{ github.event_name == 'workflow_dispatch' && inputs.dry_run || 'false' }}
        run: |
          source .venv/bin/activate
          python src/module_a.py

      # Check A
      - name: Check A - Validate Output
        shell: bash
        run: |
          source .venv/bin/activate
          if ! ls data/news_clean_*.json 1> /dev/null 2>&1 || ! ls data/news_meta_*.json 1> /dev/null 2>&1; then
            echo "필수 파일이 생성되지 않았습니다."
            exit 1
          fi
          ls data/
          python src/check_a.py

      # Warehouse Append
      - name: Warehouse Append
        shell: bash
        run: |
          source .venv/bin/activate
          python src/warehouse_append.py

      # Warehouse Commit & Push
      - name: Commit & Push Warehouse (main only)
        if: github.ref_name == 'main'
        shell: bash
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add data/warehouse
          git commit -m "chore: warehouse append $(date -u +'%Y-%m-%dT%H:%M:%SZ')" || echo "no changes"
          git pull --rebase --autostash origin ${{ github.ref_name }} || true
          git push origin HEAD:${{ github.ref_name }} || true

      # 본문 수집(본문→body, description 동기화)
      - name: Enrich meta with article bodies
        shell: bash
        env:
          BODY_MIN_LEN: "200"
        run: |
          source .venv/bin/activate
          python - <<'PY'
          import sys, subprocess
          try:
              import trafilatura  # noqa
          except Exception:
              subprocess.check_call([sys.executable, "-m", "pip", "install", "trafilatura"])
          PY
          python scripts/fetch_article_bodies.py

      - name: Dump B inputs (debug, post-enrich)
        shell: bash
        run: |
          source .venv/bin/activate
          mkdir -p outputs/debug
          python - <<'PY'
          import glob, json, os, shutil
          files = sorted(glob.glob("data/news_meta_*.json"))
          if not files:
              raise SystemExit("no news_meta_* found")
          meta_path = files[-1]

          # 본문 주입 이후의 최신 메타를 복사
          shutil.copy2(meta_path, "outputs/debug/news_meta_latest.json")

          # 본문/설명 길이 통계
          with open(meta_path, "r", encoding="utf-8") as f:
              items = json.load(f)

          def strlen(x): return len((x or "").strip())

          body_lens = [strlen(it.get("body")) for it in items]
          desc_lens = [strlen(it.get("description")) for it in items]
          n_body_ge_120 = sum(1 for v in body_lens if v >= 120)
          n_desc_ge_120 = sum(1 for v in desc_lens if v >= 120)

          # Module B가 실제로 사용할 문서 미리보기
          from src.module_b import build_docs
          docs = build_docs(items)
          sample = docs[:200]
          lens = [len(d) for d in sample]

          out = {
              "source_meta": os.path.basename(meta_path),
              "items": len(items),
              "body_len_ge_120": n_body_ge_120,
              "desc_len_ge_120": n_desc_ge_120,
              "docs_count": len(docs),
              "sample_count": len(sample),
              "length_stats": {
                  "min": min(lens) if lens else 0,
                  "max": max(lens) if lens else 0,
                  "avg": (sum(lens)/len(lens)) if lens else 0,
              },
              "docs_sample": sample
          }
          with open("outputs/debug/b_docs.json", "w", encoding="utf-8") as f:
              json.dump(out, f, ensure_ascii=False, indent=2)
          print("[DEBUG] post-enrich | body>=120:", n_body_ge_120, "desc>=120:", n_desc_ge_120, "docs:", len(docs))
          PY

      # Module B
      - name: Module B - Keywords
        shell: bash
        run: |
          source .venv/bin/activate
          echo "[INFO] USE_PRO=${USE_PRO} → Module B 실행(워크플로우)"
          python src/module_b.py

      - name: Check B - Validate Keywords
        shell: bash
        run: |
          source .venv/bin/activate
          python src/check_b.py

      # Module C
      - name: Module C - Topics/Timeseries/Insight
        shell: bash
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        run: |
          source .venv/bin/activate
          echo "[INFO] USE_PRO=${USE_PRO} → Module C 실행(워크플로우)"
          python src/module_c.py

      - name: Check C - Validate Insights
        shell: bash
        run: |
          source .venv/bin/activate
          python src/check_c.py

      - name: Export signals (trend strength & weak signals)
        shell: bash
        run: |
          source .venv/bin/activate
          python scripts/signal_export.py

    
      # C 컨텍스트 덤프(topics+timeseries)
      - name: Dump C context (debug)
        shell: bash
        run: |
          source .venv/bin/activate
          mkdir -p outputs/debug
          python - <<'PY'
          import json
          def load(p, default):
              try:
                  with open(p, encoding="utf-8") as f:
                      return json.load(f)
              except Exception:
                  return default
          topics = load("outputs/topics.json", {"topics":[]}).get("topics", [])
          ts = load("outputs/trend_timeseries.json", {"daily":[]}).get("daily", [])
          ctx = {"topics": topics, "timeseries": ts}
          with open("outputs/debug/c_context.json", "w", encoding="utf-8") as f:
              json.dump(ctx, f, ensure_ascii=False, indent=2)
          print("[DEBUG] Dumped c_context: topics=", len(topics), "days=", len(ts))
          PY

      # Module D
      - name: Module D - Biz Opportunities
        shell: bash
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
        run: |
          source .venv/bin/activate
          python src/module_d.py

      - name: Check D - Validate Biz Opportunities
        shell: bash
        run: |
          source .venv/bin/activate
          python src/check_d.py

      # Preflight
      - name: Preflight Checks
        shell: bash
        env:
          PREFLIGHT_MIN_DAILY: "1"
          PREFLIGHT_MIN_TOTAL: "1"
          PREFLIGHT_MAX_SPAN:  "400"
        run: |
          set -e
          source .venv/bin/activate
          python scripts/preflight.py

      # Module E
      - name: Module E - Build Report
        shell: bash
        run: |
          source .venv/bin/activate
          python src/module_e.py
          
      - name: Check E - Validate Report
        shell: bash
        run: |
          source .venv/bin/activate
          python src/check_e.py
          
      # Job Summary
      - name: Build Job Summary
        shell: bash
        run: |
          source .venv/bin/activate
          python - <<'PY'
          import json, os
          def load(p, default):
              try:
                  with open(p, encoding="utf-8") as f:
                      return json.load(f)
              except Exception:
                  return default
          ts = load("outputs/trend_timeseries.json", {"daily":[]})
          daily = ts.get("daily", [])
          total = sum(int(x.get("count",0)) for x in daily)
          dr = f"{daily[0]['date']} ~ {daily[-1]['date']}" if daily else "-"
          kw = load("outputs/keywords.json", {"keywords":[]}).get("keywords", [])
          kw = sorted(kw, key=lambda x: x.get("score",0), reverse=True)[:10]
          opp = load("outputs/biz_opportunities.json", {"ideas":[]}).get("ideas", [])[:5]
          lines = []
          lines.append("# Daily Pipeline Summary\n")
          lines.append(f"- 기간: {dr}")
          lines.append(f"- 총 기사 수: {total}\n")
          lines.append("## Top 10 Keywords")
          lines.append("| Rank | Keyword | Score |")
          lines.append("|---:|---|---:|")
          for i, k in enumerate(kw, 1):
              lines.append(f"| {i} | {k.get('keyword','')} | {round(float(k.get('score',0)),3)} |")
          lines.append("")
          lines.append("## Opportunities (Top 5)")
          if opp:
              for i, it in enumerate(opp, 1):
                  title = it.get('title') or it.get('idea') or '(no title)'
                  lines.append(f"- {i}. {title}")
          else:
              lines.append("- (데이터 없음)")
          txt = "\n".join(lines) + "\n"
          print(txt)
          summ = os.environ.get("GITHUB_STEP_SUMMARY")
          if summ:
              with open(summ, "a", encoding="utf-8") as f:
                  f.write(txt)
          PY

      # 일자별 아카이브(KST)
      - name: Prepare outputs daily folder (KST)
        shell: bash
        run: |
          set -e
          DATE_KST=$(TZ=Asia/Seoul date +'%Y-%m-%d')
          TIME_KST=$(TZ=Asia/Seoul date +'%H%M-KST')
          OUTDIR="outputs/daily/${DATE_KST}/${TIME_KST}"
          mkdir -p "${OUTDIR}/fig" "${OUTDIR}/debug" "${OUTDIR}/export"
          cp outputs/topics.json             "${OUTDIR}/" || true
          cp outputs/trend_timeseries.json   "${OUTDIR}/" || true
          cp outputs/trend_insights.json     "${OUTDIR}/" || true
          cp outputs/keywords.json           "${OUTDIR}/" || true
          cp outputs/biz_opportunities.json  "${OUTDIR}/" || true
          cp outputs/report.md               "${OUTDIR}/" || true
          cp outputs/report.html             "${OUTDIR}/" || true
          cp outputs/outputs/run_meta_b.json "${OUTDIR}/" || true
          cp outputs/outputs/run_meta_c.json "${OUTDIR}/" || true
          
          if [ -d outputs/export ]; then
            cp -r outputs/export/*           "${OUTDIR}/export/" || true
          fi
          cp outputs/*.csv                   "${OUTDIR}/export/" || true
          cp outputs/export/*.csv            "${OUTDIR}/export/" || true
          cp outputs/fig/*.png               "${OUTDIR}/fig/" || true
          cp -r outputs/debug/*              "${OUTDIR}/debug/" || true
          echo "OUTDIR=${OUTDIR}" >> $GITHUB_ENV

      # outputs 커밋/푸시
      - name: Commit & Push outputs (main only)
        if: github.ref_name == 'main'
        shell: bash
        run: |
          set -e
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add outputs/daily
          git commit -m "chore: outputs (KST rotation)" || echo "no changes"
          git pull --rebase --autostash origin ${{ github.ref_name }} || true
          git push origin HEAD:${{ github.ref_name }} || true

      # 오래된 warehouse 정리
      - name: Cleanup old warehouse (main only, keep 90 days)
        if: github.ref_name == 'main'
        shell: bash
        run: |
          set -e
          test -d data/warehouse || exit 0
          find data/warehouse -type f -daystart -mtime +90 -print -delete || true
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add -A data/warehouse
          git commit -m "chore: cleanup old warehouse (>90d)" || echo "no changes"
          git pull --rebase --autostash origin ${{ github.ref_name }} || true
          git push origin HEAD:${{ github.ref_name }} || true
          
      # 오래된 outputs 정리
      - name: Cleanup old outputs (main only, keep 90 days)
        if: github.ref_name == 'main'
        shell: bash
        run: |
          set -e
          test -d outputs/daily || exit 0
          find outputs/daily -mindepth 1 -maxdepth 1 -type d -daystart -mtime +90 -print -exec rm -rf {} \; || true
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add -A outputs/daily
          git commit -m "chore: cleanup old outputs (>90d)" || echo "no changes"
          git pull --rebase --autostash origin ${{ github.ref_name }} || true
          git push origin HEAD:${{ github.ref_name }} || true

      # 아티팩트 이름 안전화
      - name: Sanitize artifact name
        shell: bash
        run: |
          CLEAN=$(echo '${{ github.ref_name }}' | tr '/:*?"<>|\\ ' '-')
          echo "ARTIFACT_NAME=report-${CLEAN}" >> $GITHUB_ENV

      # 결과 아티팩트 업로드(메타/클린/디버그 포함)
      - name: Upload report artifacts
        if: success()
        uses: actions/upload-artifact@v4
        with:
          name: ${{ env.ARTIFACT_NAME }}
          retention-days: 14
          path: |
            outputs/report.html
            outputs/report.md
            outputs/fig/*.png
            outputs/export/*.csv
            outputs/debug/*.json
            outputs/debug/run_meta_b.json
            outputs/debug/run_meta_c.json
            data/news_meta_*.json
            data/news_clean_*.json
          if-no-files-found: warn
